Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University of Applied Sciences Organizational details
•Procedure: lecture + internship
–Date: Thursday 8:00 a.m. - 11:15 a.m.
–Division of lecture/internship varies
(usually 1st block lecture, 2nd block internship)
→will be announced on Moodle
•Lecture:
–Slide presentation
–Freehand notes
–Code examples
•Exam: oral (20 min)
–All of the above is relevant to the exam 3 Offenburg University Organization
•Internship
–Exercises on the lecture material
•Programming
•Understanding, analysis, calculation
–Usually submission of
•Program code 
•1-page “paper” on the tasks
•Usually every 14 days
–Programming in 
•Java 
•CUDA/C
• … 4 Offenburg University
Parallel Programming
Concepts and Practice
Bertil Schmidt, Jorge González-
Domínguez, Christian Hundt, Moritz 
Schlarb
2018 Elsevier (Morgan Kaufmann)
approx. 61.50 EUR
Language: C++ 11
Methods
C++11 Multithreading, OpenMP, CUDA, 
MPI, UPC++Literature 5 Offenburg University
Concurrent programming with Java
Concepts and programming models for 
multicore systems
Jörg Hettel, ManhTien Tran
2016 dpunkt.verlag
approx. 35.50 EUR
Language: Java
Methods
Multithreading, thread pools, atomic, 
Fork-Join, locks, secure data structures, ...Literature 6 Offenburg UniversityTopics today
•Motivation 
–Modeling perspective
–Architecture perspective
•Basic concepts
–Parallel, concurrent, distributed programs
•Threads
–Example: Java
•Parallelization of algorithms
–2 examples 7 Offenburg University of Applied SciencesParallelism in computer science
•Why parallel programming?
–Modeling perspective (software): “The world is parallel”
Goal: mapping this parallelism in modelling
–Architectural perspective (hardware): processor systems are parallel
Goal: using computer resources 8 Offenburg University of Applied SciencesModeling perspective
•Individual application
–Subtasks to be processed simultaneously in a software, e.g.
•Calculations, queries
•User interface (display, interaction)
–Shared access to resources
•Booking systems
•Traffic systems
•Operating system
–Processes to be processed simultaneously, e.g.
•Active applications
•Background processes (virus scan, etc.)
•System processes
→Concurrency:
Parts of the system must (be able to) run alongside each other 9 Offenburg University Why parallel/concurrent 
programming?
•Program structure better reflects the problem
•Throughput
–Less waiting, e.g. for I/O processes
•Response
–Interactions can be handled with the desired priority
•Increase in execution speed
–Use of as many available computing resources as possible 10 Offenburg UniversityArchitectural perspective:
Moore's Law (1960)
"Processor performance* doubles approximately every 18 -24 months"
(*actually: transistor density)
→Newer hardware automatically speeds up software
Really?
This has no longer been the case since the 2000s!
"The free lunch is over" (H. Sutter, 2005)
The number of transistors continues to increase, but no longer
on one processor (core) 11 Offenburg UniversityMoore's Law in Key Figures 12 Offenburg University of Applied SciencesArchitecture perspective
•Multicore processors (4, 8, 16, ... cores)
–Increase in performance required (Moore's Law)
–Limits of miniaturization and higher clock speeds
•Heat generation, cooling
•Principle physical limits
• "Manycore" architectures (>hundreds of cores)
–Graphics processors (GPU)
–Tensor processors (TPU)
•Computer clusters, supercomputers 13 Offenburg University of Applied Sciences “Multicore crisis”
•Increase in performance is (initially) only theoretical
–Good for already concurrent systems (independent parts are distributed 
across cores)
–But does not automatically make individual applications faster
–It is not uncommon for them to even become slower!
•In practice, there is often a critical, resource-hungry application that needs full performance
–Server application
–Complex/intensive calculations
→Parallelization of applications/calculations necessary! 14 Offenburg University of Applied Sciences Tightening: Many-Core
•Hundreds or thousands of cores
–e.g. modern graphics cards (GPUs), vector/tensor processors
–Usually limited instruction set
–Lower performance of individual cores
–Tight physical parallelism 
(SIMD model: “single instruction, multiple data)
→Forces parallel processing 
(even of actually sequential problems)
•Parallel algorithms
–Parallelized variants of “classic” or new algorithms
–Building blocks (“parallel building blocks”) 15 Offenburg UniversityBasic concepts
•Concurrent program
–Multiple threads of action
–Logically simultaneous
–Low dependency
•(Really) parallel program
–Multiple processors
–(also) physically simultaneous
–Strong dependency 
(for overall solution)
•Distributed program
–Multiple computers
•Multiple processors
•Multiple threads of action
•Separate resources•Coordinate (relatively) 
independent activities
•Try to divide a given 
activity into 
parallel sub-steps
•Coordinate activities 
only via messages 18 Offenburg University Threads for parallelization
•Dividing a task into tasks that can be completed (as far as possible)

independently of each other and thus simultaneously

•Each thread completes one task (or several)
•The threads can (hopefully) use all available

processors/cores

→Acceleration of the program flow is possible 19 Offenburg UniversityExample: Array Count
•Determine the number of elements in an array that meet a 
specific condition.
•Algorithmic building block often required in 
programming (e.g. filtering, selecting, etc.)
•Very easy to parallelize with kprocessors by 
dividing the array into kparts
•Each part is processed by a thread101111001101011111011011 20 Offenburg University Array count: Thread
classCountThread extends Thread {
private boolean [] array;
private int lo;
private int hi;
private int result;
publicCountThread( boolean[] array, intlo, inthi) {
this.array= array;
this.lo= lo;
this.hi= hi;
}
public void run() {
for(inti = lo; i <= hi; i++)
if(array[i]) // condition
result++;
}
public int getResult() {
returnresult;
}
}Pass parameters 
in the constructor!
Pass result 
using method! 21 Hochschule OffenburgArray-Count: Aufteilung
public static void main(String[] args) {
...
CountThread[] counter = newCountThread[ NUMBER_OF_THREADS ];
intlo = 0;
inthi;
intsize = ARRAY_SIZE / NUMBER_OF_THREADS ;
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
if(i < NUMBER_OF_THREADS - 1) 
hi = lo + size - 1;
else
hi = ARRAY_SIZE - 1;
counter[i] = newCountThread(array, lo, hi);
counter[i].start();
lo = hi + 1;
}
... 22 Offenburg University Array count: 
Merge results
// Synchronization (wait for all threads)
try{
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
counter[i].join();
}
} catch(InterruptedException e) {
e.printStackTrace();
}
// Calculate total result from partial results
intresult = 0;
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
result += counter[i].getResult();
}join() merges the 
thread for which
it is called with the
calling thread. 23 Offenburg University Array count: 
Merge results
// Synchronization (wait for all threads)
intresult = 0;
try{
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
counter[i].join();
result += counter[i].getResult();
}
} catch(InterruptedException e) {
e.printStackTrace();
} 24 Offenburg UniversityAcceleration through parallelization 27 Offenburg University Speedup for array count 
with k processors
•Array is broken down into k parts (1 ≤ k≤ n)
•The more parts, the higher the parallelization
–Effort per thread: O( n/k)
•But: the more parts, the higher the effort when 
merging the results
–Effort: O( k)
•Total effort: O(n/k) + O(k) 28 Offenburg University of Applied SciencesParallel algorithms
•Previous implicit assumption: 
–k fixed (constant) by the number of processors in the system
•New perspective:
–We consider k as selectable depending on the input size n
–We call k= P(n) the processor complexity of the algorithm
–Preliminary assumption: “We can use as many processors 
as we need” (to achieve an optimal result)
–We will deal with the actual speedup on a specific system 
with p processors later ☺ 29 Offenburg UniversitySpeedup for array count?
•The more parts, the higher the parallelization
–Effort per thread: O( n/k)
•But: the more parts, the higher the effort when merging the results
–Effort: O( k)
•Total effort: O(n/k) + O(k)
–What would be a good/optimal value for kin depending on n? 31 Offenburg University of Applied SciencesAdvanced example: Editing distance
Problem:
For two given strings A and B, find the minimum
number of editing operations to convert A into B.
Editing operations:
- Inserting a character
- Deleting a character
- Replacing a character 32 Offenburg University Editing distance
Mini-example: A= SONG B= HONEY
2 editing operations necessary/sufficient:
- Replace S with H: SONG→HONG
- Insert I: HONG→HONEY
i.e. the editing distance d(A,B) is 2.
Observations:
(1) 0 ≤d(A,B) ≤ max(| A|,|B|)
(2)d(A,B) = d(B,A) 33 Offenburg UniversityApplication areas
•Search engines
–Alternative suggestions for typos
•Word processing
–Recognition of spelling mistakes
–Automatic correction
•Plagiarism detection
–Finding slightly changed texts
•Databases
–Recognition of duplicates
•Bioinformatics
–Sequence alignment 34 Offenburg University Calculating the editing distance
•Idea:
If the editing distance for partial words (prefixes) is already 
known, the editing distance for longer partial words can be easily determined.
•Dynamic programming method:
Result is built up step by step from partial results 35 Offenburg University of Applied SciencesDynamic programming
•Goal: Calculate a ( m+1)×(n+1) matrix d with the 
editing distances of all prefix combinations of A and B
•Editing distance of A and B is in d[m,n]
•Determine the editing operations by backtracking H O N I G 
0 1 2 3 4 5
S11 2 3 4 5
O22 1 2 3 4
N33 2 1 2 3
G44 3 2 2 2 36 Offenburg University of Applied SciencesCalculating the matrix
// Calculates cell d[ i,j] for i,j> 0
calcCell (i,j)
if(A[i] == B[j]) 
d[i,j] = d[i-1,j-1] // nothing to do
else
d[i,j] = 1 + min( d[i-1,j], // insertion
d[i,j-1], // deletion
d[i-1,j-1] ) // replacement
H O N I G 
0 1 2 3 4 5
S1
O2
N3
G41 2 3 4 5
2
3
41 2 3 4
2 1 2 3
3 2 2 2 37 Offenburg UniversityAlgorithm
intm= A.length;
intn= B.length;
intd[][] = new int[m+1][n+1]; // Create matrix
for(inti=0; i<=m; i++) // Initialize column 0
d[i,0] = i;
for(intj=0; j<=n; j++) // Initialize row 0
d[0,j] = j;
for(inti=1; i<=m; i++) // Fill all other cells
for(intj=1; j<=n; j++)
calcCell (i,j);
returnd[m,n]; // Return result 38 Offenburg University of Applied SciencesEfficiency
•The algorithm runs in O(n·m): 
–(Essentially) n·m cells have to be calculated. 
–Every single cell can be calculated in a constant amount of time.
–The cells are calculated one after the other. 
•For long strings (e.g. in bioinformatics) this is very 
inefficient
–String length in the millions and more
•Is there a possibility for parallelization 
i.e. can cells be calculated simultaneously? 39 Offenburg University Observations
•The order of the nested for loop is irrelevant 
–Calculation by row or column is possible
•BUT: Calculating the cells in any order is not possible 
–Every row or column requires the previous row or column.
–Every row or column must also be calculated step by step.
–More precisely: 
Every cell requires, among other things, the cell that was just previously calculated (above or to the left) for the calculation. 40 Offenburg University of Applied SciencesParallelization
•Different calculation order:
–Not row or column by row, but diagonally
•All cells on a diagonal can be calculated independently of each other, i.e. in parallel
•BUT: before each new diagonal, the previous two diagonals must be calculated
–Synchronization necessary! S A M T A Y
0 1 2 3 4 5 6 7
S10 1 2 3 
T21 12
A321
R43 41 Hochschule OffenburgAnzahl und Größe der Diagonalen
S  A  M  S  T  A  G
0  1  2  3  4  5  6  7
S10  1  2 3  4  5  6
T21  12  3  3  4 5
A321  2  3  4 3  4
R43  2  2  3 4  4  4for(k=1; k<m); k++)
for(t=1; t<=k; t++)
calcCell (k-t+1, t)
for(k=m; k<=n; k++) 
for(t=k-m+1; t<=k; t++)
calcCell (k-t+1, t)
for(k=n+1; k<n+m; k++) 
for(t=k-m+1; t<=n; t++)
calcCell (k-t+1, t) 42 Offenburg University of Applied SciencesParallel algorithm (abstract)
Not very practical, but useful, e.g. for estimating runtime:
If we have m processors, the inner loop is processed in 1 
step (each processor calculates one cell).
This gives us n+m-1 steps.
As a reminder: The sequential algorithm requires n∙m steps.for(k=1; k<n+m; k++) { 
fort=max(1,k-m+1) tot<=min(k,n) in parallel {
calcCell (k-t+1, t)
}
synchronize
} 43 Offenburg University Calculation with mThreads
Each thread processes exactly one row.
Each thread works 1 column away from its neighbors.
After each step, all threads must be synchronized.
→We need a synchronization barrier. S A T I N G A Y
0 1 2 3 4 5 6 7
S1
T2
A3
R40 1 2 3 4 5 6
1 12 3 3 4 5
2 12 3 4 3 4
32 2 3 4 4 4Thread 0
Thread 1
Thread 2
Thread 3
for(intc=1-id; c<n+m-id; c++) {
if(c>=1 &&c<=n)
calcCell (id+1, c);
// There must be a BARRIER here!
} 44 Offenburg UniversityThe Java class CyclicBarrier
•Provides a barrier for a specified number m of 
threads (more precisely: runnables)
•When the await() method is called, 
each thread waits until the barrier is lifted.
•This happens when all m threads have 
arrived at the barrier (via an internal countdown).
•Optionally, a runnable can always be executed directly before 
the barrier is lifted (e.g. for sequential intermediate steps in the main program). 45 Hochschule OffenburgJava-Rahmen mit Barriere
class EditDistance {
final int m,n;
final int [][] d;
finalCyclicBarrier barrier;
publicEditDistance( char[] A, char[] B) {
m= A.length;
n= B.length;
d = new int[ m+1][n+1];
barrier = newCyclicBarrier ( m,
newRunnable() {
public void run() {
// optionale Aktionen
}
} );
initMatrix();
for(int i=0; i< m; i++) {
newThread(newCalcThread(i)).start();   
}
}
} 46 Hochschule OffenburgBenutzung in einem Thread
class CalcThread implements Runnable {
intid;    // 0 <= id < m
CalcThread( intthreadID) { 
id = threadID; 
}
public void run() {
for(intc=1-id; c<n+m-id; c++) {
if(c>=1 &&c<=n)
calcCell (id+1, c);
try{
barrier.await();
} catch(...) { };
}
}
} 47 Offenburg University of Applied SciencesWhat is the benefit of parallelization?
•If we have m processors, each diagonal is processed in 
a single step.
–This gives us a total of n+m-1 steps.
–As a reminder: 
The sequential algorithm requires n∙m steps.
•If n=20, m=16 on a 16-core computer: 
320 vs. 35 steps
•If n=800, m=768 on a GPU (NVIDIA GTX-1050): 
614,400 vs. 1567 steps.
•If the number of processors p< m, the 
number of steps is ( n+m-1) ∙m/p
•The actual speedup depends on other factors. Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences Topics
2 Offenburg University•Perfect parallelizability
–Example: Monte Carlo simulation
•Threading in Java
•Thread safety of objects
•Synchronization of threads
–Java language tools
–Libraries Parallelization
4 Offenburg University of Applied Sciences•There are problems that can be trivially parallelized:
–Completely independent steps
–No communication required
–No (or hardly any) synchronization
–Examples:
•Array count (see lecture 1)
•Graphics rendering
•Many problems in image processing
•Monte Carlo simulations
–“embarrassingly parallel”, “perfectly parallel” Monte Carlo Simulation
5 Offenburg University•Suitable for:
–Problems with unknown exact solutions
•Financial world
•Physics, chemistry
•Probabilities
–Processes/procedures can be easily simulated
•Idea:
–Run as many simulations as possible
→Law of large numbers
–Individual simulations are completely independent
→Perfectly parallelizable Simple example
•Determining π
–Area of ​​the unit circle:
F = πr² = π
–Determine as many random points as possible in the unit square
–Count how many of these points are in the unit (quarter) circle
–A value for π can be approximated from the proportion
6 Offenburg University Repetition: Threads in Java
8 Offenburg University of Applied Sciences•Instance of the Thread class or a subclass 
of it, which is activated as a hardware thread using the start() method (only then does the thread become “alive”).
•2 methods for creating threads:
–Direct derivation of Thread
–Implementation of Runnable Derivation of Thread
9 Offenburg University class MyThread extends Thread{
...
public void run(){
// whatever this thread does
}
...
}
// Create thread object 
MyThread t = newMyThread(...);
// Activate thread 
t.start(); Implementation of Runnable
10 Offenburg University class MyClass implements Runnable {
...
public void run(){
// whatever this thread does
}
...
}
// Create thread object
Thread t = new Thread(newMyClass(...));
// Activate thread 
t.start(); Implementation of Runnable
11 Offenburg University class MyClass extends OtherClass implements Runnable {
...
public void run(){
// whatever this thread does
}
...
}
// Create thread object
Thread t = new Thread(newMyClass(...));
// Activate thread 
t.start(); Simultaneity
16 Offenburg University•Threads logically run at the same time
•In reality: ???
•Scheduler (JVM) changes states of threads
–decides which thread can run (“running”) and which
has to wait (“runnable” / “suspended”) →Competition!
–doesn’t always have to be “fair”
–“arbitrary” switching back and forth (switching)
–(almost) no influence from programming
→Program as if all threads ready to run

were actually running at the same time (and leave the rest to the system) Java Threads: Life cycle
Quelle: Th. Letschert
T is selected by scheduler
and activated
T cannot be continued, because it is &
- waiting for data
- waiting to enter a synchronized method
- sleeping
T is selected by scheduler and
deactivatedT can be continued because &
- the expected event has happened
- data is available
-synchronized methode is unlocked
- sleeping time is over Thread Scheduling
20 Offenburg University •Threads run logically at the same time
•Scheduler (JVM) changes the states of the threads
–decides which thread can run (“running”) and which has to wait (“runnable” / “suspended”) →Competition!
–doesn’t always have to be “fair”
–“arbitrary” switching back and forth
–(almost) no influence from programming
•Threads are executed in an interleaved or physically parallel manner
(or both) What does thread scheduling mean for the
program?
•Without threads: deterministic
•With threads:
–Transitions:
•If we program as if all tasks 
that can be executed simultaneously are actually executed simultaneously, this 
nondeterminism cannot cause any harm.
•What if the threads are not completely independent?
21 Offenburg Universitynot (completely) deterministic
ready < –> running unknown! Multi-threading
23 Offenburg University•The activities of the threads are often independent
of each other
→Existence of threads and scheduling can be ignored
•BUT: Sometimes threads come into conflict:
–Parts of the program that can be used by one thread without any problems
cause problems when multiple threads
use them What can happen?
24 Offenburg University • Race condition
– Multiple threads modify the same object; the result varies depending on the scheduling
• Deadlock
– Multiple threads block each other because they are waiting for resources that are currently being held by another thread
• Thread starvation
– A thread is waiting for a resource that is never allocated to it Race Condition –Example
25 Offenburg University•Our array counter (see lecture 1)
•So far:
–Main thread waits for the end of all threads, then fetches the
results and sums them
–Problem: threads that have already ended have to wait
•Alternative idea:
–As soon as each thread has calculated its sum, it adds its
result to the overall result
–As soon as the last thread has ended, you immediately have the
result101111001101011111011011 Previous
26 Offenburg Universityclass CountThread extends Thread { 
private boolean [] array; 
private int lo;
private int hi; 
private int result;
public CountThread( boolean[] array, int lo, int hi) { 
this.array = array;
this.lo = lo; 
this.hi = hi;
}
public void run() {
for(int i = lo; i <= hi; i++) 
if(array[i])
result++;
}
public int getResult() { 
returnresult;
}
}Pass parameters 
in the constructor!
Pass result 
using a method! Previous
27 Offenburg University // Synchronization (wait for all threads)
try{
for(int i = 0; i < NUMBER_OF_THREADS ; i++) { 
counter[i].join();
}
} catch(InterruptedException e) { 
e.printStackTrace();
}
// Calculate total result from partial results 
int result = 0;
for(int i = 0; i < NUMBER_OF_THREADS ; i++) { 
result += counter[i].getResult();
}join() merges the 
thread for which 
it is called with the 
calling thread. Now
28 Offenburg Universityclass CountThread extends Thread { 
private boolean [] array; 
private int lo;
private int hi; 
private int result;
static int overall_result = 0;// Common variable for overall result
public CountThread( boolean[] array, int lo, int hi) { 
this.array = array;
this.lo = lo; 
this.hi = hi;
}
public void run() {
for(int i = lo; i <= hi; i++) 
if(array[i])
result++;
overall_result +=result;
}Add result to 
overall result. Array count: result
30 Offenburg University // Synchronization (wait for all threads)
try{
for(int i = 0; i < NUMBER_OF_THREADS ; i++) { 
counter[i].join();
}
} catch(InterruptedException e) { 
e.printStackTrace();
}
System.out.println( "Result = " + CountThread. overall_result );
Does this work? Race conditions
31 Offenburg University of Applied Sciences•Different implementations lead to
different and incorrect results
•can make classes thread-unsafe
•are generally to be avoided (there are exceptions) Access to shared resources
32 Offenburg University of Applied Sciences•Synchronization necessary to avoid race conditions
(partial results are “forgotten”)
•The resource to be changed must be protected from simultaneous
changes Problem
read add
33 Offenburg University write•Adding is not an atomic operation
overall_result += result corresponds to:
–Read overall_result into a register x
–Add the value of result to x
–Write the result back into overall_result
•A context switch (thread switching) can take place between the sub-operations
read add write
Thread 1
Thread 2 Atomicity
•An operation is atomic if it is only executed by one thread.
34 Offenburg University Atomicity in Java
•Simply adding is not thread-safe
(because it is not atomic)
•Atomic is for “small” primitive data types
(int, char, byte, short, float, boolean):
–Simple reading
–Simple writing
•but not for long, double
•For general objects (reference types) there are no guaranteed
atomic operations
•Responsibility of the programmerNot both together!
35 Offenburg University Thread safety
36 Offenburg University •A class is thread-safe if it remains correct even if its code is executed by multiple threads, regardless of
–scheduling of threads
–other entanglements
–executing code affects all publicly accessible
•static fields and methods
•methods and fields of any instance
–Correct means: behavior according to specification How can you achieve thread safety?
37 Offenburg University •Make operations atomic
–by “locking out” other threads during execution
•Statelessness of classes
–No shared resources
–Each method provides its own variables/objects Statelessness
38 Offenburg University of Applied Sciences•Guarantee that each thread gets its own instances
of all fields used
–Class or object has no state that can change
(immutability)
–e.g. for classes that function as a service and perform a calculation
that is based only on the input of the threads
–in practice
•often not possible
•even more often not sensible Synchronization
39 Offenburg University•Guarantee, ...
–that only one thread can change the state of an 
object (or class) at any time (mutual exclusion)
and
–that when reading, a sensible, i.e. consistent, state (according to specification) always prevails Synchronization
40 Offenburg University•Eliminates non-determinism due to

concurrency
•Exclude undesirable sequences
•Two types:
–Competitive synchronization
•Dealing with competition
•Means: mutual exclusion
–Conditional synchronization
•Enabling cooperation
•Means: conditions, barriers Cooperative multitasking
41 Offenburg University•Goal:
–not to protect one thread from another, but rather:
–the entire program runs sensibly / correctly / faster
•Threads make programs non-deterministic:
–it is not the program that decides on the exact sequence, the
scheduler (part of the JVM) also decides on it
–non-determinism is usually desired
–occasionally the non-determinism must be reduced again:
•In synchronization situations the arbitrariness of the scheduler must be
limited:
→the program influences (synchronizes) the thread execution! Cooperative multitasking
Totally non-deterministic program execution (no synchronization)
Threading, restricted non-determinism:
Atomic actions are not interrupted
everything within a thread is executed sequentially
Synchronization constructs:
Additional restriction of non-determinism
(only as much as necessary, as little as possible)
Totally deterministic program execution (completely sequential)
42 Offenburg University Contest synchronization
43 Offenburg University of Applied Sciences•Guarantee atomicity
•Options:
–Use atomic (through Java specification) constructs
–Libraries, e.g. java.util.concurrent.atomic
–Enforce atomicity through locks (locks, monitor)
•Explicitly programmed: synchronized
•Implicitly through libraries, e.g. java.util.concurrent.atomic Explicit locks
44 Offenburg University•In Java with the keyword synchronized
•Can be used for
–Methods:
–Code blockssynchronized void setValue(… x) {
this.x = x;
}
void setValue(… x) {
…
synchronized (o) {
this.x = x;
}
}
•A lock (Monitor or Lock) is set in each case synchronized
45 Offenburg University •For non-static methods, the monitor of the object whose synchronized method is called is used. •For static methods, the monitor of the class in which the synchronized method is defined is used. •For code blocks, an (arbitrarily selectable) object must be specified whose monitor is used. Monitor
•Every Java object has a monitor
–either locked (held by a thread)
–or free
•When calling a synchronized method/block
–If monitor is free, then
•Monitor is locked (held by this thread)
•Method/block entered and executed
–If monitor is locked, then
•Thread is set to blocked state by the scheduler
(Exception: the same thread already has the monitor)
46 Offenburg University Monitor
•Every object has a monitor
–either locked (held by a thread)
–or free
•When leaving a synchronized method/block
–All threads blocked because of this monitor

go into the ready state
–One of them gets the monitor, locks it and
enters the method/block
–All others are immediately blocked again
47 Offenburg University Properties of monitors/locks
48 Offenburg University Properties of monitors/locks
49 Offenburg University Additional waiting conditions

Sometimes it is necessary for threads to wait even though no monitor is occupied because the object cannot be processed for other reasons.

Typical example: Producer-Consumer pattern
•Data structure with limited capacity (see Lab 2)
•Insert only when there is space again
•Remove only when there is something to remove

At the same time, you want to avoid the waiting costing computer resources (busy waiting) Waiting threads
Options
(1) Thread waits “outside” the object, e.g.
while(stack.isFull()) {
... 
}
stack.push(...);
Problems:
Busywaiting, costs computer resources (solution: sleep)
Thread may never get a turn
Context switch after while and before push→Racecondition
(2) Waiting within the object in the synchronized method Waiting within the method
1st attempt:
synchronized void push(longk) {
while(isFull()) {
...
}
h[end++] = k;
}
Even worse:
The method will never be left if isFull()== true Waiting within the method
2nd attempt: push no longer synchronized
voidpush(longk) {
while(isFull()) {} // isFull synchronized
h[end++] = k;
}
Also not correct:
Method content not atomic!
Context switch between instructions possible. Solution: wait / notify
Methods of java.lang.Object
can only be called within synchronized methods or blocks
wait() can throw an InterruptedException
wait()
causes the current thread to wait (the scheduler blocks it
→no busy waiting!)
At the same time, the object's monitor is released (otherwise no
other thread could do anything in the object that is also synchronized) Warten mit wait()
Korrekt:
synchronized void push(longk) {
while(isFull()) {
try{ 
wait(); 
} catch(InterruptedException e) { ... }    
}
h[end++] = k;
} wait / notify
notify() 
•causes the scheduler to notify one of the threads that is waiting for this object
•The notified thread must first acquire the monitor again before it can continue!
•only at the end of the method from which notify was called is the lock released by the current thread
notifyAll()
•Like notify, but all threads that are waiting for the object are notified
•Can always be used instead of notify, but not the other way around!
•less efficient if there are many threads Notification with notify()
Notify:
synchronized long pop() {
longresult = h[--end];
notifyAll();
returnresult;
} Kombination wait/notify
Komplett:
synchronized void push(longk) {
while(isFull()) {
try{ 
wait(); 
} catch(InterruptedException e) { ... }    
}
h[end++] = k;
notifyAll();
} Kombination wait/notify
Notify:
synchronized long pop() {
while(isEmpty()) {
try{
wait();
} catch(InterruptedException e) { ... }
}
notifyAll();
returnh[--end];
} Thread-safe data structures
Thread-safe versions of various data structures are available in the 
package java.util.concurrent
Examples of blocking data structures:
ArrayBlockingQueue
LinkedBlockingQueue
SynchronousQueue
Non-blocking but thread-safe data structures
ConcurrentLinkedQueue
& Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg UniversityTopics
•Limits of parallelization
•The PRAM model
•Analysis of parallel algorithms 4 Offenburg UniversityLimits of acceleration
•How much can an application be accelerated by
parallelization?
•Idealized assumptions
–we have as many processors as we want
–our parallelized portion is p times faster with pprocessors than with one
•What is the upper limit for the speedup?
–Remember: Speedup S(p) = tseq/ tpar(p) 5 Offenburg UniversityAmdahl's Law (Gene Amdahl, 1922-2015)
•Every application also has a non-parallelizable part, 
e.g. loading, initializing variables, etc.
•Let 0 < Par< 1 be the parallelizable part of the runtime tseq
•If 1 is the total runtime, then the time for the non-parallelizable part is ( 1 –Par), since 1 = Par+ (1 –Par)
•Only Par can be accelerated: with p processors, the time can be reduced to at most Par/ p.
•The total time would then be ( 1 –Par) + Par/ p
•This means that we can never be faster than ( 1 –Par), no matter how many processors we have!
•Speedup S(p) = 1 / ( 1 –Par + Par/p) ≤ 1 / ( 1 –Par) 6 Offenburg University Amdahl's Law: Example
•The non-parallel part of a program takes up 10% of its runtime, 90% is (perfectly) parallelizable ( Par = 0.9 ; 1-Par = 0.1 )
–With 2 processors we need
•0.45 + 0.1= 0.55 of the time
Speedup: 1.8x
–With 4 processors it is
•0.225 + 0.1= 0.325
Speedup: 3.1x
–With 16 processors
•0.056 + 0.1= 0.156
Speedup: 6.4x
–With 64 processors
•0.014 + 0.1= 0.114
Speedup: 8.8x
Source: Wikipedia 7 Offenburg University Amdahl = bad news?
•Yes, for problems of a fixed size
–These can only be accelerated up to a certain point with more and more processors
• Amdahl's Law assumes that the parallelizable part Par is independent of the number p of processors
•In reality, things often look different:
–You want to solve a larger problem in the same amount of time and use more processors for this
–In this case, Par is usually also significantly larger because the non-parallelizable part of the program is often almost constant 8 Offenburg UniversityGustafson's Law (John Gustafson, 1988)
•Another perspective on acceleration (speedup): 
With more processors, the problem size can
increase
•Gustafson: S(p) = (1 –Par) + p∙ Par
“How much more can I do in the same amount of time with pprocessors than with 
one?”
•See Amdahl: S(p) = 1 / ( 1 –Par + Par/p)
“How much faster does it take to solve the same problem with pprocessors than with 
one?” 9 Offenburg UniversityGustafson's Law: Example
•The non-parallel part of a program takes up 10% of its work, 90% can be (perfectly) parallelized (Par = 0.9; 1-Par = 0.1)
–With 2 processors we can achieve
•0.1+ 2* 0.9= 1.9 times
Speedup: 1.9x
–With 4 processors it is
•0.1+ 4* 0.9= 3.7 times
Speedup: 3.7x
–With 16 processors it is
•0.1+ 16* 0.9= 14.5 times
Speedup: 14.5x
–With 64 processors it is
•0.1+ 64* 0.9= 57.7 times
Speedup: 57.7x 11 Offenburg UniversityParallel computing models
•Sequential computing model
–Von Neumann architecture:
A program (machine code) is created in memory and 
executed by passing one instruction after the other to the CPU.

–There is only one memory for data and commands
–Relatively close relationship between hardware, software and 
theoretical modelsMemory
CPU 12 Offenburg University of Applied SciencesParallel computing models
•Various parallel computing and computer models
•Each is programmed differently
•Therefore, when designing algorithms, one always has a 
specific model in mind (e.g. editing distance)
•Different parallel machines are suitable for 
solving different problems 13 Offenburg UniversityThe PRAM model
•Parallel Random Access Machine
–n processors, connected to a
shared memory
–Each processor can access any location in the
memory in each clock cycle
–Access conflicts can occur. The memory management

of the hardware must regulate this.Shared Memory
P1P2P3 Pn 14 Offenburg University PRAM
•Theoretical model
•There is no hardware that implements it exactly like this
•ignores many specific things
–e.g. cache, …
•still useful
–No separate algorithm needed for every specific architecture
–Can be adapted to various architectures with slight changes 15 Offenburg University of Applied SciencesParallel architectures
•Classification of parallel architectures (according to Flynn, 1966):
•Examples: ?Single instruction Multiple instruction
Single Data SISD MISD*
Multiple Data SIMD MIMD 16 Offenburg University MIMD
•The processors process various programs/ 
algorithms that communicate from time to time
•Suitable for solving problems without a special 
internal structure
–Various tasks
–Exchange/communication at irregular intervals 17 Offenburg UniversitySIMD
•The same algorithm runs on each processor, but with 
different inputs in each case
•Suitable for solving problems with regular 
internal structure
–Similar tasks on a (divisible) set of data
–Communication/synchronization at regular intervals
–There are many such problems 
(e.g. graphics rendering, array count, editing distance, ...) 18 Offenburg UniversitySIMD Properties
•An n-processor SIMD computer has the following 
properties:

–Each processor can store both instructions and data in 
its local memory (register).

–Each processor has an identical copy of the same program in 
its memory.

–In each clock cycle, each processor executes the same instruction 
of this program. However, the data 
(input) of the different processors differs.

–The processors communicate either over a network or 
through a shared memory. 19 Offenburg UniversitySIMD with shared memory
•Classification of access (PRAM):
–Exclusive Read Exclusive Write (EREW )
Each memory location can only be accessed by one processor at a time.
–Concurrent Read Exclusive Write (CREW )
Multiple processors can read from the same memory location at the same time, but only one processor can write at a time.
–Concurrent Read Concurrent Write (CRCW )
Multiple processors can read or write to the same memory location at the same time.
•How is simultaneous writing regulated? 20 Offenburg UniversityCRCW variants
•Common CRCW
–All processors that want to write must write the same value
•Arbitrary CRCW
–One of the processors “wins” (its value is at the end of the memory location). We don’t know which one!
•Priority CRCW
–Processors are assigned priorities; the one with the highest priority writes its valueEREW
CREW 21 Offenburg UniversityParallel algorithms 
•Simple example: Add n numbers in parallel
•Different from before:
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3
Time: O(log n) Processors: n/2 = O(n) 22 Offenburg University Analysis of parallel algorithms
•Time complexity Tp(n)= #time steps with pprocessors
•Processor complexity P(n)= #processors (depending on n)
•Work W(n)= number of primitive operations that are executed on all processors together.
→corresponds to T1(n), i.e. sequential execution
•Span (also: depth, critical path length): number of primitive operations on the longest sequential path
→corresponds to T∞(n), i.e. execution on an idealized machine with an infinite number of processors
•Cost: Cp(n) = p ∙ Tp(n)= cost of execution 23 Offenburg UniversityAnalysis of parallel algorithms
•Our example:
–Tp(n) = 
–P(n) =
–W(n) =
–T∞(n) =
–Cp(n) = 24 Offenburg University Properties
•Work law: p∙Tpg T1
–The cost always corresponds to at least the work . (Because pprocessors 
can only perform poperations in parallel).
•Span law: Tpg T∞
–With a finite number of processors you cannot be faster than 
with an infinite number (but possibly equally fast!) 25 Offenburg University Properties
•Speedup Sp= T1/ Tp
•The work law follows: T1/ Tpf p
→Sp is limited by p
•Sp/ p is also known as efficiency
•Parallelism T1/ T∞ is the highest possible speedup ( S∞) 26 Offenburg University Properties
•Slackness: T1/ pT∞ ≤T1/ pTp≤T1/ T1 =1
span law work law
•If slackness < 1 →Speedup Sp<p
–No perfect parallelizability 27 Offenburg University of Applied SciencesOptimality in parallel algorithms
•For sequential algorithms:
–Determine lower asymptotic bound T(n) for a problem P 
of size n
•Example: T(n) = Ω(nlog n) for sorting any n numbers
–If algorithm A solves the problem for all n in time T(n), A is 
optimal for P
•Example: Merge sort, heapsort, some variants of quicksort
•Consider a parallel algorithm that solves Pof size 
nwith work W(n) in time Tp(n). 28 Offenburg University of Applied SciencesOptimality in parallel algorithms
Let Tseq(n) be the running time of the best sequential algorithm
•A parallel algorithm is called work-optimal if
–W(n) = T1(n) = O(Tseq(n))
•It is also called work-time-optimal if
–Tp(n) cannot be further improved
•A parallel algorithm is called cost-optimal if
–c(n) = p ∙Tp(n) = O(Tseq(n)) 29 Offenburg UniversityOur example 
•Add n numbers in parallel
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3 30 Offenburg University of Applied SciencesKey figures using the example
•Span: T∞(n) = O(log n)
•Work: T1(n) = O(n) ( n-1 additions)
•Parallelism: T1(n) / T∞(n) = O(n) / O(log n) = O(n / log n)
With p= n/2 processors:
•Time: Tp(n) = O(log n)
•Cost: p∙Tp(n) = n/2 ∙ O(log n) = O(nlog n)
•Speedup: Sn/2= T1/ Tn/2= O(n / log n)
•Efficiency: Sp/ p= Sn/2/ (n/2)= O(1 / log n) 31 Offenburg UniversityOur example
• … is not cost-optimal!
–cost(n) = p ∙ Tp(n) = O(n) ∙ O(log n) = O(n log n)
–The sequential algorithm only takes O(n) time.
–Our previous parallel algorithm (array count) 
with a constant number p of processors was cost-optimal:
•T(n) = n/p+ p= O(n) + O(1) = O(n)
•p= O(1)
–It was also cost-optimal with n1/2 processors:
•T(n) = n / n1/2+ n1/2= O(n1/2)
•p= n1/2= O(n1/2)
•Can the new algorithm be made cost-optimal?
–Yes, you can reduce pre. 32 Offenburg University of Applied SciencesReducing the number of processors
•Idea –Step 1: 
–As with Array-Count:
•Give each processor a (larger) part of the array
•Each processor first adds its part sequentially
–But: 
•Make sure that the time per processor remains in O(log n)
log n log n
Total: n/ log nparts → n/ log nprocessors 33 Offenburg UniversityTime per processor
•Each of the n/log n processors first sums up log n
numbers
•After that, n/log n numbers remain
•These can be added with the n / log n processors using the
original algorithm in time
O(log(n/log n)).
•Total time: T(n) = O(log n) + O(log(n/log n)) 
= O(log n) 34 Offenburg UniversityNew Cost
•Before: Now:
–T(n) = O(log n) T(n) = O(log n)
–P(n) = O(n) P(n) = O(n/log n)
–C(n) = O(n log n) C(n) = O( n/log n) ∙ O(log n)
= O(n) 35 Offenburg University of Applied SciencesParallel prefix sum
•Problem:
–Given:
•Array A of n numbers
a0, a1, a2, …, an-1
•Binary associative operator (e.g. +, MAX, MIN)
–Required:
•Array
a0, (a0a1), (a0a1a2), …, ( a0a1…an-1) 36 Offenburg UniversityExample
•let the addition be and the input be 
5, 3, -6, 2, 7, 10, -2, 8
•Then the output is
5, 8, 2, 4, 11, 21, 19, 27
•Sequential computing time: 
O(n) 37 Offenburg UniversityHow do you calculate this in parallel?
•First of all: why at all?
–often required building block for other parallel algorithms
–“parallel primitive” 38 Offenburg University of Applied SciencesParallel prefix sum
•Two passes through the array
–Bottom-up: calculate the total
–Top-down: distribute partial sums
•Representation as a tree:
5 3 -6 2 7 10 -2 8 39 Offenburg UniversityParallel prefix sum
•First pass: bottom-up
–sum[v] = sum[v.left] + sum[v.right]
–Also works “in -place”
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3 40 Hochschule OffenburgPseudocode
ford=0; d<=log(n)-1; d++ do
fori=0; i<=n-1; i+=2d+1in parallel
a[i+ 2d+1-1] = a[ i+ 2d-1] + a[ i+ 2d+1-1]
5 3 -6 2 7 10 -2 8
5 8 -6 -4 7 17 -2 6
5 8 -6 4 7 17 -2 23
5 8 -6 4 7 17 -2 27 41 Offenburg University of Applied SciencesParallel prefix sum
•Second pass
•Idea: 
–Top-down calculation to generate all prefix sums
•Notation:
–pre[v] stands for the sum in each node v
•pre[root] = 0
–0 is the neutral element of +, i.e. x + 0 = x for all x
–For a different operator, the corresponding neutral 
element must be selected (e.g. -∞ for MAX or ∞ for MIN ) 42 Offenburg UniversityParallel prefix sum
•Second round: top-down
–pre[v.right] = sum[v.left] + pre[v]
–pre[v.left] = pre [v]
Input:
• This is “almost” the desired solution!5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 05 3 -6 2 7 10 -2 8
27 43 Offenburg UniversityInclusive vs. exclusive prefix sum
5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0 275 3 -6 2 7 10 -2 8
Inclusive prefix sum Exclusive prefix sum 44 Hochschule OffenburgPseudocode
a[n-1] = 0
ford=log(n)-1; d>=0; d-- do
fori=0; i<=n-1; i+=2d+1in parallel
temp= a[i+ 2d-1] 
a[i+ 2d-1] = a[ i+ 2d+1-1]
a[i+ 2d+1-1] = temp+ a[i+ 2d+1-1]
5 8 -6 4 7 17 -2 23
5 8 -6 0 7 17 -2 4
5 0 -6 8 7 4 -2 21
0 5 8 2 4 11 21 190 45 Offenburg University Complexity
•The algorithm requires 
–Time: O(log n)
–Processors: n/2 = O(n)
•This can be reduced to O(n/log n) Exercise! 46 Offenburg UniversityIs the algorithm correct?

Definition:

Node x is called the predecessor of node y if x is reached in the tree during a

preorder traversion (= depth-first search). 47 Offenburg UniversityCorrectness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that correspond to its predecessors in the 
original tree.
5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 48 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
Proof (by structural induction):
1. The statement holds for the root (with value 0) because it is the 
first visited node and has no predecessors.
2. To show: If the statement holds for any node v 
it also holds for its two children v.left and v.right . 49 Offenburg University Structural induction
•The left child node of v has exactly the same 
predecessor leaves as v itself, namely those in region A.
•And by definition, pre[v.left ] = pre[v]Av 50 Offenburg University Structural induction
•The right child node of v has two sets of 
predecessors
–Region A: corresponds to pre[ v]
–Region B: corresponds to sum[ v.left]
•By definition, pre[v.right ] = pre[v] + sum[v.left ]A Bv 51 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
If it is true for every node, it is especially true for the 
leaves.
This corresponds exactly to what we want to show.5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 3 Offenburg UniversityTheory vs. Practice
• “There’s nothing more practical than a good theory” 
(Kurt Lewin)
•Our previous assumptions:
–Time complexity is given as a function of the 
processor complexity
–We have as many processors “as we need” 
•e.g. O(n/ log n), O(n), …
•But:
–In practice, you usually only have a 
constant number p of processors available 4 Offenburg University of Applied SciencesParallelization with pprocessors
• Let ...
–T1the time for the sequential execution of the algorithm
–Tpthe time for the parallel execution with pprocessors
–T∞the time for the maximum parallel execution
•Then the following applies:
–Tpg T1/p (limited by linear speedup)
» The linear speedup would be Θ(p)
–Tpg T∞ (limited by maximum speedup)
» The maximum possible speedup is T1 /T∞ 5 Offenburg University Brent's Theorem (also: Brent's Law)
An algorithm A can be executed with p processors in parallel in time
㕇㕝≤㕇1
㕝+㕇∞
.
Proof of assignment of jobs to processors 6 Offenburg University of Applied Sciences 7 Offenburg UniversityExample
•Parallel summation (general reduction)
–T1= O(n)
–T∞= O(logn)
–Tpf O(logn)+n/p 8 Offenburg University Conclusion 1
•From the lower bounds Tp≥ T1/p and Tp≥ T∞
and Brent's theorem follows:
max㕇1
㕝, 㕇∞ ≤㕇㕝≤2∙ max㕇1
㕝, 㕇∞
㕐 ≤ 㕇㕝≤2∙ 㕐 9 Offenburg University Conclusion 2
•With the number p= O(T1/ T∞) of processors 
the (asymptotically) optimal time, 
i.e. a maximum speedup can be achieved
•Proof: 㕇㕝=㕇1
㕝+ 㕇 ∞=㕂(㕇 1)
㕂T1
T∞+ 㕇 ∞
= 㕂(㕇1
T1
T∞) + 㕂(㕇 ∞) = 㕂(㕇 ∞) 10 Offenburg UniversityParallel prefix sum
•Problem:
–Given:
•Array A of n numbers
a0, a1, a2, …, an-1
•Binary associative operator (e.g. +, MAX, MIN)
–Required:
•Array
a0, (a0a1), (a0a1a2), …, ( a0a1…an-1) 11 Offenburg UniversityExample
•let the addition be and the input be 
5, 3, -6, 2, 7, 10, -2, 8
•Then the output is
5, 8, 2, 4, 11, 21, 19, 27
•Sequential computing time: 
O(n) 12 Offenburg UniversityHow do you calculate this in parallel?
•First of all: why at all?
–often required building block for other parallel algorithms
–“parallel primitive” 13 Offenburg University of Applied SciencesParallel prefix sum
•Two passes through the array
–Bottom-up: calculate the total
–Top-down: distribute partial sums
•Representation as a tree:
5 3 -6 2 7 10 -2 8 14 Offenburg UniversityParallel prefix sum
•First pass: bottom-up
–sum[v] = sum[v.left] + sum[v.right]
–Also works “in -place”
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3 15 Hochschule OffenburgPseudocode
ford=0; d<=log(n)-1; d++ do
fori=0; i<=n-1; i+=2d+1in parallel
a[i+ 2d+1-1] = a[ i+ 2d-1] + a[ i+ 2d+1-1]
5 3 -6 2 7 10 -2 8
5 8 -6 -4 7 17 -2 6
5 8 -6 4 7 17 -2 23
5 8 -6 4 7 17 -2 27 16 Offenburg University of Applied SciencesParallel prefix sum
•Second pass
•Idea: 
–Top-down calculation to generate all prefix sums
•Notation:
–pre[v] stands for the sum in each node v
•pre[root] = 0
–0 is the neutral element of +, i.e. x + 0 = x for all x
–For a different operator, the corresponding neutral 
element must be selected (e.g. -∞ for MAX or ∞ for MIN ) 17 Offenburg UniversityParallel prefix sum
•Second round: top-down
–pre[v.right] = sum[v.left] + pre[v]
–pre[v.left] = pre [v]
Input:
• This is “almost” the desired solution!5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 05 3 -6 2 7 10 -2 8
27 18 Offenburg UniversityInclusive vs. exclusive prefix sum
5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0 275 3 -6 2 7 10 -2 8
Inclusive prefix sum Exclusive prefix sum 19 Hochschule OffenburgPseudocode
a[n-1] = 0
ford=log(n)-1; d>=0; d-- do
fori=0; i<=n-1; i+=2d+1in parallel
temp= a[i+ 2d-1] 
a[i+ 2d-1] = a[ i+ 2d+1-1]
a[i+ 2d+1-1] = temp+ a[i+ 2d+1-1]
5 8 -6 4 7 17 -2 23
5 8 -6 0 7 17 -2 4
5 0 -6 8 7 4 -2 21
0 5 8 2 4 11 21 190 20 Offenburg University Complexity
•The algorithm requires 
–Time: O(log n)
–Processors: n/2 = O(n)
•This can be reduced to O(n/log n) Exercise! 21 Offenburg UniversityIs the algorithm correct?

Definition:

Node x is called the predecessor of node y if x is reached in the tree during a

preorder traversion (= depth-first search). 22 Offenburg UniversityCorrectness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that correspond to its predecessors in the 
original tree.
5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 23 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
Proof (by structural induction):
1. The statement holds for the root (with value 0) because it is the 
first visited node and has no predecessors.
2. To show: If the statement holds for any node v 
it also holds for its two children v.left and v.right . 24 Offenburg University Structural induction
•The left child node of v has exactly the same 
predecessor leaves as v itself, namely those in region A.
•And by definition, pre[v.left ] = pre[v]Av 25 Offenburg University Structural induction
•The right child node of v has two sets of 
predecessors
–Region A: corresponds to pre[ v]
–Region B: corresponds to sum[ v.left]
•By definition, pre[v.right ] = pre[v] + sum[v.left ]A Bv 26 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
If it is true for every node, it is especially true for the 
leaves.
This corresponds exactly to what we want to show.5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University Topics
•Thread pools 3 Offenburg University of Applied SciencesParallelization: Tasks and threads
•Task, completed work unit
•To be completed (largely) independently of other tasks
•Examples
–Processing a request, e.g. in a web server
–Calculation, e.g. image processing
•Task structure of an application
–Identify tasks
–Break down tasks:
•To be processed sequentially
•To be processed in parallel 4 Offenburg UniversityTasks and threads
Task
•Task thread
• “Completer” of tasks
Assignment can be done in different ways:
•Sequential: one thread for all tasks
•Thread per task: a separate thread for each task
•Thread pool: a set of threads completes a set of
tasks
•Note: Application is at most as fast as its

longest-running thread (cf. span = “critical path length”) 5 Offenburg University Sequential assignment: 1 thread 
•Advantages
–Easy to implement
–No synchronization required in the code
–No overhead for thread management
•Disadvantages
–Poor throughput (e.g. with web servers)
–Tasks may have to wait for a long time unnecessarily
–Parts of the hardware may be unused unnecessarily 6 Offenburg University of Applied SciencesOwn thread per task
•Advantages:
–Improved throughput
–Hardware resources are used
•Disadvantages:
–Code must be thread-safe
–Overhead, especially with many tasks:
•Permanent creation/destruction of threads
•Resource consumption (e.g. stack area)
•See example merge sort, edit distance, heapify
–Possible stability problems 7 Offenburg University Compromise: Thread pool
•Idea: use a limited number of threads that is not exceeded
•Use the advantages of both approaches, minimize disadvantages
–uses resources (e.g. multi-core CPUs)
–conserves resources (e.g. memory for virtual address space)
•Disadvantage
–work to manage the pool
–code must be thread-safe
•The executor framework in java.util.concurrent
provides interfaces and classes for thread pools 8 Offenburg University Executor framework
•Interface that describes the execution of tasks
•An executor (i.e. a class that implements the interface) executes tasks using threads
–Can assign in any way (thread policy), e.g.
•Single-threaded
•Thread per task
•Thread pool
–Creation and reuse of threads is left to the executor
•Advantage: decoupling of the application from the thread policy
–Simple modification of the thread policy possible 
–Application does not have to worry about thread policy at all 9 Offenburg University of Applied SciencesExecutor framework as 
intermediate layer
•simple standardized means 
for creating, managing 
and scheduling threads
•supports the decoupling
–of tasks 
(application-specific)
from
–the configuration of the execution 
(depending on the 
runtime environment)
•preferable for software development over the 
direct use of threads
Task
Task
Task
Task
Task
Task
Executor framework
Java Virtual Machine
Operating System
Task
CPU CPUThreads 10 Offenburg UniversityExecutor
•Executor:
–public void execute(Runnable r)
•Example 1:
–Execution of each task in its own thread:
classThreadPerTaskExecutor implements Executor { 
public void execute(Runnable r) { 
newThread(r).start();
}
}
•Tasks must be programmed in a thread-safe manner! 11 Offenburg UniversityExecutor
•Example 2:
–Execution of the task in the calling thread:
classDirectExecutor implements Executor { 
public void execute(Runnable r) { 
r.run(); 
} 
}
•What about the thread safety of tasks here? 12 Offenburg UniversityExecutor
•Example 3:
–Serial execution of all tasks guaranteed in the same thread:
public class OneThreadExecutor extends Threadimplements Executor {
private final Queue<Runnable> tasks = new 
LinkedBlockingQueue<Runnable>();
public void run() {
while(true) // or condition for termination
try {
tasks.take().run();
} catch(InterruptedException e) { ... } 
}
public synchronized void execute(Runnable r) {
tasks.offer(r);
}
} 13 Hochschule OffenburgExecutor Framework
•Comes with most important types of Executor
–ThreadPoolExecutor
–ScheduledThreadPoolExecutor für 
•delayed or
•periodic execution of tasks
•Even better:  ExecutorService extends Executor
–Additional methods for managing, e.g.
•shutdown()
•isTerminated()
•submit()
•... 14 Hochschule OffenburgThreadPoolExecutor
•Components
–Queue Qfor incoming tasks
–Threads: take tasks from Qand execute them
•Number tof threads in Pool
–Core pool size : desired number of threads, only exceeded if
necessary
–Maximum pool size : maximum number of threads, will never be
exceeded 15 Hochschule OffenburgHow it works
•New task comes in:
–Ift< core pool size :
•Create a new thread for the task
–Ift≥ core pool size und t< maximum pool size
•Put task on Q
–Ift> core pool size und t< maximum pool size und Q voll
•Create new thread
–Ift≥ maximum pool size und Q voll
•?  (find out!)
•Ift> core pool size andQis empty
•Terminate next idle thread 16 Hochschule OffenburgTasks with result
•So far: tasks for Executor must be Runnable s
–No (explicit) result –public void run()
–Any result must be fetched manually (e.g. with get method)
•For tasks with result: Callable<V>
–Interface: publicVcall()
–Like Runnable, but returns result of type V
–Can be executed by an Executor
–Call withexecutor.submit(task) 
•How to retrieve the result? 17 Hochschule OffenburgResults
•Problem: 
–Result has not been calculated yet at the time of task submission
–Return value of executor.submit(task) must represent a 
future object of Type V
•Solution:Future<V>
–Interface: publicVget()
–Represents the result (of Type V) of the execution
•of a  Callable<V>
•by an Executor
–Call:future = executor.submit(task) 18 Hochschule OffenburgCallable and Future
•Application
–Submits a Callable<V> o anExecutorService
–Receives a Future<V> immediately
–Result can be retrieved from Future
•Executor
–Receives a Callable<V>
–Returns a Future<V> 
–Computes the task with the help of threads
–Hands the result to Future
•How to know when the result is there? 19 Hochschule OffenburgMethods of Future
–get()
•Blocks the calling thread until result is there
–get(long timeout, TimeUnit unit)
•Waits for result or until time is up (whatever comes first)
–isDone()
•true if and only if task has ended (no matter how!)
–isCancelled()
•true if and only if task has been cancelled
–boolean cancel(boolean b)
•Tries to cancel the corresponding task
(with/without interruption)
•true if and only if successful 21 Hochschule Offenburgjava.util.concurrent.atomic
•Atomic data types
–AtomicBoolean
–AtomicInteger undAtomicIntegerArray
–AtomicLong undAtomicLongArray
–AtomicReference<V> undAtomicReferenceArray<E>
– …
•Atomic methods for above types
–compareAndSet()
–addAndGet()
–addAndGet() und getAndAdd()
–incrementAndGet() undgetAndIncrement() 
– … 23 Offenburg Universityjava.util.concurrent.locks
•Interfaces:
–Lock
•tryLock (…)
•lock()
•unlock()
–ReadWriteLock
–Condition
•Classes:
–ReentrantLock
–ReentrantReadWriteLock 24 Hochschule OffenburgSynchronization classes
•CountDownLatch
–Barrier with countdown which must be counted down explicitly
(usingcountDown() )  Not reusable.
•CyclicBarrier
–Barrier for a number of threads, reusable.
•Exchanger
–Barriere for two threads to exchange data
•Semaphore
–Implementation of a typical counting semaphore with key
methods acquire (…) andrelease (…) Parallel programming and 
algorithms
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University Topics
•Thread pools
–Disadvantages of standard thread pools
–Fork/Join framework
•Parallel algorithms
–Parallel maximum determination 3 Offenburg University Thread pools
•Thread pools of the executor framework are not ideal for
–recursive tasks
•Potentially high number of threads
•Risk of deadlocks (see merge sort) if limited
–highly dependent tasks
•Either all or no tasks
•Risk of deadlocks if pool is too small
•One reason: queuing all tasks in one queue
–order is often crucial! 4 Offenburg University Alternative: Fork/Join Framework
•Suitable for algorithms that
–consist of a large number of sub-actions 
•that can be processed independently of one another (per “level”)

–contain as little synchronization code or blocking 
actions as possible (except for task-subtask dependencies)
•Typical applications
–Divide-and-Conquer algorithms
–Parallel algorithms on arrays 
•Worthwhile on very large arrays if you have a lot of 
processors/cores available 5 Offenburg UniversityFork/Join: Task constructs
•RecursiveAction
–Class for recursive action (without return of result)
–Method: voidcompute()
–Calls (independent) sub-actions of the same type
•Instances of the same class are used for this
–Start of the sub-actions (scheduling in the pool) with
voidinvokeAll(... sub-actions... )
•Method waits for return
–Alternative: asynchronous start of a single sub-action
sub-action .fork() 6 Hochschule OffenburgBeispiel
•Array-Increment: Erhöhe jedes Element eines Arrays um 1 
classIncrementAction extends RecursiveAction { 
final long [] array; 
final int lo, hi; 
IncrementAction (long[] array, intlo, inthi) { 
this.array = array; 
this.lo = lo; this.hi = hi; 
} 
protected void compute() { 
if(hi - lo < THRESHOLD ) { // eine sinnvoll gewählte Grenze
for(inti = lo; i < hi; ++i) array[i]++; 
} else{ 
intmid = (lo + hi) >>> 1; 
invokeAll( newIncrementAction (array, lo, mid), 
newIncrementAction (array, mid, hi)); 
} 
} 
} 7 Offenburg UniversityFork/Join: Task constructs
•RecursiveTask< V> 
–Class for recursive action with result return of type V
–Method: Vcompute()
–Calls (independent) sub-tasks of the same type
•Instances of the same class are used for this
–Start the sub-tasks (scheduling in the pool) with
voidinvokeAll(... Sub-Tasks... )
•Method waits for result return
–Alternative: asynchronous start of a single sub-action
subtask.fork()
–Fetch the results
Vjoin() 8 High School Offenburg Theater
classFibonacci extends RecursiveTask<Integer> { . 
final int n; 
Fibonacci(intn) { 
this.n = n; 
} } 
Integer compute() { 
if ( n <= 1 ) return n ; 
Fibonacci f1 = newFibonacci(n-1);
f1.fork(); 
Fibonacci f2 = newFibonacci(n-2); 
return f2 . compute ( ) + f1 . join ( ) ; 
} } 
} } 9 Offenburg University of Applied SciencesHow does the ForkJoinPool work?
• A ForkJoinPool looks “from the outside” like any other
thread pool:
–There is a task queue in which new tasks are submitted by applications…
–… and threads that pick up and execute these taskssubmit
submit
taketake 10 Offenburg University of Applied SciencesThread-specific task queues
•Special feature:
Each thread also has its own task queue
–Sub-tasks of the currently executed task are inserted here…
–… and also executed primarily by the thread!submit
submit
taketakepush
pushpop
pop 11 Offenburg University of Applied SciencesThread-specific queues
•Advantages
–A started (overall) task is completed before the thread starts a completely new task
–The central queue is not burdened with many small sub-tasks, 
but only contains (relatively) independent tasks
–This means fewer conflicts (blocking) between threads when accessing 
the queues (both central and thread-specific)
•Access is implemented like a stack (LIFO)
–The last task inserted is processed first
–Subtasks are processed before the parent tasks
–Avoidance of deadlocks 12 Offenburg University of Applied SciencesThread-specific queues
•BUT:
–A started (overall) task is completed before the thread starts a completely new task
•BUT: What do threads do that are already finished while others are still busy with sub-tasks?

–Central queue is not burdened with many small sub-tasks, but only contains (relatively) independent tasks
•BUT: What if there is only 1 large task that is split up (e.g. merge sort)???

–This means fewer conflicts (blocking) between threads when accessing the queues (both central and thread-specific)
•BUT: Danger of unemployed threads because central and own queues are empty! 13 Offenburg University of Applied SciencesUnemployed threads
• If a thread's own queue is empty...
–First look in the task queues of other threads...
–... and take work away from them if possible (work stealing).submit
submit
taketakepush
pushpop
popsteal 14 Offenburg University Work stealing
•Advantages:
–A started (entire) task is only completed before a completely new task is started – also by other threads
–Threads also complete fine-grained tasks using a division of labor
•Possible disadvantage:
–Access conflicts with the “owner” thread of the queue
–Fine-grained tasks are prioritized (LIFO), so many accesses are likely
•This can be prevented: 
–by implementing the thread’s own queue as a deque. 15 Offenburg UniversityDeque (double ended queue)
•Allows operations at the beginning (first) and at the end (last)
–Owner thread only works at the beginning of the queue
•push: addFirst(…)
•pop: removeFirst()
–Foreign threads only remove at the end 
•steal: removeLast()
push
pushpop
popsteal: removeLast() 16 Offenburg University Conclusion: Fork/Join
•Not necessarily more efficient than ThreadPoolExecuter!
–Additional overhead for managing queues and work-
stealing
–If the input tasks are very balanced, work-stealing often does not take place. 
–If this is known in advance, you can be more efficient with a “normal” 
thread pool or manual threading
•Work-stealing ensures finer granular load distribution
–Well suited if a large task would use up a thread while other threads have to wait
–e.g. with unevenly distributed input that can be broken down further
–Fork/Join is preferable for recursively formulated tasks 17 Offenburg University Parallel maximum
•Problem:
Find the largest number in an array A of n numbers
or the index (position) of the largest number.
•Sequential solution: O(n)
•Claim: We already know how to determine the maximum
of n numbers in parallel in time O(log n)
–MAX is like + a binary associative operator
–Therefore the same algorithm as for the simple sum can be used, only with MAX instead of + 18 Offenburg UniversityParallel maximum
•Time T(n) = O(log n)
•Can it be done even faster?5 3 -6 2 7 10 -2 85 2 10 85 1010
P1P1P1
P2 P3 P4P3 19 Offenburg UniversityParallel maximum
•Claim:
You can even determine MAX in time O(1)!
•However: 
We need a lot of processors (and extra space) for this 20 Offenburg UniversityParallel maximum
A:
B:
•Start: 
–Create an array B of length n
–Initialize it with B[k] = 1 for all k
–This takes O(1), and we “only” need n processors5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1 21 Offenburg University of Applied SciencesParallel maximum
•Step 1
–Use n processors for each element A[ i] 
–We consider an element A[ i] and its n processors 
Pi,0, Pi,1, …, Pi,j, …, Pi,n-1
–Each processor Pi,j compares A[ i] with A[ j]:
if(A[i] < A[j])
B[i] = 0
else
// do nothing 22 Offenburg UniversityExample: P4,0
i= 4
5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1
Do this in parallel for all j. j = 0 23 Offenburg University of Applied SciencesExample: P4,jfor all j
i= 4
5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1 0
Do this in parallel for all i. 24 Offenburg UniversityExample: all Pi,j
5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 25 Offenburg University of Applied SciencesParallel maximum
•Step 2:
–Observation: 
At the end of step 1, B[k] = 1 if and only if A[k] 
is the maximum (or a maximum) element
–Use n processors, one for each entry in B
ifB[i] == 1
result_index = i (or result_max = A[i])
(depending on what is expected as the answer)
•Observation: In both steps, simultaneous 
writing (concurrent write) may be required 26 Offenburg University of Applied SciencesSimultaneous writing
•Can there be writing conflicts?
–In step 1:
If writing is done simultaneously, all processors write 
the same thing (“false” race condition)
→possible from Common CRCW
–In step 2:
Since the index (position) of the maximum is written, 
different threads could write different things
→possible only from Arbitrary CRCW (real race condition)
–However, if the value of the maximum is written directly, 
this step also goes with Common CRCW 27 Offenburg University Complexity
•Time: T(n) = O(1)
–It doesn’t get any better than this! ☺
•Processors: p(n) = O(n²)
–Not good at all! 
•Costs: c(n) = O(n²)
–Not cost-optimal  28 Offenburg University Can it be done better?
•Goals:
–More realistic number of processors (less than n)
–Still runtime better than O(logn)
–Cost optimality
•Approach:
–Combine the two previous algorithms
1. T(n) = O(log n) c(n) = O(n)
2. T(n) = O(1) c(n) = O(n²)
–Goal
•T(n) = O(log log n) c(n) = O(n) 29 Offenburg UniversityAlgorithm with time O(log log n)
5 3 -6 2 71
0-2 8 41
56 -5 9 11
1-3 30 Offenburg UniversityAlgorithm with time O(log log n)
•Example: k= 2, n= 222= 16
•Depth of the tree: k + 1
•Since n=22k, the following applies: k= O(log log n)
•The number of nodes on level i is 22k-2k-ifor 0 ≤ i≤ k5 3 -6 2 71
0-2 8 41
56 -5 9 11
1-3 31 Offenburg UniversityAlgorithm
•Proceed level by level
–Start at the nodes directly above the leaves
–In each level: 
•Calculate the maximum of all child nodes for each node in 
constant time using the O(1) algorithm
–Use this to calculate the entire level in parallel in constant time 
•The only important thing is: How many processors do you need for this?
•Then the total time is proportional to the number of 
levels (depth of the tree), i.e. in O(log log n) 32 Offenburg UniversityCosts per level
•Every node on level ihas c= 22k-i-1children
•The processor effort per node is O(c²) = O((22k-i-1)²)
•On level ithere are a total of 22k-2k-inodes that we want to calculate in parallel (→time O(1) per level)
•This means that the total costs Ci(n)for level i are
O((22k-i-1)²) ∙ 22k-2k-i= O(22k) = O(n)
•Observation: The costs Ci(n) are independent of i,

so the same for every level! 33 Hochschule OffenburgNebenrechnung
(22k-i-1)2· 22k–2k-i
= 22·2k-i-1· 22k–2k-i
= 22k-i-1+1· 22k–2k-i
= 22k-i· 22k–2k-i
= 22k-i+ 2k–2k-i
= 22k
=n 34 Offenburg University Number of processors
•Start from the top
–Level 0 (root)
•1 node with n1/2 children →1 · (n1/2))2= n processors
–Level 1
•n1/2 nodes with n1/4 children each →n1/2· (n1/4))2= n processors
–Level 2
•n3/4 nodes with n1/8 children each →n3/4· (n1/8))2= n processors
–Level 3
•n7/8 nodes with n1/16 children each →n7/8· (n1/16))2= n processors
–…
–Level above the leaves:
•n/2 nodes with 2 children →works with n/2 processors 35 Offenburg UniversityExample 1
•n= 16 (k= 2)
•Start from the bottom
–Level above the leaves:
•8 nodes/processors for a maximum of 2 leaves each
–Level above:
•4 nodes with 2 children each → 16 processors
(4 processors)
–Level above:
•1 node with 4 children → 16 processors 36 Offenburg UniversityExample 2
•n= 256 (k= 3)
•Start from the bottom
–Level above the leaves:
•128 nodes with 2 children (leaves) → 128 processors
–Level above:
•64 nodes with 2 children → 256 processors
(or: 64 processors)
–Level above:
•16 nodes with 4 children → 256 processors
–Level above
•1 node with 16 children → 256 processors 37 Offenburg University Total costs
•For O(log log n) levels with cost O(n) each, the
total effort (costs)
c(n) = O(nlog log n) →not cost-optimal
•Improvement by reducing the input
(algorithmic cascading)
–We already know this from sum, parallel prefix etc.
–First divide the input into suitably large parts
–Calculate the maximum for each part sequentially

(but ​​all parts in parallel)
–Use the algorithm just presented on the result 38 Offenburg UniversityPhase 1: Input reduction
Reminder: Brent's theorem: 㕇㕝≤㕇1
㕝+㕇∞
Conclusion: With the number p= O(T1/ T∞) of processors, the 
(asymptotically) optimal time, i.e. a maximum speedup, can be achieved! 39 Offenburg UniversityPhase 2 40 Offenburg University Total effort
•Phase 2:
–Cost O(n)
–Time O(log log n)
•Phase 1:
–Cost O(n)
–Time O(log log n)
Conclusion (theorem):
The maximum of n elements can be calculated in time O(log log n) 
and cost O(n) on a CRCW PRAM.
(It can be shown that this is also the optimum.) 41 Offenburg University of Applied SciencesParallel algorithms from Java 1.8
•Parallel methods for arrays:
–Sorting, setting values, etc.
–implemented in java.util.Arrays
•Java 1.8 has a default fork/join pool that the above

parallel methods use
–Access: ForkJoinPool.commonPool()
•Streams: flexible calculations on large data sets
–Supports parallelization: ParallelStream class
–Caution: not everything there is actually calculated in parallel! GPU 
Programming 
Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous 
Offenburg University 
Source: Nvidia Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Motivation 
2 Mathematics 
Numerics 
Computational Finance 
Simulations (Monte Carlo) 
Risk analysis 
Derivative valuation 
Signal processing 
Video analysis and processing 
Sensor data 
Deep learning 
Training neural networks Physics 
Particle simulations 
Biology, bioinformatics 
Sequence alignment 
Protein folding 
Cryptology 
Password security 
Meteorology 
Weather and climate 
simulations 
Mining 
Bitcoin etc. 
and much more. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPGPU Motivation 
3 General Purpose Computation on Graphics Processing Unit 

GPUs offer very high computing power per euro 
(also due to lower energy consumption per FLOP/s). 

GPUs can execute MAC (multiply-accumulate) operations in one cycle (number cruncher). 

Sophisticated SIMD hardware for data-parallel applications 

GPUs are accessible and built into many workstations and laptops. Every specialist retailer has them in their range. 

Many computationally intensive applications can be significantly 
accelerated with GPUs. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Review 
4 Shader programming with OpenGL 1992, Glide 1996 or other 
shader programming languages ​​or specifications are cumbersome. 

In 2007, CUDA (Compute Unified Device Architecture) was released by Nvidia for 
exclusively Nvidia GPUs and makes programming much easier. 

In 2009, OpenCL (Khronos Group) was released as a standard, so 
it is possible to program on non-Nvidia graphics cards. 

In 2011, OpenACC (PGI, Cray, Nvidia, CAPS) was released so that 
CPU and GPU programming is portable and can be used in heterogeneous environments 
(various manufacturers: CPU, CPU-GPU, GPU). Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPUs Today 
5 •“General-Purpose Computation on Graphics 
Processing Units< 
•2009 
•First GPU Technology Conference from NVIDIA 
•2010 
•Fastest supercomputer “Tianhe -1A< 
with 7,168 GPUs (NVIDIA Tesla M2050) 
•2012 
•Fastest supercomputer “Titan< 
with 18,688 GPUs (NVIDIA Tesla K20X) 
•2014 
•GPUs in 17 of the top 100 
supercomputers 
•2019 
•Summit supercomputer with 27,648 GPUs and 9,216 CPUs Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPGPU and algorithms 
6 First opportunity to implement massively (data-)parallel 
algorithms on inexpensive hardware 
Theory of parallel algorithms goes back 
to the 1980s and beyond 
At that time only a few special computers 
Scalability of the number of processors with the data size 
in principle possible 
e.g. through multi-GPU 

Relatively simple implementation 
Primarily through CUDA and OpenCL Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPU structure 
7 Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPU structure 
8 Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Hardware: Überblick (GTX 280) 
9 Double Precision Special Function Unit (SFU) 
TP Array Shared Memory Streaming Processor (SP) 
Thread Processor (TP) 
FP / Integer Multi-banked  
Register File 
Special 
Ops Streaming Multiprocessor (SM) 
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryMain 
Memory 
Thread Manager 
30 SMs pro chip Core Processor 
Chip Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Thread processor (CUDA Core) 
10 •Floating point / Integer 
Unit 

•Move, compare, logic, 
branch 

•Local registers 

•"Stripped down< 
processor core or 
highly developed ALU FP / Integer Multi-banked 
Register File 
Special 
Ops Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor 
11 •8 / 32 / 192 / 128 / 64 thread processors (CUDA cores) 
•Double-precision unit 
(IEEE-754 compliant) 
•16 KB / 64 KB shared memory 
•Shared instruction unit and instruction cache 
•Simultaneous management of a large number of threads 
•Hundreds of threads simultaneously 
•Hardware scheduler with context switching with almost no overhead 
•NVIDIA GTX 280: 30 multiprocessors (8 CUDA cores each) 
•NVIDIA Tesla P100: 56 multiprocessors (64 CUDA cores each) Double Precision Special Function Unit (SFU) 
TP Array Shared Memory Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor – Maxwell generation 
12 •Up to 16 multiprocessors per GPU 
(e.g. GeForce GTX 980) 
•Per multiprocessor: 
•128 thread processors (CUDA cores) 
•64-96 KB shared memory 
•64 KB register file 
•Up to 64 active warps (32 threads each) 
 32768 active threads Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor – Pascal generation 
13 •Up to 60 multiprocessors per GPU 
(e.g. GeForce GTX 1080) 
•Per multiprocessor: 
•64 thread processors (CUDA cores) 
•64 KB shared memory 
•64 KB register file 
•Up to 64 active warps (32 threads each), 
 122880 active threads Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor – Turing generation 
14 •Up to 72 multiprocessors per GPU 
(e.g. Quadro RTX 6000) 
•Per multiprocessor: 
•64 thread processors (CUDA cores) 
•64 KB shared memory 
•64 KB register file Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Processor array 
15 •Scalable by design 
•Multiprocessors form a scalable processor array 
•Varying number of multiprocessors depending on GPU 
(automatic scaling execution) 

•Architecture has an impact on programming 
•Each multiprocessor is a SIMD-like 
unit Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University SIMD – Single instruction, multiple data 
16 •All processors execute the same instruction in each step 
but on different data 

•see MIMD (multiple instruction, multiple data) 
•Every processor can execute any program 
•Standard multicore architecture (dual core, quad core, ...) 

•If everyone always executes the same instruction, how can 
one then implement branches (if ... else)? Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Conditionals/branches in SIMD 
17 Elimination of branching 
•Parallel evaluation of the 
predicate for all elements 
•Parallel calculation of the 
TRUE branch for all 
elements 
•Parallel calculation of the 
FALSE branch for all 
elements 
•Parallel selection of the 
correct value for all 
elements (how?) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University of Applied Sciences Storage system 
18 Up to 24 GB memory supported 
(e.g. Turing Quadro P6000) 

Generic load/store model 
(Concurrent Read, Concurrent Write) 
ATTENTION: arbitrary CRCW 
Every processor can write to any memory location 
Conflicts (race condition) possible 
  Programmer is responsible! Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory types 
19 Global memory 
Write/Read 
Large: hundreds of MB to 32 GB/GPU 
Slow (600+ cycles) 

Texture memory 
Physically the same as global memory 
Read-only 
Cached for streaming (2D neighborhoods) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory types 
20 Constant memory 
Read-only 
64kB per chip 
Very fast (1-4 cycles) 

Shared memory (partly shared with L1 cache) 
Read/write 
16kB - 64kB per multiprocessor 
Very fast, provided DRAM bank conflicts are avoided 

Register 
Read/write 
16kB-64kB per multiprocessor; max. 255 registers per thread 
Fastest memory Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory types 
21 The performance of applications depends crucially on 
the efficient use of the memory hierarchy! 

For example 
Shared memory as a cache for global memory 
Constant memory for transferring larger parameter sets 
Efficient use of registers 
… Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Programming model: CUDA 
(Compute-Unified Device Architecture) 
22 Scaling to 
Hundreds to thousands of cores 
Tens of thousands to millions of threads 

Programmer should be able to concentrate on parallel algorithms 
No special parallel programming language 
C for CUDA plus runtime API 

Support for heterogeneous systems (i.e. CPU+GPU) 
CPU & GPU are separate devices with separate DRAMs 
Today transparent (unified address space, unified memory) 
CPU = <Host=, GPU = <Device= Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Programmiermodell: CUDA 
23 NVCC C/C++ CUDA 
Application 
PTX to Target 
Translator 
   GPU    …     GPU  
Target device code PTX Code Generic 
Specialized CPU Code Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Parallel abstractions in CUDA 
24 
Hierarchy of concurrent threads 

Lightweight synchronization primitives 

Shared memory model for cooperating 
threads Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Hierarchy of concurrent threads 
25 Parallel kernels with many threads 
All threads execute the same sequential 
program 

Threads are grouped in thread blocks 
Blocks are arranged 1-, 2- or 3-dimensionally 
Threads in the same block can cooperate 

Threads/blocks have unique IDs 

Thread blocks are grouped in a grid 
Grid is arranged 1-, 2- or 3-dimensionally Thread t 
t0 t1 … tB Block b Prof. Dr. rer. nat. Tobias Lauer Dr.-Ing. Alexander Vondrous Offenburg University Grid structure 
26 Thread blocks are arranged as a 
1- or 2-dimensional 
grid 

This makes it easy to adapt the grid structure to 
the problem structure 

Each thread is defined by 
its position in the block and 
the position of the Blocks in the 
Grid clearly 
identifiable t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University CUDA programs 
27 GPU is coprocessor 
Host code and device code 
Application-controlled calls to parallel functions 
No "strict< SIMD, but 
SPMD – Single Program, Multiple Data 
Between different warps 
(Warp = thread bundle that is scheduled as a unit) 
The same program (but not necessarily the same instruction) is 
executed simultaneously on all data 
SIMT – Single Instruction, Multiple Thread 
Within a warp 
Each thread has its own registers Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Typical CUDA program structure 
28 
Application is split into sequential 
and parallel computable 
parts/tasks 

For each parallelizable part 
Memory is allocated to GPU(s) 
Required data is copied from RAM to GPU memory 
CUDA kernel(s) started 
Result(s) copied back to host 
Memory released Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Beispiel: Vektoraddition 
29 // Compute vector sum C = A+B 
// Each thread performs one pair-wise addition 
__global__  void vecAdd(float* A, float* B, float* C) 
{ 
    int i = threadIdx.x  + blockDim.x  * blockIdx.x ; 
    C[i] = A[i] + B[i]; 
} 
 
int main() 
{ 
    // Run N/256 blocks of 256 threads each 
    vecAdd<<< N/256, 256>>> (d_A, d_B, d_C); 
} Device Code Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Beispiel: Vektoraddition 
30 // Compute vector sum C = A+B 
// Each thread performs one pair-wise addition 
__global__  void vecAdd(float* A, float* B, float* C) 
{ 
    int i = threadIdx.x  + blockDim.x  * blockIdx.x ; 
    C[i] = A[i] + B[i]; 
} 
 
int main() 
{ 
    // Run N/256 blocks of 256 threads each 
    vecAdd<<< N/256, 256>>> (d_A, d_B, d_C); 
} Host Code Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Synchronization 
31 Threads within a block can be synchronized with barriers 
 … Step 1 … 
__syncthreads(); 
… Step 2 … 

Coordination using atomic memory access operations 
e.g. incrementing a shared pointer with atomicInc() 

Implicit barrier between dependent kernels 
 vec_minus<<<nblocks, blksize>>>(a, b, c); 
vec_dot<<<nblocks, blksize>>>(c, c); 

ATTENTION: Differences between architectures (compute capability)! Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Compute Capabilities 
32 CC1.1 atomic int32 operations in global memory 
CC1.2 atomic int32 operations in shared memory 
atomic int64 operations in global memory 
CC1.3 double-precision floating-point arithmetic 
CC2.x atomic int64 operations in shared memory 
atomicAdd on 32-bit floats 
new synchronization mechanisms 
CC3.5 Dynamic parallelism 
shuffle instruction (threads exchange data) 
… 
CC7.5 Tensor Cores for training DNN Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University What is a thread? 
33 Independent execution thread 
Has its own program counter, variables (registers), processor 
status, etc. 
No influence on thread scheduling 

CUDA threads can be physical threads 
"regular< on NVIDIA GPUs 

CUDA threads can be virtual threads 
e.g. 1 block = 1 physical thread on multicore CPUs 
(as in MCUDA) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University What is a thread block? 
34 Thread block = virtualized multiprocessor 
Freely selectable <processor size = for data volume 
Can be adjusted for each kernel start 

Thread block = a (data) parallel task 
All blocks in a kernel have the same entry point 
But can execute any code 

Thread blocks of a kernel must be independent tasks 
Block executions can be "interleaved" as desired Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Blocks must be independent 
35 Arbitrary interleaving of blocks should lead to correct 
results 
Run without a predetermined order 
Can be executed concurrently OR sequentially 

Blocks can coordinate but not synchronize 
Shared queue pointer: OK 
Shared lock: BAD ... risk of deadlocks 

Independence requirement guarantees scalability 
And makes hardware implementation manageable Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Levels of parallelism 
36 Thread parallelism 
Each thread is an independent thread of execution 

Data parallelism 
Between threads in a block 
Between blocks in a kernel 

Task parallelism 
Different blocks are independent 
Kernels are independent Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
37 thread 
Per-thread 
Local memory block 
Per-block 
Shared 
Memory Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
38 Kernel 0 
. . . 
Per-device 
Global 
Memory 
. . . Kernel 1 Sequential 
Kernels Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
39 Device 0 
memory 
Device 1 
memory Host memory cudaMemcpy() Prof. Dr. rer. nat. Tobias Lauer Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
40 Every thread can 
read/write its registers 
read/write its local memory 
read/write in the shared memory of its block 
read/write global memory 
only read constant memory 
only read texture memory 
The host ( CPU) can 
Read/write global memory 
Read/write constant memory 
Read/write texture memory 
Grid 
Constant 
Memory 
Texture 
Memory Global 
Memory Block (0, 0) 
Shared Memory 
Local 
Memory Thread (0, 0) Registers 
Local 
Memory Thread (1 , 0) Registers Block (1, 0) 
Shared Memory 
Local 
Memory Thread (0, 0) Registers 
Local 
Memory Thread (1, 0) Registers Host Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Warps and half-warps 
41 thread 
Block multiprocessor 32 threads 
32 threads 
32 threads ... 
Warps 
16 
Half warps 16 DRAM 

Global 
Local A thread block consists of warps 
with 32 threads each. 

A warp is physically executed in parallel on 
the same multiprocessor 
. 
Device 
Memory = 
A half-warp of 16 threads can 
access the global memory as a 
single transaction 
(coalescing) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University C for CUDA: Minimal extensions 
43 Declarations (indicate where things are) 
__global__ void KernelFunc(...); // Kernel can be called from the host 
__device__ void DeviceFunc(...); // Function can be called on the device 
__device__ int GlobalVar; // Variable in the device memory 
__shared__ int SharedVar; // Variable in the block-internal shared memory 

Extended syntax for calling parallel kernels 
KernelFunc<<<500, 128>>>(...); // 500 blocks, 128 threads per block 
KernelFunc<<<500, 128, 1024>>>(...); // ... 1024B shared memory per block 

Special variables for thread identification in kernels 
dim3 threadIdx ; dim3 blockIdx ; dim3 blockDim ; 

Intrinsics for specific operations in kernel code 
__syncthreads(); // Synchronization barrier Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Runtime Support 
44 •Explicit memory allocation, returns pointer to GPU memory 
cudaMalloc() , cudaFree() 

•Explicit copying for host device, device device 
(since CUDA 4.0 also between different devices) 
cudaMemcpy() , cudaMemcpyPeer() , ... 

•Interoperability with OpenGL & DirectX 
cudaGLMapBufferObject() , cudaD3D9MapVertexBuffer() , ... Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Summary 
45 CUDA = C + simple extensions 
Easy start when writing basic 
parallel programs 

Three important abstractions: 
1. Hierarchy of parallel threads 
2. Corresponding levels of synchronization 
3. Corresponding memory locations 

Supports massive parallelism of many-core GPUs Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Example: Matrix multiplication 
46 Two matrices A and B are 
multiplied: 
The vector scalar product of the i-th 
row of A with the j-th column 
of B results in the entry Ci,j in the 
result matrix C 

A and B must be correctly 
dimensioned in order to be multiplied. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Example: Matrix multiplication ( n × n) 
47 Two ( n × n) matrices A and B are multiplied: 
The vector scalar product of the i-th row of A with the j-th 
column of B results in the entry Ci,j in the result matrix C 

Vector scalar product: a ∙ b = a0 ∙ b0 + a1 ∙ b1 + … + an-1 ∙ bn-1 

Sequential algorithm 

for (i = rows of A) 

for (j = columns of B) 

for ( k = value in row i, column j) 
C[i,j] += a[i, k] * b[k, j] Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Representation 
48 
Matrix as a one-dimensional array 
in row-major representation Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Repräsentation 
49 for (int i=0; i<width; i++) { 
 for (int j=0; j<width; j++) { 
    float scalar = 0; 
    for (int k=0; k<width; k++) { 
   float a = A[i*width + k]; 
   float b = B[k*width + j]; 
   scalar += a * b; 
    } 
    C[i*width + j] = scalar; 
 } 
} Explizite row-major  Darstellung: 
 
Aus M[i][j] wird M[i*width + j] 
 
Sequenzieller Algorithmus: Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Matrix multiplication 
50 Observations: 
Each entry of the result matrix can be calculated independently 
of the others 
Order of all loops is irrelevant 

Idea: 
One thread for each entry (i, j) of the result matrix 
"Embarrassingly parallel < (no dependencies) 

Theoretically, the innermost loop 
(scalar product) could also be parallelized Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University One thread per matrix entry 
51 Make thread block 2-dimensional to reflect the structure of the 
matrix. 
Thread position in block: (x, y) 

Thread at position ( i, j) in block calculates entry ( i, j) 
of the result matrix: 
int i = ThreadIdx.x ; 
int j = ThreadIdx.y ; 
float scalar = 0; 
for (k=0; k<width; k++) { 
float a = A[i*width + k]; 
float b = B[k*width + j]; 
scalar += a * b; 
} 
C[i*width + j] = scalar; Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Implementation in CUDA C 
52 Initialization 
Memory allocation on the host side, reading the input 
Memory allocation on the device side: cudaMalloc 
Copying the input from the host to the device: cudaMemcpy 

Starting the GPU kernel 
MatrixMulKernel<<<blocksize, threads>>>(...) 

Conclusion 
Copying the output from the device to the host: cudaMemcpy 
Releasing memory: cudaFree Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Initialization (Host) 
53 // Memory allocation for matrices M and N (Host) 
unsigned int size = width * width * sizeof(float) ; 
float* hostM = ( float*) malloc(size); 
float* hostN = ( float*) malloc(size); 

// Fill matrices with values ​​
... 

// Memory allocation for result matrix P (Host) 
float* hostP = ( float*) malloc(size); Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Initialization (Device) 
54 // Memory allocation for matrix M (Device) 
float* deviceM; 
cudaMalloc ((void**)&deviceM, size); 

First parameter of cudaMalloc : 
Address of a pointer that points to the allocated object after allocation 
Cast to (void**) is recommended because a generic pointer is expected 
(and not to a specific type) 
Second parameter: 
Number of bytes to be allocated Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Copying from host to device 
55 // Memory allocation for matrix M (device) 
float* deviceM; 
cudaMalloc ((void**)&deviceM, size); 

// Copy matrix M from host to device 
cudaMemcpy (deviceM, hostM, size, cudaMemcpyHostToDevice); 

Parameters of cudaMemcpy: 
Pointer to target location (device) 
Pointer to source object (host) 
Number of bytes to be copied 
Memory locations involved (source, target) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Implementation in CUDA C 
56 Initialization 
Memory allocation on the host side, reading the input 
Memory allocation on the device side: cudaMalloc 
Copying the input from the host to the device: cudaMemcpy 

Starting the GPU kernel 
MatrixMulKernel<<<grid, block>>>(...) 

Conclusion 
Copying the output from the device to the host: cudaMemcpy 
Releasing memory: cudaFree Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Start kernel 
57 // Define CUDA execution configuration 
dim3 dimBlock( width, width); 
dim3 dimGrid( 1, 1); // Warning, unrealistic! 

// Call kernel 
MatrixMulKernel <<<dimGrid, dimBlock >>>(deviceM, deviceN, 
deviceP, width); 

Kernel call is made by: 
Name of kernel function 
<<<CUDA configuration >>> 
Parameters of kernel function 

dim3: Struct for displaying the dimensions of blocks and grids. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Configuration 
58 // Define CUDA execution configuration 
dim3 dimBlock( width, width); 
dim3 dimGrid( 1, 1); 

Restrictions for block and grid configuration (CC 1.x): 
Block: max. dimensions 512 x 512 x 64 
and total size ≤ 512 
and total size multiple of 32 (CC 1.x) 
or 1024 x 1024 x 64 
and total size < 1024 (CC 2.0+) 
Grid: max. dimensions 65535 x 65535 x 1 (CC 1.x) 
or 65535 x 65535 x 65535 (CC 2.x) 
or 231-1 x 65535 x 65535 (CC 3.0+) 

Source: http://en.wikipedia.org/wiki/CUDA Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Kernel-Code 
59 // Kernel-Funktion, wird von jedem Thread ausgeführ t 
__global__  void MatrixMulKernel (float* A, 
       float* B, 
       float* C, 
       int width) 
{ 
 int i = ThreadIdx.x ;  // x-Koordinate des Threads im 
Block 
 int j = ThreadIdx.y ;  // y-Koordinate des Threads im 
Block 
 float scalar = 0; 
 for ( k=0; k<width; k++) { 
    float a = A[ i*width + k]; 
    float b = B[ k*width + j]; 
    scalar += a * b; 
 } 
 C[i*width + j] = scalar; 
} Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University of Applied Sciences CUDA Keywords 
60 Keyword Execution Call 

__global__ void KernelFct() Device Host 

__device__ float DevFct() Device Device 

__host__ float HostFct() Host Host Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Implementation in CUDA C 
61 Initialization 
Memory allocation on the host side, reading the input 
Memory allocation on the device side: cudaMalloc 
Copying the input from the host to the device: cudaMemcpy 

Starting the GPU kernel 
MatrixMulKernel<<<grid, block>>>(...) 

Conclusion 
Copying the output from the device to the host: cudaMemcpy 
Releasing memory: cudaFree Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Degree 
62 
// Copy result matrix from device to host 
cudaMemcpy (hostP, deviceP, size, cudaMemcpyDeviceToHost); 

// Free memory on device 
cudaFree (deviceM); 
cudaFree (deviceN); 
cudaFree (deviceP); Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Example is simplified 
63 Requirement for matrices 
Square (n x n) 
Size must fit in a block and be a multiple of 32 
n ≤ 16 

Only 1 thread block 
Limits size to 16 x 16 

Extension to general matrices? Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg CUDA-Programme 
64 
CUDACC 
 CPU Compiler 
C for CUDA 
Kernels 
CUDA object 
files 
Rest of C 
Application 
CPU object 
files 
CPU-GPU 
Executable 
NVCC 
C for CUDA 
Application 
Linker Combined CPU-GPU Code Parallel programming and 
algorithms
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University Topics
•Parallel sorting
–Part I: Quicksort
–Part II: Radix sort
–Part III: Mergesort (final)
–Part IV: Bitonic sort (later) 3 Offenburg University of Applied SciencesParallel Sort
•Sorting data
–important building block, omnipresent
•Databases
•Search engines (ranking)
• …
–performance bottleneck
• Ω(nlog n) for general sequential sorting methods 
•Time grows more than linearly with the amount of data
→Good candidate for parallelization 4 Offenburg University of Applied SciencesParallel Quicksort
•Quicksort sequential with input length n:
–In-place (no extra space required)
–Complexity 
•O(nlog n) in the best and average case
•O(n²) in the worst case (we ignore here)
–Algorithm: Time T (n):
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to the pivot
•Sort A and Crecursive 2 · T(n/2) 5 Offenburg University Parallelization
•Algorithm: Time (average)
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to pivot
•Sort A and C cursively 2 · T(n/2)
•Obvious:
–Carry out the two sorts of A and C in parallel 6 Offenburg University of Applied Sciences 7 Offenburg University of Applied SciencesParallelization
•Algorithm: Time (average)
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to pivot
•Sort A and C cursively in parallel 1 · T(n/2)
•Complexity:
–Time required (span): T∞(n) = O(n) + 1· T(n/2) = ? 8 Offenburg University of Applied Sciences 9 Offenburg University of Applied SciencesParallelization
•Algorithm: Time (average)
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to pivot
•Sort A and C cursively in parallel 1 · T(n/2)
•Complexity:
–Time required (span): T∞(n) = O(n) + 1· T(n/2) = O(n)
–Speedup is in the range O(log n)
–How many processors are needed? 10 Offenburg University of Applied SciencesParallel Quicksort
•This means that a billion (= 109) elements would be sorted about 30x
faster...
•... with O(n) (e.g. 1/2 billion) processors... 
•Can we parallelize even better, 
e.g. also the more complex partitioning step?
→Yes, if we allow extra space! 11 Offenburg UniversityPartition
•Reminder
–A: elements smaller than pivot
–B: pivot
–C: elements greater than/equal to pivot
•Sequential time: O(n)
•Can you filter A and C from the array faster?
•Yes: execute pack twice! 12 Offenburg UniversityPack
•Pack 1:
–Condition: a[i] < pivot
•Pack 2:
–Condition: a[i] ≥ pivot5 3 10 2 8 6 4 7
5 3 2 6 4
5 3 10 2 8 6 4 7
5 3 2 6 4 10 8 13 Offenburg UniversityReminder: Pack (Compaction) 14 Offenburg University Extra storage space
•Array to remember whether the condition is fulfilled
–Prefix sum on top to calculate the target position
•Auxiliary array for the results of pack 1 and 2
–A and C can write their results into the same auxiliary array 
(insert pivot in between!)
–Pack 1 and 2 can also be carried out together
(does not change the asymptotic complexity)
•At the end, swap the auxiliary array with the original array
(always sort back and forth between 2 arrays) 15 Offenburg University Complexity
•Partition
–2x Pack
–Time (span): O(log n)
–Processors: O(n / log n)
•Total:
–Time (span): T(n)= O(log n) + 1· T(n/2) = 
–Processors: O( n / log n) 16 Offenburg University of Applied SciencesParallel Quicksort: Conclusion
•Quicksort can be parallelized in a cost-optimal way
•There are several other parallelization variants 17 Offenburg UniversityRadix Sort
•Fastest known parallel sorting method in
practice (on GPUs)
–especially for large numbers of elements …
–… and not too large elements
•Not only based on comparing and swapping
–Not a general sorting method
–makes use of the number format used! 18 Offenburg UniversityIdea
• Sort by iterative distribution into "compartments", where there is a compartment for each digit of the number representation.
–Start with the last digit (far right)
–Keep the order of the elements per "compartment" the same
•Was previously used for manual or mechanical sorting (e.g. postcodes, punch cards) 19 Offenburg UniversityExample (board) 20 Offenburg University of Applied Sciences Implementation
1. Choose a "suitable" base d
2. Start with the lowest value i
3. Partition elements according to the value at position i
–Do not change the existing order between the elements within the group
4. Attach the partitions to one another 
→ new global arrangement
5. Increase ium 1 
6. If not yet sorted, go to step 3. 21 Offenburg University Complexity
•Number of runs:
–up to e (e is the exponent of the representation basis)
–e is constant for the specific application (but relevant)
–can be selected within a certain framework
–determines potential additional space requirements
•Number of steps per run:
–view and distribute n elements
•O( e·n) 22 Offenburg University of Applied SciencesParallelization
•First: d= 2 (binary representation)
•Example: 4, 7, 2, 6, 3, 5, 1, 0 (3 bits required)
–Consider the first run:
–We split the sequence into two parts:
•Elements that end with 0
•Elements that end with 1
–How can we do that? 23 Offenburg UniversityParallel Primitive: Split
•Partition a sequence into two subsequences based on a condition
•We already know this!
•Essentially corresponds to 2x Pack 24 Offenburg UniversitySplit
•Select the relevant bit in the input
•Store the inverse of the considered 
bit in temporary array e
•Prefix sum on e →f: f contains the 
target addresses of the elements with 0
•Store the total number of 
0 values ​​in a global variable
•Another temporary array t for 
target position of the 1 values
•Select d from t or f, depending on the bit value
•Copy input elements at position d in 
output 25 Offenburg University Radix Sort with Split
•For complete sorting:
–Repeat split with the next highest bit …
–… until the sequence is sorted.
–When is that?
•At the latest after the split with the most significant bit
•If the keys are not too large, possibly much earlier
•Complexity:
–Time per split: O(log n) 26 Offenburg University Radix Sort with Split
•For complete sorting:
–Repeat split with the next highest bit …
–… until the sequence is sorted.
–When is that?
•At the latest after the split with the most significant bit
•If the keys are not too large, possibly earlier
•Still a problem: lots of passes!
–Solution: see lab 27 Offenburg University of Applied SciencesPractical aspects
•Implementation for very large input?
–Division into blocks the size of a SIMD array
–Sorting per block
–Merging the sorted blocks
→Parallel merge 28 Offenburg University of Applied SciencesMerge-Sort in practice
•Problem:
Tree-like division into ever smaller subsequences
•Leads to different scenarios:
–More merges than threads in the lower part of the tree
–Merges ~ threads in a middle region
–More threads than merges in the upper part
•Idea: 
Use different approaches depending on the region in the tree 29 Offenburg University Merge methods
•In the lower part:
–Sequential:
Each thread merges small subsequences sequentially 
(or sorts them sequentially in another way)
–Middle part:
Parallelization according to architecture (e.g. GPU multiprocessors)
•1 or a few merges per thread 
•enables optimal parallel calculation of independent subsequences 
(e.g. per processor group)
–Upper part:
•Parallelize individual merge steps
(→see later: Parallel Merge) 30 Offenburg University Sorting networks
•Property of many sorting algorithms
–Data dependency
–Next action depends on the value of the key in question
–Threads are unevenly loaded / diverge
–Not optimal for SIMD-like architectures
•Better suited algorithms
–As similar a task as possible for all threads
→“Sorting Networks” 31 Offenburg UniversityBitonic Sort
•Similarities to Merge Sort
–Recursive approach
•Halving the sequence in each step
–Sorting the halves again with Bitonic Sort
•New: Merging the sorted subsequences is also recursive
– “Bitonic Merge” 
–but: in-place (no extra space required)
•No copying, just swapping
•Uses an important property of bitonic sequences 32 Offenburg UniversityBitonic number sequences
•Bitonic (cf. monotonic):
–Sequence consists of a monotonically increasing and a monotonically 
decreasing part
–Example:
7, 10, 24, 21, 18, 10, 4, 3 33 Offenburg UniversityDefinition
•A sequence e0, e1, …, is called bitonic if there is an 
index i(0 < i< n) such that 
–either e0, …, eimonotonically increasing and ei, …, enmonotonically decreasing 
–or there is a cyclical shift of the sequence for which the above 
applies.
•Example:
Image source: Philipp Slusallek 34 Offenburg UniversityComparison Network
•Element-wise comparison of two episodes (here: sorted) 35 Offenburg UniversityComparison Network
•Element comparisons
–Completely independent of each other
–Always the same number (namely n/2)
–Always structured the same (regardless of data quality)
→Perfectly parallelizable (“embarrassingly parallel”)
→Very well suited for SIMD, e.g. for GPUs 36 Offenburg University Observation 1
•Decomposition of a bitonic sequence into 2 subsequences (of the same length):
•Comparison step: both subsequences are then also 
bitonic again (proof not trivial!)
•All elements of the right subsequence are larger than all elements of the 
left subsequence 37 Offenburg UniversityBitonic Split
•Splits a sequence using a comparison network: 38 Hochschule OffenburgBitonicSplit
bitonicSplit( intlo, inthi) {
if(lo>= hi) return;
intm= (hi+lo)/2;
intstride= (hi-lo+1)/2;  // Abstand verglichener Elemente
for(inti=lo; i<=m; i++) in parallel {
if(input[i] > input[i+stride]) {
swap(i, i+stride);
}
}
} 39 Offenburg University Observation 2
•By repeated (recursive) splitting, a bitonic sequence is completely sorted (→“BitonicMerge”) 40 High School OffenburgBitonicMerge (recursive)
bitonicMerge( house, house) {
if ( lo >= hi ) return ;
bitonicSplit( no, hi);
intm= (hi+lo)/2;
bitonicMerge ( lo , m ) ;
bitonicMerge( m+1, hi);
} } 41 Offenburg University of Applied SciencesWhat to do with non-bitonic sequences?
•Unsorted sequences are usually not bitonic
•Can you make them bitonic?
–Idea: 
•Divide the unsorted sequence into two equal halves
•Sort the left half in ascending order and the right half in descending order 
(how? →recursively with BitonicSort)
•The sequence is now bitonic and can now be sorted with a BitonicMerge. 42 Offenburg UniversityBitonicSort
•BitonicSort( n) 43 Hochschule OffenburgBitonicBlack (recursive)
static void bitonicSort( home, home) {
if ( lo >= hi ) return ;
// Split the input
intm= (hi+lo)/2;
// Sort lower half in same order
bitonicSort ( lo , m ) ;
// Sort upper half in reverse order
bitonicSort_reverse( m+1, hi);
// Sort bitonic sequence lo..hi
bitonicMerge( lo , hi );
} } 44 Offenburg University of Applied SciencesFor GPU: Resolve recursion
•Iterative view (example with 16 elements):
k=2 k=4 k=8 k=16s=2s=1 s=2s=1 s=4 s=2s=1 s=4 s=8 s=1
k: current size of the subsequence(s) s: distance between two compared elements 46 Offenburg University of Applied SciencesIterative algorithm
bitonicSort_iterative (int[] input) {
final int NUM= input.length;
for(intk=2; k<=NUM; k*=2) { // log2(n) iterations
for(ints=k/2; s>0; s/=2) { // <= log2(n) iterations
for(inttid=0; tid<NUM; tid++) in parallel {
intixj= tid^ s; // XOR // Determine exchange partner
if(ixj> tid) { // only 1 exchange per 2 elements
if((tid& k) == 0) { // In “even k-group” ...
if(input[tid] > input[ixj]) // ... sort in ascending order
swap(input, tid, ixj);
} else{ // else...
if(input[tid] < input[ixj]) // ... sort descending
swap(input, tid, ixj);
}
}
} // sync
}
}
} 47 Offenburg University Complexity
•Cost: O(nlog² n)
•Time/Span: O(log² n)
•Processors: O(n) 49 Offenburg University Summary
•Parallelization can speed up sorting significantly
•General sorting methods (quicksort, merge sort) are 
flexible in terms of the elements to be sorted
•Special methods such as radix sort can be very fast 
(~1 billion 32-bit numbers per second on a GPU).
•Bitonic sort has no data dependency and therefore 
minimal thread divergence Parallel Programming and Algorithms
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg UniversityTopics
•Parallel algorithms 
–Parallel merge
(combining 2 large sorted arrays) 3 Offenburg University of Applied SciencesParallel Merge
•We already know merging from merge-sort
•Input: 
Two sorted arrays A and B of lengths m and n
–Here: not necessarily the same/similar length
–We assume that m≥ n (otherwise swap A and B)
•Output: 
An array C of length m+n that contains the elements 
of A and B sorted 4 Offenburg UniversityExample
A=( 2, 8, 11, 13, 17, 20) B=( 3, 6, 10, 15, 16, 73)
→ C= (2, 3, 6, 8, 10, 11, 13, 15, 16, 17, 20, 73)
•Sequential algorithm (see merge sort)
–Go through A and B simultaneously using two pointers
–In each step:
•Copy the smaller of the two elements from branch B
to the current position in C
•Increment the corresponding pointer and the current position in C
–The complexity is O(m+n) 5 Offenburg University of Applied SciencesParallelization: 1st attempt
•Observation:
–You can perform the merging from the left (ascending) or from the right 
(descending)
–Can both be done at the same time?
•Yes, if you fill the target array exactly halfway 
(then there can be no write conflicts)
–Parallel merging with 2 threads/processors possible
•You can't do more with this approach!
•Is there even more parallelism possible? 6 Offenburg University of Applied SciencesParallel merging
Idea: 
Divide the arrays appropriately and run the sequential 
algorithm in parallel on the parts
Definition:
rank(ai: A) = number of elements in A 
that are less than or equal to ai ( aiєA)
rank(bi: A) = number of elements in A 
that are less than or equal to bis ( biєB) 7 Offenburg UniversityExample and remark
A=( 2, 8, 11, 13, 17, 20 ) B=( 3, 6, 10, 15, 16, 73 )
rank(11 : A) = 3 rank(11 : B) = 3
rank(16 : A) = 4 rank(16 : B) = 5
Remark:
The rank of an element efrom Aor Bin 
result array Ccorresponds to
rank(e: A) + rank(e: B)
(if you start counting at 1) 9 Offenburg University Division
• Divide array A into blocks so that each block consists of log m
elements (except possibly the last one).
• This gives us m/log m blocks
• The ith block ends at position i∙ log m(1 ≤ i< m/log m)
(and the last block ends at position m) a1… …… alog ma(log m)+1 … a2log m ………. 10 Offenburg University Division
•There is a unique assignment of each block Ai to 
a block in B
•We call such a pair of blocks a matching pair
•Problem: How do you find a matching pair? a1… ……alog ma(log m)+1 …a2log m ………. 11 Offenburg University of Applied SciencesParallel merging
Goal:
–Running time of the sequential algorithm for a matching pair
in O(logm+ logn)
–For this, the size of each piece should be in O (logm)
or O (logn)
–This division was easy for A; what about B?
Approach:

–Use the rank of elements to divide B
into suitable pieces
–Then run the sequential algorithm in parallel on one 
matching pair each 12 Offenburg University Processor allocation
•Use a processor for each block A of A
•For all i in parallel (except the last):
–Find a rank in B for the last element of A
i.e. determine rank(ai log m : B)
–How can you determine the rank efficiently?
•Binary search
–Time complexity for this step
•O(log n)
–Binary searches can take place in parallel (CREW)
•After this step, array B is also implicitly divided into 
m/log m pieces 13 Offenburg University Division
•There is a unique assignment of each block Ai to 
a block in B (blocks in B can also be empty)
•We call such a pair of blocks a matching pair a1… …… alog malog m + 1 … a2log m ………. 14 Offenburg University Division
•The rank of each element within a block Ai lies 
in the corresponding matching block Bi.
•Therefore, merging a matching pair is an 
independent subproblem.ai log m a(i+1) log m
rank(ai log m : B) rank(a(i+1)log m : B) 15 Offenburg University Merging Matching Pairs
Size of the blocks in B:
•Good case: 
–Size of each block is (approximately) in O(logn)
→Use the sequential algorithm for merging
•Bad case: 
–There are one or more long blocks
→We have to “fix” that (how?) 16 Offenburg University of Applied SciencesWorst case
•A single block Aivon Ahas the 
complete array B as a matching partner
•Idea: Use the same algorithm for Bund Ai 17 Offenburg UniversityGood case
•Use one processor per matching pair
•Merge the two blocks of the pair using the sequential algorithm
–The time for this is O (logm + logn)
•All pairs can be processed simultaneously 18 Offenburg University Complexity analysis
•Step 1: Division of array A
–The division is purely based on position (index)
–We have m/log m processors 
(one processor per block of size log m )
–The end element of each block can therefore be determined in 
constant time O(1)
–The cost is in O(m/log m) 19 Offenburg University Complexity analysis
•Step 2: Division of array B
–This division is done according to values ​​(rank( a: B))
–We use the binary search in B for this
This takes time in O(logn)
–Here too we have m/logm processors
–Cost: O( logn∙ (m/logm) ) 20 Offenburg University Assessment 21 Offenburg University Complexity analysis
•Step 2: Division of array B
–This division is done according to values ​​(rank( a: B))
–We use the binary search in B for this
This takes time in O(logn)
–Here too we have m/logm processors
–Cost: O( ( m/logm)∙ logn) = O( m+ n) 22 Offenburg University Complexity analysis
•Step 3: Sequential merging of matching pairs
–The time for this is O( log m + log n ) 
–Here too we have m/logm processors
–Cost: O( ( m/logm)∙ (logm+ logn) ) 
= O( m+ ( m/logm∙ logn) ) 
= O(m) + O( m+ n)
= O(m+ n) 23 Offenburg University Complexity analysis
•Summary:
Time Cost
Step 1: O(1) O( m/log m )
Step 2: O( logn) O( m+ n)
Step 3: O( log m+ logn) O( m+ n)
Total: O(logm+ logn) O( m+ n) 24 Offenburg University of Applied SciencesWhat about the bad cases?

•Worst case:
A single block Aivon Ahas the complete array B as a matching partner
•Then use the same algorithm for block Ai:
–Length of Ai= log m < m
–Length of B= n
–Total runtime for this part never worse than before
• For cases that lie “in between”, you can proceed analogously. Optimizing Parallel Reduction in CUDA
Mark Harris
NVIDIA Developer Technology 2
Parallel Reduction
Common and important data parallel primitive
Easy to implement in CUDA
Harder to get it right
Serves as a great optimization example
We’ll walk step by step through 7 different versions
Demonstrates several important optimization strategies 3
Parallel Reduction
Tree-based approach used within each thread block
Need to be able to use multiple thread blocks
To process very large arrays
To keep all multiprocessors on the GPU busy
Each thread block reduces a portion of the array
But how do we communicate partial results between 
thread blocks?4 7 5 9
11 14
253 1 7 0 4 1 6 3 4
Problem: Global Synchronization
If we could synchronize across all thread blocks, could easily 
reduce very large arrays, right?
Global sync after each block produces its result
Once all blocks reach sync, continue recursively
But CUDA has no global synchronization.  Why?
Expensive to build in hardware for GPUs with high processor 
count
Would force programmer to run fewer blocks (no more than # 
multiprocessors * # resident blocks / multiprocessor) to avoid 
deadlock, which may reduce overall efficiency 
Solution: decompose into multiple kernels
Kernel launch serves as a global synchronization point
Kernel launch has negligible HW overhead, low SW overhead 5
Solution: Kernel Decomposition
Avoid global sync by decomposing computation 
into multiple kernel invocations
In the case of reductions, code for all levels is the 
same
Recursive kernel invocation4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163Level 0:
8 blocks
Level 1:
1 block 6
What is Our Optimization Goal?
We should strive to reach GPU peak performance
Choose the right metric:
GFLOP/s: for compute-bound kernels
Bandwidth: for memory-bound kernels
Reductions have very low arithmetic intensity
1 flop per element loaded (bandwidth-optimal)
Therefore we should strive for peak bandwidth
Will use G80 GPU for this example
384-bit memory interface, 900 MHz DDR
384 * 1800 / 8 = 86.4 GB/s 7
Reduction #1: Interleaved Addressing
__global__ void reduce0( int*g_idata, int*g_odata) {
extern __shared__ int sdata[];
// each thread loads one element from global to shared mem
unsigned int tid = threadIdx.x ;
unsigned int i = blockIdx.x *blockDim.x + threadIdx.x ;
sdata[tid] = g_idata[i];
__syncthreads() ;
// do reduction in shared mem
for(unsigned int s=1; s < blockDim.x ; s *= 2) {
if(tid % (2*s) == 0) {
sdata[tid] += sdata[tid + s];
}
__syncthreads() ;
}
// write result for this block to global mem
if(tid == 0) g_odata[ blockIdx.x ] = sdata[0];
} 8
Parallel Reduction: Interleaved Addressing
10 1 8 -1 0 -2 3 5 -2 -3 2 7 0 11 0 2 Values (shared memory)
0 2 4 6 8 10 12 14
11 1 7 -1 -2 -2 8 5 -5 -3 9 7 11 11 2 2 Values
0 4 8 12
18 1 7 -1 6 -2 8 5 4 -3 9 7 13 11 2 2 Values
0 8
24 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 Values
0
41 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 ValuesThread 
IDsStep 1 
Stride 1
Step 2 
Stride 2
Step 3 
Stride 4
Step 4 
Stride 8Thread 
IDs
Thread 
IDs
Thread 
IDs 9
Reduction #1: Interleaved Addressing
__global__ void reduce1( int*g_idata, int*g_odata) {
extern __shared__ int sdata[];
// each thread loads one element from global to shared mem
unsigned int tid = threadIdx.x ;
unsigned int i = blockIdx.x *blockDim.x + threadIdx.x ;
sdata[tid] = g_idata[i];
__syncthreads() ;
// do reduction in shared mem
for(unsigned int s=1; s < blockDim.x ; s *= 2) {
if(tid % (2*s) == 0) {
sdata[tid] += sdata[tid + s];
}
__syncthreads() ;
}
// write result for this block to global mem
if(tid == 0) g_odata[ blockIdx.x ] = sdata[0];
}Problem: highly divergent 
warps are very inefficient, and 
% operator is very slow 10
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Note: Block Size = 128 threads for all testsBandwidth Time (222 ints) 11for(unsigned int s=1; s < blockDim.x ; s *= 2) {
if(tid % (2*s) == 0) {
sdata[tid] += sdata[tid + s];
}
__syncthreads() ;
}
for(unsigned int s=1; s < blockDim. x; s *= 2) {
int index = 2 * s * tid;
if (index < blockDim. x) {
sdata[index] += sdata[index + s];
}
__syncthreads();
}
Reduction #2: Interleaved Addressing
Just replace divergent branch in inner loop:
With strided index and non-divergent branch: 12
Parallel Reduction: Interleaved Addressing
10 1 8 -1 0 -2 3 5 -2 -3 2 7 0 11 0 2 Values (shared memory)
0 1 2 3 4 5 6 7
11 1 7 -1 -2 -2 8 5 -5 -3 9 7 11 11 2 2 Values
0 1 2 3
18 1 7 -1 6 -2 8 5 4 -3 9 7 13 11 2 2 Values
0 1
24 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 Values
0
41 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 ValuesThread 
IDsStep 1 
Stride 1
Step 2 
Stride 2
Step 3 
Stride 4
Step 4 
Stride 8Thread 
IDs
Thread 
IDs
Thread 
IDs
New Problem: Shared Memory Bank Conflicts 13
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 14
Parallel Reduction: Sequential Addressing
10 1 8 -1 0 -2 3 5 -2 -3 2 7 0 11 0 2 Values (shared memory)
01234567
8 -2 10 6 0 9 3 7 -2 -3 2 7 0 11 0 2 Values
0123
8 7 13 13 0 9 3 7 -2 -3 2 7 0 11 0 2 Values
01
21 20 13 13 0 9 3 7 -2 -3 2 7 0 11 0 2 Values
0
41 20 13 13 0 9 3 7 -2 -3 2 7 0 11 0 2 ValuesThread 
IDsStep 1 
Stride 8
Step 2 
Stride 4
Step 3 
Stride 2
Step 4 
Stride 1Thread 
IDs
Thread 
IDs
Thread 
IDs
Sequential addressing is conflict free 15for(unsigned int s=1; s < blockDim. x; s *= 2) {
int index = 2 * s * tid;
if (index < blockDim. x) {
sdata[index] += sdata[index + s];
}
__syncthreads();
}
for (unsigned int s=blockDim. x/2; s>0; s>>=1) {
if (tid < s) {
sdata[tid] += sdata[tid + s];
}
__syncthreads();
}
Reduction #3: Sequential Addressing
Just replace strided indexing in inner loop:
With reversed loop and threadID-based indexing: 16
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 17for (unsigned int s=blockDim.x/2 ; s>0; s>>=1) {
if (tid < s) {
sdata[tid] += sdata[tid + s];
}
__syncthreads();
}
Idle Threads
Problem: 
Half of the threads are idle on first loop iteration!
This is wasteful… 18// each thread loads one element from global to shared mem
unsigned int tid = threadIdx.x ;
unsigned int i = blockIdx.x *blockDim.x + threadIdx.x ;
sdata[tid] = g_idata[i];
__syncthreads() ;
// perform first level of reduction,
// reading from global memory, writing to shared memory
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockDim.x*2 ) + threadIdx. x;
sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];
__syncthreads();
Reduction #4: First Add During Load
Halve the number of blocks, and replace single load:
With two loads and first add of the reduction: 19
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 20
Instruction Bottleneck
At 17 GB/s, we’re far from bandwidth bound
And we know reduction has low arithmetic intensity
Therefore a likely bottleneck is instruction overhead
Ancillary instructions that are not loads, stores, or 
arithmetic for the core computation
In other words: address arithmetic and loop overhead
Strategy: unroll loops 21
Unrolling the Last Warp
As reduction proceeds, # <active= threads decreases
When s <= 32, we have only one warp left
Instructions are SIMD synchronous within a warp
That means when s <= 32:
We don’t need to __syncthreads()
We don’t need <if (tid < s)= because it doesn’t save any 
work
Let’s unroll the last 6 iterations of the inner loop __device__ void warpReduce( volatile int* sdata, int tid ) {
sdata[tid] += sdata[tid + 32]; 
sdata[tid] += sdata[tid + 16]; 
sdata[tid] += sdata[tid +  8]; 
sdata[tid] += sdata[tid +  4]; 
sdata[tid] += sdata[tid +  2]; 
sdata[tid] += sdata[tid +  1]; 
}
// later…
for(unsigned int s=blockDim. x/2; s>32 ; s>>=1) {
if (tid < s)
sdata[tid] += sdata[tid + s];
__syncthreads();
}
if (tid < 32) warpReduce(sdata, tid);
22
Reduction #5: Unroll the Last Warp
Note: This saves useless work in allwarps, not just the last one!
Without unrolling, all warps execute every iteration of the for loop and if statementIMPORTANT: 
For this to be correct,
we must use the 
<volatile= keyword! 23
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34x
Kernel 5:
unroll last warp0.536 ms 31.289 GB/s 1.8x 15.01xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 24
Complete Unrolling
If we knew the number of iterations at compile time, 
we could completely unroll the reduction
Luckily, the block size is limited by the GPU to 512 threads
Also, we are sticking to power-of-2 block sizes
So we can easily unroll for a fixed block size
But we need to be generic –how can we unroll for block 
sizes that we don’t know at compile time?
Templates to the rescue!
CUDA supports C++ template parameters on device and 
host functions 25
Unrolling with Templates
Specify block size as a function template parameter:
template <unsigned int blockSize>
__global__ void reduce5( int *g_idata, int *g_odata) 26
Reduction #6: Completely Unrolled
if (blockSize >= 512) {
if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }
if (blockSize >= 256) {
if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }
if (blockSize >= 128) {
if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); }
if (tid < 32) warpReduce< blockSize >(sdata, tid);
Note: all code in RED will be evaluated at compile time.
Results in a very efficient inner loop!Template < unsigned int blockSize >
__device__ void warpReduce( volatile int* sdata, int tid ) {
if (blockSize >= 64) sdata[tid] += sdata[tid + 32]; 
if (blockSize >= 32) sdata[tid] += sdata[tid + 16]; 
if (blockSize >= 16) sdata[tid] += sdata[tid +  8]; 
if (blockSize >=   8) sdata[tid] += sdata[tid +  4]; 
if (blockSize >=   4) sdata[tid] += sdata[tid +  2]; 
if (blockSize >=   2) sdata[tid] += sdata[tid +  1]; 
} 27
Invoking Template Kernels
Don’t we still need block size at compile time?
Nope, just a switch statement for 10 possible block sizes:
switch ( threads)
{
case 512:
reduce5<512><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 256:
reduce5<256><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 128:
reduce5<128><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 64:
reduce5< 64><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 32:
reduce5< 32><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 16:
reduce5< 16><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  8:
reduce5<  8><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  4:
reduce5<  4><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  2:
reduce5< 2><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  1:
reduce5<  1><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
} 28
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34x
Kernel 5:
unroll last warp0.536 ms 31.289 GB/s 1.8x 15.01x
Kernel 6:
completely unrolled0.381 ms 43.996 GB/s 1.41x 21.16xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 29
Parallel Reduction Complexity
Log( N)parallel steps, each step Sdoes N/2S
independent ops
Step Complexity is O(log N)
For N=2D, performs S[1..D]2D-S= N-1 operations 
Work Complexity is O( N)–It is work-efficient
i.e. does not perform more operations than a sequential 
algorithm
With Pthreads physically in parallel ( Pprocessors), 
time complexity is O( N/P + log N) 
Compare to O( N) for sequential reduction
In a thread block, N=P, so O(log N) 30
What About Cost?
Cost of a parallel algorithm is processors time 
complexity
Allocate threads instead of processors: O( N) threads
Time complexity is O(log N), so cost is O( Nlog N) : not 
cost efficient!
Brent’s theorem suggests O( N/log N) threads
Each thread does O(log N) sequential work
Then all O( N/log N) threads cooperate for O(log N) steps
Cost = O(( N/log N) * log N) = O( N) cost efficient
Sometimes called algorithm cascading
Can lead to significant speedups in practice 31
Algorithm Cascading
Combine sequential and parallel reduction
Each thread loads and sums multiple elements into 
shared memory
Tree-based reduction in shared memory
Brent’s theorem says each thread should sum 
O(log n) elements
i.e. 1024 or 2048 elements per block vs. 256
In my experience, beneficial to push it even further
Possibly better latency hiding with more work per thread
More threads per block reduces levels in tree of recursive 
kernel invocations 
High kernel launch overhead in last levels with few blocks
On G80, best perf with 64-256 blocks of 128 threads
1024-4096 elements per thread 32unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockDim.x*2 ) + threadIdx. x;
sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];
__syncthreads();
Reduction #7: Multiple Adds / Thread
Replace load and add of two elements:
With a while loop to add as many as necessary:
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockSize*2) + threadIdx. x;
unsigned int gridSize = blockSize*2* gridDim. x;
sdata[tid] = 0;
while (i < n) {
sdata[tid] += g_idata[i] + g_idata[i+blockSize];
i += gridSize;
}
__syncthreads(); 33unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockDim.x*2 ) + threadIdx. x;
sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];
__syncthreads();
Reduction #7: Multiple Adds / Thread
Replace load and add of two elements:
With a while loop to add as many as necessary:
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockSize*2) + threadIdx. x;
unsigned int gridSize = blockSize*2* gridDim. x;
sdata[tid] = 0;
while (i < n) {
sdata[tid] += g_idata[i] + g_idata[i+blockSize];
i += gridSize;
}
__syncthreads();Note: gridSize loop stride 
to maintain coalescing! 34
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34x
Kernel 5:
unroll last warp0.536 ms 31.289 GB/s 1.8x 15.01x
Kernel 6:
completely unrolled0.381 ms 43.996 GB/s 1.41x 21.16x
Kernel 7:
multiple elements per thread0.268 ms 62.671 GB/s 1.42x 30.04x
Kernel 7 on 32M elements: 73 GB/s!Step
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 35template <unsigned int blockSiz e>
__device__ void warpReduce( volatile int *sdata , unsigned int tid) {
if (blockSize >=  64) sdata[tid] += sdata[tid + 32];
if (blockSize >=  32) sdata[tid] += sdata[tid + 16];
if (blockSize >=  16) sdata[tid] += sdata[tid +  8];
if(blockSize >=   8) sdata[tid] += sdata[tid +  4];
if (blockSize >=   4) sdata[tid] += sdata[tid +  2];
if (blockSize >=   2) sdata[tid] += sdata[tid +  1];
}
template <unsigned int blockSize>
__global__ void reduce6( int *g_idata, int *g_odata, unsigned int n) {
extern __shared__ int sdata[];
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockSize*2) + tid;
unsigned int gridSize = blockSize*2* gridDim. x;
sdata[tid] = 0;
while (i < n) { sdata[tid] += g_idata[i] + g_idata[i+blockSize];  i += gridSize;  }
__syncthreads();
if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }
if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }
if (blockSize >= 128) { if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); }
if (tid < 32) warpReduce(sdata, tid);
if (tid == 0) g_odata[ blockIdx. x] = sdata[0];
}Final Optimized Kernel 36
Performance Comparison
0.010.1110
131072262144
524288
1048576
2097152
4194304
8388608
16777216
33554432
# ElementsTime (ms)1: Interleaved Addressing:
Divergent Branches
2: Interleaved Addressing:
Bank Conflicts
3: Sequential Addressing
4: First add during global
load
5: Unroll last warp
6: Completely unroll
7: Multiple elements per
thread (max 64 blocks) 37
Types of optimization
Interesting observation:
Algorithmic optimizations
Changes to addressing, algorithm cascading
11.84x speedup, combined!
Code optimizations
Loop unrolling
2.54x speedup, combined 38
Conclusion
Understand CUDA performance characteristics
Memory coalescing
Divergent branching
Bank conflicts
Latency hiding
Use peak performance metrics to guide optimization 
Understand parallel algorithm complexity theory
Know how to identify type of bottleneck
e.g. memory, core computation, or instruction overhead
Optimize your algorithm, then unroll loops
Use template parameters to generate optimal code
Questions: mharris@nvidia.com Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Hochschule OffenburgOverview
•CUDA-optimized algorithms
–Warp-internal functions for optimization
–Atomic operations on recent CUDA architectures 4 Hochschule OffenburgExample: Reduction (Redux)
•Aggregate an input of nvalues to one single output
value
–E.g. sum, min, max, product, count , …
•Sequential algorithm: O(n) time 
•Parallel algorithm with tree-like structure
–O(log n) time with O(n) processors
•Can be improved to O( n/ log n) processors
→Cost complexity O( n) →cost-optimal 
–Requires synchronization between each level of the tree
•In CUDA: __syncthreads() 5 Offenburg UniversityParallel reduction algorithm 6 Hochschule OffenburgReduction
•Lots of potential for optimization (see Slides by Nvidia)
–Shared memory
–Avoiding memory bank conflicts
–Sequential computation per thread (algorithm cascading)
–…
•Today : use SIMD characteristics of CUDA
–Direct access to registers of neighboring threads
–No synchronisation required inside a warp
•Also: use atomic operations
–Very efficient on recent CUDA architectures (not on older ones) 7 Hochschule OffenburgRecap: Warps
•A warp is a bundle of 32 „neighboring “ threads
within a thread-block
–All threads operate in lockstep („im Gleichtakt“), 
i.e. they carry out the same instruction at the same clock cycle
•Exception: threads that don‘t have anything to do (because of a 
branch) do nothing
•SIMT (single instruction multiple thread ), „almost “ SIMD
–Inside the same warp, the thread IDs % 32 are 0..31 
•i.e. for the first thread of a warp, threadId.x % 32 == 0 8 Hochschule OffenburgWarp-internal operations
•Threads of the same warp have access to registers of
other threads in the same warp
–Can use other threads ‘ values of variables, share or broadcast their
own values , …
•How can this be done (from a language point of view)?
–Problem: variable names are the same in all threads
–Solution: Special warp-internal commands (intrinsics)
•__ballot()
•__shfl()
•… 9 Hochschule OffenburgThe shuffle command (SHFL)
•__shfl(intvar, intsourceThread, intwidth)
•__shfl_down (intvar, intthreadDelta, intwidth)
•__shfl_up (intvar, intthreadDelta, intwidth)
•__shfl_xor (intvar, intbitMask, intwidth)
Returns the value of var of the thread in the same (part of the) warp 
(of sizewidth ), whose ID is given by the second parameter. 10 Offenburg UniversityExample: Shuffle-down
inti = threadIdx.x % 32; 
intj = __shfl_down (i, 2, 8);
Source: https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/ 11 Hochschule OffenburgAdvantages
•Sequence of several shared memory instructions is 
replaced by one single instruction
–Increases the bandwidth
–Reduces latency
•Requires noshared memory
–can be used for other things (limited resource)
•Synchronization in warp is implicit in every instruction, 
hence noneed for__syncthreads()
–Less block-wide synchronization required, which otherwise would
decrease performance 12 Hochschule OffenburgOptimization of the reduction algorithm
•Idea:
–Reduction per warp
•using shuffle
–Reduction of all warp results in a block
•again using shuffle
–Reduction of all block results of the kernels
•by calling a second kernel
–The above limits the number of blocks (≤ 1024 = 32*32)
•Solution: start with a sequential reduction per thread 13 Hochschule OffenburgWarp reduction
__device__ int warpReduceSum (int val) { 
for (int offset = warpSize/2; offset > 0; offset /= 2) 
val += __shfl_down (val, offset); 
return val; 
} 14 Hochschule OffenburgBlock reduction using warp reduction
__device__ int blockReduceSum (int val) { 
static __shared__ int shared[32]; // Shared mem for 32 partial sums 
int lane = threadIdx.x % warpSize; // thread id inside warp
int wid = threadIdx.x / warpSize; // warp id inside block
val = warpReduceSum (val); // Each warp performs partial reduction 
if (lane==0) shared[wid]=val; // Write reduced value to shared memory 
__syncthreads(); // Wait for all partial reductions 
//read from shared memory only if that warp existed 
val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;   
if (wid==0) val = warpReduceSum (val); // Final reduce within first warp
return val; 
} 15 Hochschule OffenburgComplete kernel
__global__ void deviceReduceKernel (int *in, int* out, int N) { 
int sum = 0; 
// sequentially pre-reduce data to size of grid
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; 
i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread writes out block result
out[blockIdx.x]=sum; 
} 16 Hochschule OffenburgKernel call
void deviceReduce(int *in, int* out, int N) { 
int threads = 512; // enough to saturate GPU
int blocks  = min((N + threads - 1) / threads, 1024);
deviceReduceKernel<<<blocks, threads>>>(in, out, N);
// Second kernel call for final reduction
deviceReduceKernel<<<1, 1024>>>(out, out, blocks); 
} 17 Hochschule OffenburgFurther optimization?
Potentially inefficient:
•Second kernel call
–Under-occupies GPU (only one SM is used)
–Global synchronization between kernels
•Need to wait for every thread inevery block
•Block reduction uses __syncthreads()
–Block-wide synchronization
•Need to wait for every thread in that block 18 Hochschule OffenburgAtomic Add
•Thread-safe addition of a value to a variable
–Implicitly synchronized –no manual synchronization required
•Syntax:   atomicAdd(variable, valueToBeAdded);
•Attention: this is a blocking operation!
–Only 1 thread per cycle can add
–Threads in same warp will be serialized
–Danger of contention of threads („Staugefahr“) 19 Hochschule OffenburgPrevious kernel
__global__ void deviceReduceKernel (int *in, int* out, int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread writes out
out[blockIdx.x]=sum; // block result
} 20 Hochschule OffenburgNew kernel
__global__ void deviceReduceBlockAtomicKernel (int *in, int* out, 
int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread atomically adds
atomicAdd(out, sum); // block result to final result
} 21 Hochschule OffenburgWhat has changed?
• Don‘t need a second kernel call
–Each block directly adds its own partial result to final result
•Only 1 thread per block will callatomicAdd
–Danger of contention is small
•But: atomicAdd synchronizes implicitly
–Can this be more efficient?
•Yes! Throughput increased by up to 10%
•But only works on recent CUDA architectures (Kepler and later)
•Noticeable for Nbetween 200.000 and 100.000.000 22 Hochschule OffenburgFurther optimization?
Potentially inefficient:
•Second kernel call
–Under-occupies GPU (only one SM is used)
–Global synchronization between kernels
•Need to wait for every thread inevery block
•Block reduction uses __syncthreads()
–Block-wide synchronization
•Need to wait for every thread in that block
•Idea: even more atomics instead of block reduction 23 Hochschule OffenburgPrevious kernel
__global__ void deviceReduceBlockAtomicKernel (int *in, int* out, 
int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread atomically adds
atomicAdd(out, sum); // block result to final result
} 24 Hochschule OffenburgNew kernel
__global__ void deviceReduceWarpAtomicKernel (int *in, int* out, 
int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = warpReduceSum (sum); 
if (threadIdx.x & (warpSize-1) == 0) // first thread in warp
atomicAdd(out, sum);   // adds warp result to final result
} 25 Hochschule OffenburgWhat has changed?
•No block reduction at all!
–Each warp directly adds its own partial result to final result
•1 Thread per warp will callatomicAdd
–Higher danger of contention than before!
•Still more efficient?
–Yes! Throughput increased by up to 50%
•But only works on recent CUDA architectures (Kepler and later)
•Noticeable for Nbetween 150.000 and 50.000.000 26 Hochschule OffenburgSummary
•Warp internal functions can use SIMD-like features to
avoid blockwide synchronization ( __syncthreads )
•Atomic operations
–Performance has been greatly improved in recent CUDA 
architectures!
–Can increasingly be used als alternative to explicitly synchronized
algorithms To appear : Workshop on High- Dimensional Data Mining (HDM 821)
im Rahmen der IEEE International Conference on Data Mining (ICDM 921)Accelerating Density-Based Subspace 
Clustering in High-Dimensional Data
Jürgen Prinzbach, Tobias Lauer, Nicolas Kiefer Data Clustering
▪Find (disjoint) subsets of similar data 
in large/complex data sets
▪Similarity defined by distance function
▪Euclidean distance, Manhattan distance, 
Hamming distance, ... 
▪Basic approaches:
▪Segment the space (e.g. k-means)
▪Divide the points based on density 
(e.g. DBSCAN)
oCluster = 
Set of g kPoints with distance < ε
(k,εselectable parameters)
oAlso supports outliers High-Dimensional Data
▪High number of attributes, possibly greater than the number of data points
▪(Global) distances lose their significance
▪Differences in a few dimensions →high distance despite “similarity”
▪Points are always far apart; there is no “good” value for ε
→Clusters are not found
Solution approaches:
▪Dimension reduction
▪Principal component analysis (PCA)
▪Deep neural networks
▪Subspace clustering Subspace clustering
Search for clusters in subspaces, i.e. in spaces that are 
each spanned by only a subset of the attributes
▪Problem: dAttributes →2dSubspaces
▪Checking all subspaces is often impossible (2100> 1 quadrillion)
▪Necessary: ​​Avoid redundant calculations
▪Known existing approaches: 
CLIQUE, MAFIA, SUBCLU, SUBSCALE SUBSCALE (Kaur & Datta 2015)
(1)Identify clusters in each 1-dimensional subspace
(2)Determine Calle subsets of size k for each cluster (dense units = subclusters of smallest size)
(3)Check whether these also represent dense units in other dimensions →using hash collision: each dense unit receives a unique signature that is hashed
(4)In this way, build successively higher-dimensional dense units until they have maximum dimensionality
(5)Combine overlapping dense units in each subspace to form clusters of maximum size
*uniquely “with very high probability” (Erdos & Lehner, 1941) SUBSCALE (Kaur & Datta 2015)
(1)Identify clusters in each 1-dimensional subspace
(2)Determine Calle subsets of size k for each cluster
(dense units = clusters of smallest size) 
(3)Check whether these are also dense units in other dimensions
(4)In this way, build successively higher-dimensional dense units until they have 
maximum dimensionality
(5)Combine overlapping dense units in each subspace to form clusters 
of maximum size
Pro: Only 1 pass per dimension: O(d) instead of O(2d)!
Con: Passes (steps (2) & (3)) are computationally intensive :
Determine all subsets of size k
There is |㔶| Such subsets per cluster C
→combinatorial explosion
Running time not in O(2d∙n), but in O(d∙ nk)
In addition: High space requirement for hash table Acceleration through parallelization
Possibilities:
▪Parallel processing of dimensions
▪Brings less results than hoped (Datta et al. 2017)
▪Parallel processing of parts of the hash table
▪Scales linearly with #processors, but with higher overall work
(1 data pass per partition per dimension)
→Real time gain <50%
▪Parallel processing of individual dense units
▪Very fine-grained
▪Many (millions) small tasks
→GPU as suitable hardware Acceleration through parallelization
▪GPU: many processors (>5000)
▪Data parallelism: 
GPU threads process identical tasks, but on different 
inputs (“Single Instruction Multiple Thread” – SIMT)
▪Here: Each GPU thread processes 1 dense unit:
Thread i:
(1) Determine the i-th subset (dense unit)
(2) Calculate its unique signature
(3) Enter the subset into the hash table based on the signature “Parallel enumeration” of subsets
▪Subsets (dense units) could be determined in a simple way by
enumerating in (co)lexicographic order.
▪Subset ican be determined in O(k)
▪But: for subset im, subset i-1 must already be known
▪Sequential, not parallelizable
▪Alternative:
Determine each subset independently, only based on its number i
▪Disadvantage: More complex per subset: O(k∙ log n)
▪But: subsets can be calculated in parallel Binomial decomposition of a number i
▪Theorem (cf. Kruskal 1963, Katona 1968)
Every natural number i can be uniquely represented for every k> 0 as the sum of exactly k binomial coefficients (with 0 f n1< n2< … < nk):
㕖=㕛1
1+㕛2
2+ ⋯ +㕛Ā
㕘
We call this the binomial or combinatorial decomposition of i.
•Observation: 
n1,…,nk correspond to the element numbers of the i-th subset of C Direct calculation of the i-th subset
Example: i=7, k= 4 
7=㗎
1+ÿ
2+Ā
3+㗓
4= 0 + 1 + 1 + 5
The set { 0, 2, 3, 5 } is just the 7th subset with 4 elements in 
colexicographic enumeration:
0: {0,1,2,3} 4: {1,2,3,4} 8: {1,2,3,5} 
1: {0,1,2,4} 5: {0,1,2,5} 9: {0,1,4,5} 
2: {0,1,3,4} 6: {0,1,3,5} 10: {0,2,4,5} 
3: {0,2,3,4} 7: {0,2,3,5} 11: {1,2,4,5} Data parallel algorithm
Calculates for a 1-dimensional cluster C= { p1, p2, …, pN} in 
Dimension dalle dense units and enters these into the hash table Hein:
1fori = 0 to㕁
Ā-1 in parallel
2 find n1,…,nksuch that 㕖 =σÿ=1Ā 㕛ÿ
ÿ
3 Di= 㕝㕛1, … , 㕝 㕛Ā
4 calculate signature S(Di)
5 ifHcontains S(Di)
6 append dto dimension list of element S(Di)
7 else
8 insert new element S(Di) withd in dimension list Implementation and tests
▪Master's thesis (Nicolas Kiefer, INFM)
▪Implementation for NVIDIA GPUs with CUDA/C++
▪Efficient GPU hashing method "Stadium Hashing" (Khorasani et al., 2015)
▪Comparative tests with previous methods
▪Results
▪GPU algorithm integrated into SUBSCALE method
▪Acceleration by a factor of about 5 compared to

previous version Signatures and hash table partitions
▪Large amounts of data
→Hash table too large for RAM
▪Simple solution: Partition the hash table (according to hash value = signature)
▪Iterate over all pPartitions
▪Run the algorithm for each partition
▪Only hash the signature if it falls into the current partition
January 27, 2023 16 Signatures and hashtable partitions
▪SUBSCALE divides the range of hash values ​​into equally wide partitions
▪But signatures (= sums of random numbers) are not evenly distributed
▪Most of the partitions are only slightly filled → inefficient
January 27, 2023 17 Further optimization
▪Adaptation of partition sizes
to the distribution function
▪Approx. 50% saving in storage space 
or partitions
▪Approx. 40% time saving for entire
clustering
January 27, 2023 18 Implementation and tests
▪Tests
▪GPU algorithm integrated into SUBSCALE process
▪Acceleration by a factor of about 5 compared to

previous version Performance Tests
▪Overall Runtime
▪SUBSCALE
▪DBSCAN (merge dense units)
▪SUBSCALE
▪Calculate partitions
▪Merge partitions
January 27, 2023 20 Calculating a partition – time shares 
▪Speedup calculation+hashing: ~70x
January 27, 2023 21
Dense unit calculation
and hashing
Write to diskTransform 
hash tableClean up
hash table
Copy dataReset
hash table(s) GPU-accelerated Weighted Aggregation and Disaggregation  
in Multidimensional Databases 
Tobias Lauer  
Hochschule Offenburg Business Intelligence and Corporate Planning Online Analytical Processing (OLAP) 
• Data modeled as multidimensional <cube=  
 
–Dimensions are structured hierarchically: 
•Base elements 
•Consolidated elements 
 
–Data cube operations:  
•Slice, dice, pivot across dimensions 
•Roll-up, drill-down along hierarchies 
 
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget Multidimensional aggregation 
•Pre-computation of aggregates 
–not possible for all aggregates (dimensional 
explosion) 
–not desired 
 
•Online aggregation 
–On demand 
–Useful for planning scenarios (interactive 
write-back) 
–Common when data is stored 
in-memory 
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget In-memory OLAP storage model 
•All data stored in main memory  
  
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 10 11 3 0 14 2 11 0 13 0 0 0 0 37 9 3 4 16 0 0 7 7 5 1 0 6 4 0 0 4 33 10 1 0 11 8 6 3 17 0 2 0 2 0 0 0 0 30 20 9 8 37 19 9 10 38 7 14 0 21 4 0 0 4 100 1 2 0 3 4 0 0 4 0 1 6 7 3 0 1 4 18 0 3 1 4 6 2 0 8 10 0 7 17 0 0 0 0 29 6 0 5 11 0 9 0 9 3 3 2 8 0 0 0 0 28 7 5 6 18 10 11 0 21 13 4 15 32 3 0 1 4 75 27 14 14 55 29 20 10 59 20 18 15 53 7 0 1 8 175 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget In-memory OLAP storage model 
•All data stored in main memory  
•Only store base cells 
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 11 3 0 2 11 0 0 0 0 9 3 4 0 0 7 5 1 0 4 0 0 10 1 0 8 6 3 0 2 0 0 0 0 1 2 0 4 0 0 0 1 6 3 0 1 0 3 1 6 2 0 10 0 7 0 0 0 6 0 5 0 9 0 3 3 2 0 0 0 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget In-memory OLAP storage model 
•All data stored in main memory  
•Only store base cells 
•Do not store zero-value cells 
 
 Memory saving, data consistency 
 
 
 
 
 1 3 5 2 14 2 8 12 16 8 5 7 9 12 Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 11 3 2 11 9 3 4 7 5 1 4 10 1 8 6 3 2 1 2 4 1 6 3 1 3 1 6 2 10 7 6 5 9 3 3 2 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget 
Represent cells as ( key, value ) pairs, 
e.g. 
( (2, 1, 0) , 4.0 ) 
Note: Values are double precision! In-memory OLAP storage model 
•All data stored in main memory  
•Only store base cells 
•Do not store zero-value cells 
 
 Memory saving, data consistency 
 
•Compute other cells on the fly  
when needed 
 
 Use GPU to accelerate 
 
 1 3 5 2 14 2 8 12 16 8 5 7 9 12 Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 11 3 2 11 9 3 4 7 5 1 4 10 1 8 6 3 2 1 2 4 1 6 3 1 3 1 6 2 10 7 6 5 9 3 3 2 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget 27 14 14 55 29 20 10 59 20 18 15 53 7 0 1 8 175 GPU Two algorithmic approaches to GPU aggregation 
•Target-driven 
–For each target  cell 
•find relevant base cells (parent-to-child map) 
•aggregate 
 
•Source-driven 
–Pre-filter base cells relevant for area of all target cells 
–For each base  cell 
•create all relevant target paths (child-to-parent map) 
•look up in hashmap 
•add to aggregate Jan Feb Mar Apr May Jun Jul Aug Sep Dec Nov Oct Q1 Q2 Q3 Q4 Year 
Jan Feb Mar Apr May Jun Jul Aug Sep Dec Nov Oct Q1 Q2 Q3 Q4 Year Target-driven aggregation 
•Fast parallel aggregation step 
 
 
•Utilizes shared, global and constant memory 
•Coalesced memory access 
•Almost no thread divergence  
 
 Multi-GPU solution 
Performance optimized 
bulk aggregations 
2-step prefiltering Target-Driven Aggregation 
(3) Aggregation on GPU 
–Parallel reduction 
–Completely done in  
shared memory of GPU 
 
 
 
Copy result back to CPU 
 
(4) Final summarization on CPU Timing results 
050100150200250300
99%50%34%20%17% 5% 3% 1%
0.60%
0.30%
0.10%
0.04%CPU
GPUSelectivity CPU GPU Speedup  
99% 298ms 7ms 42.6x 
50% 146ms 7ms 20.6x 
34% 106ms 3ms 35.3x 
20% 59ms 4ms 14.8x 
17% 50ms 3ms 16.6x 
5% 20ms 3ms 6.7x 
3% 13ms 3ms 4.3x 
1% 7ms 3ms 2.3x 
0.6% 4ms 2ms 2.0x 
0.3% 2ms 2ms 1.0x 
0.1% 1ms 2ms 0.5x 
0.04% 1ms 2ms 0.5x Test data: 
3M records 
7 dimensions Optimizing groups of queries 
•Fact table prefiltering 
–<Stream compaction=  
 
•Simultaneous calculation  
of multiple queries 
–Fewer kernel launches Sparse  target areas 
•Sparsity: most cell values in an area of computed cells are 0, because 
no underlying cells exist 
 
•With target-driven aggregation, those are still „computed <  
–Lookup of base cells  
–Non-neglibile cost 
 
•Too expensive: 0-value computation dominates processing time in 
sparse areas Handling large sparse areas 
•Requirements  
Performance and memory consumption that correlate 
with number of non-zero  target cells 
•Solution 
Source-driven approach  
 - Serialized aggregation with atomics 
 - Utilize hash tables on GPU            source cells 
target cells + + + + + + Source-driven aggregation 
Jan ... Apr parent 
 map Q1 Q2 Year ...  ... ...  ... ... ...  ... ... ...  ... ... ...  ... ... ... 
... 10 
atomic add h(x) ... ... ... ... ... ... ... ... ... Jan, 2011  
sold units  
Q1, all years  
sold units  source cells 
target cell  
hash table Year 
... 10 target cell area  
+ ... 10 + Year, all years  
sold units  
atomic add Atomics: Contention 
thread serialization Great improvement 
 in Fermi and Kepler 
 over CC 1.X ...  ... ...  ... ... ...  ... ... ...  ... ... ...  ... ... ... ... 
h(x) ... ... ... ... ... ... ... ... ... 
... ... + 
atomic add Reducing contention 
...  ... ...  ... ... ...  ... ... 
h1(x) ... ... ... ... ... ... ... ... ... __ballot : 
merge with 
first? 
+ 
warp 
preaggregation warp 1 
...  ... ...  ... ... ...  ... ... 
+ warp 2 
h2(x) warp-wise 
different 
hash 
functions Speedup: Small target areas (1-11 cells) 
GPU Target Driven GPU Source Driven  Speedup vs. CPU 
Speedup vs. CPU 
Selectivity Selectivity 55,9 
54,9 
27,8 76,7 
26,4 
13,7 
3,5 2,0 0,9 
0,010,020,030,040,050,060,070,080,0
0,00001 0,0001 0,001 0,01 0,1 113,2 
12,9 18,3 
9,5 
3,0 1,6 0,7 0,5 0,1 0,010,020,030,040,050,060,070,080,0
0,00001 0,0001 0,001 0,01 0,1 1
Database:  1B records (filled cube cells) 
GPU:  3x Tesla C2070 (18 GB RAM) Larger areas  
Calculation times Speedup Time in seconds 
Database:  40M records (filled cube cells) 
GPU:  2x Tesla K20 (10 GB RAM) 15,6 35,0 
3,2 3,6 
0510152025303540
1547 target cells 38012 target cellsCPU
GPU source driven
4,9 9,7 
0,0 2,0 4,0 6,0 8,0 10,01547 target cells38012 target cells
Speedup over CPU CPU algorithm  
(multi-core) Comparison: aggregation algorithms 
•CPU algorithm good for 
low aggregation 
 
•Target driven algorithm: 
good for small and/or 
dense target areas 
 
•Source driven algorithm: 
optimized for large and 
sparse areas 
 Target size Selectivity 
GPU 
target 
driven 
GPU source 
driven 
Sparsity Remaining challenges  
•Decision mechanism 
–When GPU, when CPU? 
–Which GPU algorithm in which situation? 
 
•Find suitable thresholds 
 
•What about large and dense  target areas?  
 GPU memory problem Writeback in Top-Down Planning 
• Writeback: =opposite direction= 
of aggregation (disaggregation) 
 
•Value inserted at high level of 
aggregation is broken down to 
lower levels until the base level 
 
•All underlying base cells are 
modified, depending on the type 
of writeback Ranges and areas 
•Base elements in each dimension are 
collected in ranges   
 
D0: { [0,0] , [2,2] }        | D0| = 2 
 
D1: { [0,0] , [2,3] }        | D1| = 3 
 
D2: { [0,2] }         | D2| = 3 
 
 
•The Cartesian product of ranges 
across all dimensions forms an area 
D0× D1× D2 Multiply-base distribution Multiply-base distribution Multiply-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 240 Multiply-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 240 Multiply-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 240 
x2 Multiply-base distribution 
16 3 24 10 21 35 
4 26 58 31 17 32 
67 48 16 13 16 26 
51 10 54 78 73 44 24 92 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 480 Multiply-base distribution 
16 3 24 10 21 35 
4 26 58 31 17 32 
67 48 16 13 16 26 
51 10 54 78 73 44 24 92 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 480 Set-base distribution 
•Every relevant base cell in the area 
is set to the same given value 
 
•Naïve approach:  
search for all relevant paths and 
replace cell values 
–Problem: what about zero-value cells, 
which are not represented? 
 
•Better approach: 
(1) Delete all existing cells in area 
(2) Create all cells in area with new 
value Set-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B 10 Set-base distribution 
16 3 24 10 21 35 
4 10 10 10 31 17 10 10 
67 10 10 10 13 10 10 
10 10 10 10 10 
51 10 10 10 73 44 10 10 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B 10 Set-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 Set-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 Set-base distribution 
16 3 24 10 21 35 
4 31 17 
67 13 
51 73 44 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 Set-base distribution 
16 3 24 10 21 35 
4 10 10 10 31 17 10 10 
67 10 10 10 13 10 10 
10 10 10 10 10 
51 10 10 10 73 44 10 10 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 Parallel creation of all cell paths in an area 
• <Parallel enumeration= of the 
area: 
–Each thread computes the path of  
=its= cell from the thread ID  
–Problem:   
•Gaps between ranges of a 
dimension prevent simple iterations  
•Iterating over all ranges and counting 
all visited elements is inefficient 
–Solution: 
•Represent ranges by pre-calculated 
prefix sums  (rather than start and 
end points) Prefix sum representation of ranges 
(1)  Find smallest m such that  r[m] ≥ k 
(2)  i = g[m] + k - 1 
Prefix sums of gap lengths:   g = 
Prefix sums of range lengths:  r = 
Index i of kth relevant element in D: Add-base distribution 
•The same given value v is added  
to the value of each relevant base 
cell  
•Approach: 
–Create all cells of the area and set 
value to v (as before) and store them 
temporarily 
–Find all previously  existing relevant 
cells and add their (old) value to the 
one in the new temporary area 
–Delete old relevant cells and persist  
temporary storage Add-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B+5 Add-base distribution 
16 3 24 10 21 35 
4 13
+5 29
+5 +5 31 17 16
+5 +5 
67 +5 24
+5 8 
+5 13 8 
+5 13
+5 
+5 +5 +5 +5 +5 
51 5 
+5 27
+5 39
+5 73 44 12
+5 46
+5 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B+5 Add-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5  27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 5 5 5 5 5 
5 5 5 5 5 
5 5 5 5 5 
5 5 5 5 5 Add-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5  27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 18 34 5 21 5 
5 29 13 13 18 
5 5 5 5 5 
10 32 44 17 51 Add-base distribution 
16 3 24 10 21 35 
4 31 17 
67 13 
51   73 44 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 18 34 5 21 5 
5 29 13 13 18 
5 5 5 5 5 
10 32 44 17 51 Add-base distribution 
16 3 24 10 21 35 
4 18 34 5 31 17 21 5 
67 5 29 13 13 13 18 
5 5 5 5 5 
51 10 32 44 73 44 17 51 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 340 Performance tests 
Timings 
(in ms) CPU  
Intel DualCore  2x GPU  
GeForce 260  3x GPU  
Tesla C1060  4x GPU  
Tesla C2050  
Multiply-base 1 3,548  466  (7.6 x)  558  (6.4 x)  435   (8.2 x)  
Refresh 1 1,131  200  (5.7 x)  127  (8.9 x)  74 (15.3 x)  
Sum 4,679  666 ( 7.0 x ) 685 ( 6.8 x ) 509  ( 9.2 x ) 
Multiply-base 2 21,542  513   (42 x)  580   (37 x)  448  (48 x)  
Refresh 2 5,508  961  (5.7 x)  617  (8.9 x)  347  (16 x)  
Sum 27,050  1,474  ( 18 x) 1,197  ( 23 x) 795  ( 34 x) 
Timings 
(in ms) CPU  
Intel DualCore  2x GPU  
GeForce 260  3x GPU  
Tesla C1060  4x GPU  
Tesla C2050  
Set-base 14,979  900   (17 x)  715  (21 x)  572  (26 x)  
Refresh 5,598  962  (5.8 x)  610 (9.2 x)  347  (16 x)  
Sum 20,577  1,862  ( 11 x) 1,325 ( 16 x) 919  ( 22 x) 
Timings 
(in ms) CPU  
Intel DualCore  2x GPU  
GeForce 260  3x GPU  
Tesla C1060  4x GPU  
Tesla C2050  
Add-base 110,387  1,465   (75 x)  899 (123 x)  872 (127 x)  
Refresh 5,621  953  (5.9 x)  608      (9 x)  346   (16 x)  
Sum 116,008  2,418  ( 48 x) 1,507   ( 77 x) 1,218  ( 95 x) Speed-up factors (compared to CPU) 
2x GeForce 2603x Tesla C10604x Tesla C2050020406080100
7x 11x 18x 48x 
7x 16x 23x 77x 
9x 22x 34x 95x 
Add-base 
Multiply-base 2 
Multiply-base 1 Set-base Concluding remarks 
•Top-down planning creates and/or manipulates large numbers of data 
records 
•These updates are systematic  and structured 
–Involved data points can be enumerated 
–Perfectly suited for SIMD-like parallelization on GPUs 
•Increased work compared to CPU algorithm 
–but pays off in speedup! 
•CUDA implementation up to 95x faster compared to sequential CPU 
algorithm 
•Easy scaling to multiple GPUs 
