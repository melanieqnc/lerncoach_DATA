Offenburg University of Applied Sciences
Economics
Outline, overviews, exercises Offenburg University of Applied Sciences
General information about the course
The aim of the course is to give students an introductory overview of the scientific principles of economics, to create an understanding of the basic models of supply and demand and to demonstrate macroeconomic relationships. The focus is on providing an orientation framework for classifying economic issues. The structure and overviews form the framework of the lecture, but only reflect a fraction of the lecture content and are not a substitute for attending the lecture. The literature provided is recommended as preparation. The practice questions are for self-assessment. 2 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
References
The lecture mainly refers to the following textbooks (primary literature):
Roth, S. J. (2016): Economics for beginners, 5th edition, Konstanz and Munich.
Mankiw , N. G./Taylor, M. P. (2018): Basics of economics, 7th edition, Stuttgart.
Supplementary:
Bartling, H./ Luzius , F./Fichert , F: (2019): Basics of economics, 18th edition, Munich.
Bofinger, P. (2015): Basics of economics, 4th edition, Hallbergmoos.
Beck, H. (2014): Behavioral Economics, Wiesbaden.
Hermann, M. (2016): Workbook Basics of economics, 5th edition, Stuttgart.
3 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Structure
Part 1: Introduction to economics
Part 2: The theory of households
Part 3: The theory of companies
Part 4: Market equilibrium
Part 5: The market failure theory
Part 6: Macroeconomic data
Part 7: Long-term real economic development
Part 8: Interest rates, money and prices
Part 9: Economic fluctuations
Part 10: Selected current topics for further study and discussion
4 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Part 1: Introduction to economics
Key points:
•What is economics
•Economic rules for getting started
•History of economics, social market economy
•Differentiation between micro and macroeconomics
•Methodological approach
•Utility maximization, shortages and opportunity costs
•Marginal consideration
•Exchange, trade, comparative advantages and relative prices
•Pareto criterion and allocative efficiency
•Function of prices and competition
5 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
What is economics?
Economics is the science of managing scarce social resources. There are three basic questions:
•What goods and services should be produced?
•How much of them should be produced?
•Who should receive the goods and services produced?

The scarce resources (production factors) are roughly divided into three categories:
•Land in the sense of natural resources,
•Labor in the sense of mental and physical human performance that goes into production,
•Capital in the sense of real capital, e.g. equipment and facilities that are needed to produce goods and services.
See Mankiw /Taylor (2018), pp. 1 -3.
6 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Economic rules to get you started
Economics deals with human decision-making behavior, e.g. 
how much work is done, what is bought, how much is saved. 
•All people are faced with alternatives that need to be weighed up (“There is no such thing as a free
lunch”).
•The cost of a good consists of what you give up in order to get something else (opportunity costs). Decisions require comparing the costs and 
benefits of alternative actions. 
•Rational decision-makers think in terms of marginal changes.
•People react to incentives, e.g. taxes and subsidies. 
•Trade can make everyone better off.
•Markets are usually good for organizing economic life, with price being the instrument for coordinating economic activities.
•Governments can sometimes improve market outcomes, e.g. in the event of market failure. •The standard of living of a country depends on its ability to produce goods and services.
See Mankiw /Taylor (2018), pp. 3 -12.
7 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
History of economics
Economics is a sub-discipline of economics, which is part of the humanities. 
•Predecessor: Aristotle (384 –322 BC): He emphasized the importance of 
private property and contrasted the supply economy with the commercial economy.
•Mercantilism (16th – 18th century): Basic idea: wealth instead of justice. Representatives: 
Jean Bodin, John Locke, John Law (invented paper money).
•Physiocracy (ca. 1750): Basic idea: nature instead of culture. Representative: Fran çois Quesnay 
(developed the first circular flow model). 
•Classics (18th – 19th century): Basic idea: innate right to personal freedom.
Representatives: Adam Smith (main work: An Inquiry into the Nature and Causes of the Wealth of Nations ), David Ricardo, John Stewart Mill.
•Neoclassicists (19th - 20th century): Basic idea: Marginal utility is the determining factor for the price. Representatives: L éon Walras (equilibrium model), Vilfredo Pareto 
(welfare economics), Arthur Pigou .
•20th century: Representatives: John Maynard Keynes (idea of ​​stimulating demand 
by the state), Paul Samuelson , Milton Friedman, especially in Germany: Walter 
Eucken (intellectual father of the social market economy) 
8 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Social market economy
The social market economy embodies an economic model that was implemented in Germany from 1948 onwards against the background of market failure by A. Müller-Armack and L. Erhard. 
It takes up the demands of ordoliberalism (Freiburg School, W. Eucken). At its core, it is about the state guaranteeing a functioning competitive order and a social, but market-compliant orientation of economic policy. 
Where market failure is to be feared, the state should intervene while respecting the 
subsidiarity principle. It states that the individual is as far as possible 
responsible for his own life, but can count on state solidarity in an emergency. Personal responsibility should be strengthened and the solidarity community should be protected from being overburdened. 
9 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Discussion
1. "Dare to be more Adam Smith" (FAZ, August 21, 2020)
➢Thesis: "The economy in Europe must become more productive."
➢According to Adam Smith, there is a positive connection between a country's labor productivity and its prosperity.
2. "What economists owe to David Hume" (FAZ, November 13, 2020)
➢Thesis: Hume's claim that economic development should serve to improve the situation of broad sections of the population is still relevant.
➢Hume as an economic order economist: "Like other thinkers of the Scottish Enlightenment, Hume also saw the need for institutions to direct the self-interested actions of individuals into socially acceptable channels."
10 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Differentiation between micro and macroeconomics
Microeconomics examines the economic behavior of individual economic entities and thus equilibria or imbalances in individual markets. 
The basic question of microeconomics is: How do markets work and where do they fail? This involves the optimal allocation (distribution) of given, scarce resources to different uses. 
Macroeconomics examines macroeconomic variables (e.g. total consumption, gross domestic product) and thus the equilibrium or imbalance of macroeconomic supply and macroeconomic demand. Since macroeconomic variables are influenced by economic policy, macroeconomics provides theoretical foundations for economic policy decisions. 
11 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Methodological approach
Economics asks about dependencies (for example, how does size A depend on size B?). 
Dependencies can be represented mentally using functions. 
The assumed explanatory relationships are called hypotheses. 
A mental system of effects that reduces the complexity of reality to a system of hypotheses and definitions is a model. When creating models, economists often use the ceteris paribus clause, in which all influencing factors, except for the factor being examined, are assumed to be constant. 
Models contain several variables, e.g. demand quantity, prices. The 
demand quantity, for example, is an endogenous variable, the value of which is determined by the model. In this case, the price is an exogenous variable, the value of which is determined outside the model. It is important to distinguish between cause and effect. Is 
a change in price the cause of a change in the quantity demanded 
or does it affect the quantity demanded?
See Mankiw /Taylor (2018), pp. 26 -35.
12 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Utility Maximization and Scarcity
13 Prof. Dr. Sybille SchwarzEconomic theory assumes that individuals maximize their utility, focusing on an ordinal concept of utility. At the same time, it is assumed that individuals behave rationally, i.e. consistently to maximize utility. Scarcity is the basic problem of people. Economics in the narrower sense is the science of managing scarce resources within society. For the economist, unlike the accountant, costs are not just the expenditure in monetary units. Costs arise from foregoing the utility of the alternative use (opportunity). Opportunity costs express what must be given up in order to obtain something else. This is not just about the use of money, but about all resources for which a decision is made between alternatives, e.g. time. See Roth (2016), pp. 6 -12. Offenburg University of Applied Sciences
Discussion
We do not always act rationally! Some typical analysis errors (keyword: 
behavioral economics), e.g.
➢Perception distortions: through framing (influencing decisions 
through the type and context of the problem formulation), anchoring (influencing the 
decision through currently available information despite limited relevance
➢Status quo distortion: negative assessment of a deviation from the status quo 
despite its advantages
➢Sunk costs: influencing the decision through the costs of previous and irreversible decisions 
➢Confirmation bias: only information that corresponds to expectations is perceived
See Mankiw/Taylor (2018), pp. 164 -166.
14 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Exercise on opportunity costs
Farmer Michel gives guitar lessons for €20/hour. In one day he spends 10 hours planting the cuttings he bought for €200, which he wants to sell for €400 in the fall. What profit (accounting and economic in terms of opportunity costs) has he made?
15 Prof. Dr. Sybille Schwarz
Accounting: 400-200 = €200
Economic (opportunity costs): 400-200-(20*10) = €0 Offenburg University of Applied Sciences
Marginal consideration
16 Prof. Dr. Sybille SchwarzSee Roth (2016), p. 12 -14. Many decisions relate to small (marginal) changes to existing activities. Benefits and costs per unit often differ depending on how many units have already been consumed or invested. For a specific supply or demand decision, the marginal benefit (additional benefit of the marginal change) and the marginal costs (additional costs of the marginal change) are then relevant. Discussion question: Is the marginal benefit of a glass of water large or small? Offenburg University of Applied Sciences
Exchange, trade, comparative advantages and relative prices
17 Prof. Dr. Sybille SchwarzSee Roth (2016), pp. 14 -20.Exchange and trade enable specialization in activities that one can do better than other activities. Comparative advantages are crucial, not absolute advantages. For this purpose, the opportunity costs are expressed as relative prices in units of the other good. The theory of comparative cost advantages goes back to the English economist David Ricardo (1772 -1823). Practice example:
Farmer Meier and Schmitt are both able to produce both meat and potatoes, but Meier is less skilled in both areas: Meier needs Schmitt to produce 1 kg of potatoes 10 hours 1 kg of potatoes 5 hours 1 kg of meat 20 hours 1 kg of meat 15 hours Assumption: initially no exchange relationships, available working time: 100 hours. Meier decides to use 60 hours for potato production and the rest for meat, Schmitt uses 40 hours for potato production and the rest for meat. Offenburg University of Applied Sciences
Comparative advantages – continued exercise example
In 100 hours, Meier produces, for example In 100 hours, Schmitt produces, for example 
? kg of potatoes and ? kg of meat ? kg of potatoes and ? kg of meat
Intermediate result: absolute cost advantages in both areas for ?
Relative prices for Meier Relative prices for Schmitt 
1 kg of potatoes = ½ kg of meat 1 kg of potatoes = ?
1 kg of meat = ? 1 kg of meat = ?
Intermediate result: Measured in relative prices, meat is cheaper for farmer Meier.
Meier therefore specializes in meat production and Schmitt in potatoes.
In 100 hours, Meier produces, for example In 100 hours, Schmitt produces, for example 
? kg meat 17 kg potatoes (85 hours) and 1 kg meat (15 hours)
Assumption: An exchange ratio of 2.5 kilos of potatoes for one kilo of meat is agreed upon, 
whereby Meier exchanges three kilos of meat and receives 7.5 kilos of potatoes in return.
After the exchange, Meier has e.g. After the exchange, Schmitt has e.g. 
7.5 kg of potatoes and ? kg of meat ? kg of potatoes = ? meat
18 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Pareto criterion and allocative efficiency
19 Prof. Dr. Sybille SchwarzSee Roth (2016), p. 20 -25The Pareto criterion as an efficiency criterion assesses the welfare of a society. Even if an interpersonal comparison of benefits is excluded, a measure always increases the welfare of a group under consideration if it increases the benefit of at least one person without reducing the benefit of another person.

Allocative efficiency ensures that the allocation of scarce resources to specific uses is achieved with as little sacrifice of other beneficial uses of resources as possible.

Pareto efficiency is achieved when there is no longer a measure that at least one individual welcomes and against which no one has any objections.

. Offenburg University of Applied Sciences
Function of prices and competition
20 Prof. Dr. Sybille SchwarzWith decentralized coordination, millions of suppliers and buyers decide 
what, when, how, by whom and for whom something is produced. 
These decisions are coordinated by the price mechanism. The task of 
prices is to balance supply and demand (market balancing function). 
Price changes indicate that the level of scarcity of a good has changed 
(signal function). 
A perceived signal provides incentives for behavioral change (incentive function). 
As a result of the incentives set by rising or falling prices, 
production factors are reallocated (steering or allocation function). 
As a result, market economies have the appropriate structural adjustment flexibility 
and consumer sovereignty.
. Offenburg University of Applied Sciences
Part 1: Questions for the self-test
21 Prof. Dr. Sybille Schwarz1.Give an example of the opportunity costs of attending a football match 
of SC Freiburg. 
2.What does the invisible hand of the market do?
3.Briefly describe the three production factors of economics.
4.What is meant by the following sentence: "There is no such thing as a free lunch"
5.Which of the following statements is true or false:
•Macroeconomics examines individual economic behavior.
•Walter Eucken was concerned with the state guaranteeing a functioning 
competitive order.
•J. M. Keynes wanted to stimulate a lack of supply through government spending.
. Offenburg University of Applied Sciences
Part 1: Answers to the self-test
22 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 2: The theory of households
Key points:
•Households in the goods and labor market
•Budget constraint and budget line
•Preferences
•Indifference curves
•Optimal demand decision
•Changes in the parameters of individual demand
•Factors influencing demand
•Price and income elasticities of demand
Key questions:
What does the consumer want and what can he afford?
What factors determine household demand?
23 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Households on the goods and labor market
24 Prof. Dr. Sybille SchwarzHouseholds are the smallest economic units that are considered in economic analysis. They are demanders, i.e. consumers, on the goods and services markets. At the same time, in their function as employed persons, they are suppliers on the labor market. See Roth (2016), p. 28 -29
Illustration: Offenburg University of Applied Sciences
Budget constraint and budget line
25 Prof. Dr. Sybille SchwarzThe theory of households examines the question of what individuals can afford with the help of the instrument of budget constraint.
The budget line indicates all combinations of goods of x and y that are achievable with a given budget of m and whose consumption does not leave any scarce resources unused. The slope of the budget line shows the objectively possible exchange ratio of the goods determined on the basis of the price relationship.
Example: Farmer Meier:
m = 100 hours
x = amount of potatoes, y = amount of meat
px= 10 hours, py= 20 hours
See Roth (2016), p. 29 -30 Offenburg University of Applied Sciences
Budget line
26 Prof. Dr. Sybille Schwarz Roth (2016), p. 31. Offenburg University of Applied Sciences
The slope of the budget line
27 Prof. Dr. Sybille Schwarz Roth (2016), p. 37. Offenburg University of Applied Sciences
Preferences
28 Prof. Dr. Sybille SchwarzWhat is “best” for an individual depends on their preferences in the sense of their
wishes.

The ordinal utility theory assumes that individuals can rank their
benefit perceptions with regard to different bundles of goods and relies primarily on the following three axioms:

➢Completeness (all bundles of goods can be compared and ranked),

➢Transitivity (freedom from contradiction, if A > B and B > C then A > C),

➢Non-saturation (more is better than less).

See Roth (2016), pp. 38 -44. Offenburg University of Applied Sciences
Indifference curves
29 Prof. Dr. Sybille SchwarzIndifference curves are used to graphically represent preferences. They 
connect all combinations of different quantities of goods x and y that provide the 
individual with the same level of utility. 
There are different forms of indifference curves, depending on whether goods can be exchanged for 
each other (perfect substitutes) or complement each other (perfect complements).

Due to the assumption of non-saturation, indifference curves have a negative slope, 
assuming a decreasing marginal utility. 
The slope at the points on the indifference curve indicates the subjective willingness to exchange 
of the individual, the quantity of a good that he is willing to give up in order to receive an additional 
unit of another good (marginal rate of substitution). 
See Roth (2016), pp. 44 -55. Offenburg University of Applied Sciences
Exercise on indifference curves
30 Prof. Dr. Sybille SchwarzGraphically represent the indifference curve of Coke and Pepsi (assumption: perfect substitutes)!
Graphically represent the indifference curve of hiking boots! Offenburg University of Applied Sciences
Indifference curve
31 Prof. Dr. Sybille Schwarz Roth (2016), p. 44. Offenburg University of Applied Sciences
Properties of indifference curves
32 Prof. Dr. Sybille SchwarzIndifference curves are negatively inclined and convex!
Indifference curves cannot intersect!Combination of goods Food Clothing
A 1 6
B 2 3
C 3 2 Offenburg University of Applied Sciences
Optimal demand decision
33 Prof. Dr. Sybille SchwarzThe optimal bundle of goods that the individual can still afford must lie both on the budget line and on the highest achievable indifference curve. 
At the point of optimal demand decision, the subjective willingness to exchange 
of the individual (= slope of the indifference curve) corresponds to the objectively possible exchange ratio 
(= slope of the budget line). 
The marginal rate of substitution corresponds to the price ratio .
The optimal demand decision in each case therefore depends on the preferences, the 
income (budget) and the prices of goods.
See Roth (2016), pp. 55 -61. Offenburg University of Applied Sciences
The optimal bundle of goods
34 Prof. Dr. Sybille Schwarz Roth (2016), p. 56 Offenburg University of Applied Sciences
Changes in the parameters of individual demand
35 Prof. Dr. Sybille SchwarzEconomists do not usually deal with changes in preferences, but with 
changes in income and prices as exogenous variables.
How does the quantity demanded for a certain good change, 
for example, when income changes? A distinction is made between normal, inferior and 
superior goods. Normally, more of a good is consumed as income increases, but less with inferior goods and more with superior goods (goods of higher need). The line connecting various demand optima 
of the household in question is called the income-consumption curve.
How does the demand for a certain good change, for example, when income remains constant but prices for a good change? The connection between the prices of a good and the quantity demanded of this good is shown by the 
demand curve.
See Roth (2016), pp. 61 -76. Offenburg University of Applied Sciences
Income-consumption curve
36 Prof. Dr. Sybille Schwarz Roth (2016), p. 62. Offenburg University of Applied Sciences
Factors influencing demand
37 Prof. Dr. Sybille SchwarzThe law of demand states that a drop in price increases the quantity demanded. 
The demand function indicates which quantities of a good households demand when 
the price reaches different levels (ceteris paribus). 
There are various factors influencing demand, e.g. 
•price of the good,
•price of comparable goods (substitutive goods),
•price of goods that are associated with the use of the good (complementary goods),
•disposable income, 
•subjective appreciation (preferences).
Theoretically, the market demand curve is obtained from the aggregation of the individual 
demand curves of all buyers belonging to the relevant market. Offenburg University of Applied Sciences
Exercise on the factors influencing demand
38 Prof. Dr. Sybille SchwarzUse a graphic to describe the influence of the different factors on the
quantity demanded:
1.Price of the good
2.Price of comparable goods (substitutive goods)
3.Price of the goods that are associated with the use of the good (complementary goods) Offenburg University of Applied Sciences
The price and income elasticities of demand
39 Prof. Dr. Sybille SchwarzIn practice, it plays a major role how strongly demand reacts to price changes, e.g. whether price changes cause a relatively large change in quantity or not. This depends on various factors, e.g. whether the goods are essential or luxury goods or whether substitute goods are available.
The price elasticity of demand measures the sensitivity (responsiveness) of the quantity demanded for a good (effect) to changes in its price (cause).

With an inelastic demand curve (steeper), a price increase leads to a proportionally

smaller decrease in quantity (price elasticity < 1). Therefore, sales increase.

With an elastic demand curve (flatter), a price increase leads to a proportionally

larger decrease in quantity (price elasticity > 1). Therefore, sales decrease.
The cross-price elasticity of demand measures the responsiveness of the quantity demanded 
of one good to price changes of another good. 
The income elasticity of demand measures the responsiveness of the quantity demanded 
of a good to a change in income. Offenburg University of Applied Sciences
Exercises on the price elasticity of demand
40 Prof. Dr. Sybille SchwarzAssume that the price of a “Tannenzäpfle” increases from €2 to €2.20. The quantity you demand then decreases from ten to eight bottles (per month).
Graphically sketch the cross-price elasticity of demand for substitute goods. Offenburg University of Applied Sciences
Continued: Exercises on price elasticity of demand
41 Prof. Dr. Sybille SchwarzSales and price elasticity of demand: e.g. Price increases from 4 to 5, the quantity demanded decreases from 50 to 20. Offenburg University of Applied Sciences
Part 2: Questions for the self-test
42 Prof. Dr. Sybille Schwarz1.What does the slope of the budget line say?
2.What does the slope at a point on the indifference curve say?
3.The demand function for tickets to a SC Freiburg game is: x = 800 –20p.
The entrance fee is €30. How high is the price elasticity of demand?
4.Describe two factors influencing the demand function and their
interaction with the quantity demanded.
. Offenburg University of Applied Sciences
Part 2: Answers to the self-test
43 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 3: The theory of companies
Key points:
•Production technology
➢Average product, marginal product, production function, returns to scale
•Costs
➢Fixed costs, variable costs, total costs, average costs, marginal costs
•The supply of a company in a polypoly
➢Marginal cost-price rule
•Influencing factors of supply
•The market supply
Key question:
What is the connection between production, factor use and costs?
It is about analyzing the input/output relationships, distinguishing between partial and total factor variation.
44 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Production technology
45 Prof. Dr. Sybille SchwarzCompanies acquire production factors that they use to produce goods or services using a specific production technology. 
This involves looking at the change in output depending on a production factor (ceteris paribus). 
The total output divided by the input factor used (e.g. number of working hours used) is called the average product. 
The marginal product (marginal return) indicates the additional units that are achieved by a marginal increase in a production factor (cf. law of diminishing marginal returns). 
The relationship between output and a varied production factor is represented graphically using the production function. See Roth (2016), pp. 87 -92. Offenburg University of Applied Sciences
Production function
46 Prof. Dr. Sybille Schwarz Roth (2016), p. 92. Offenburg University of Applied Sciences
Returns to scale
47 Prof. Dr. Sybille SchwarzReturns to scale indicate the rate at which output increases when all inputs are increased by the same amount (total factor variation).
With constant returns to scale, changing all inputs leads to a proportional change in output.
With decreasing returns to scale, changing all inputs leads to a less than proportional increase in output.
With increasing returns to scale, changing all inputs leads to a more than proportional increase in output (“economies of scale”).
Production elasticity describes the ratio of the relative change in output to the relative change in an input factor (partial factor variation) and thus the weight with which individual input factors contribute to output. Offenburg University of Applied Sciences
Exercise: Returns to scale and cost trends
48 Prof. Dr. Sybille SchwarzCase 1: Constant returns to scale
Case 2: Decreasing returns to scale
Case 3: Increasing returns to scale Offenburg University of Applied Sciences
Costs
49 Prof. Dr. Sybille SchwarzFixed costs arise in the short term regardless of the production level chosen. Variable
costs change with the production level. 
The sum of fixed costs and variable costs results in the total costs (total costs). 
Dividing the total costs by the production quantity results in the average costs. 
These fall as long as the decrease in the average fixed costs is greater than the 
increase in the average variable costs (U-shaped curve).
The marginal costs are the additional costs incurred to produce another unit. Increasing marginal costs are normally assumed. As long as 
the marginal costs are lower than the average costs, any further expansion of production reduces the average costs. 
See Roth (2016), pp. 93 -96. Offenburg University of Applied Sciences
Marginal costs and average costs
50 Prof. Dr. Sybille Schwarz Roth (2016), p. 99. Offenburg University of Applied Sciences
The supply of a company in a polypoly
51 Prof. Dr. Sybille SchwarzPolypoly (perfect competition) is the market form of competition in which many suppliers compete for the favor of the buyers. The individual entrepreneur cannot influence factor and sales prices (price taker or quantity adjuster). Profit maximization requires the choice of the optimal production technology (most favorable factor input ratio) and the optimal production quantity at which the total profit is maximum. As long as the marginal revenue of an additional unit of goods (corresponds to the market price) is greater than the marginal costs required to produce this additional unit of goods, profit will increase by increasing the production quantity. The supply curve of a polypolistic company therefore corresponds to the marginal cost curve. See Roth (2016), pp. 99 -105. Offenburg University of Applied Sciences
Marginal cost-price rule
52 Prof. Dr. Sybille Schwarz Roth (2016), p. 103. Offenburg University of Applied Sciences
Factors influencing supply
53 Prof. Dr. Sybille SchwarzThe supply function reflects the quantity behavior of producers at alternative prices. The supply curve is positively inclined. The higher the price of the good, the more the companies produce. In addition to the price of the good, the supply of goods depends on other influencing factors, e.g. the price of other goods, the prices of the production factors, the level of technical knowledge. The reaction of the quantity of a good offered to price changes of the good is shown by movements along the supply curve. The reaction of the supply to changes in other variables is shown by a shift in the supply curve. The price elasticity of supply measures the responsiveness of the quantity offered of a good (effect) to changes in its price (cause). The supply of lakeside properties on Lake Constance is presumably inelastic. Offenburg University of Applied Sciences
Exercise on the factors influencing supply
54 Prof. Dr. Sybille SchwarzUse a graphic to describe the influence of the different factors on the
supply quantity:
1. Price of the good
2. Price of other goods
3. Price of the production factors Offenburg University of Applied Sciences
Special cases of price elasticity of supply
55 Prof. Dr. Sybille SchwarzGraphically sketch the following cases:
1. Completely inelastic supply
2. Infinitely elastic supply
3. Unit elastic supply Offenburg University of Applied Sciences
The market offer
56 Prof. Dr. Sybille Schwarz Roth (2016), p.106. Offenburg University of Applied Sciences
Part 3: Questions for the self-test
57 Prof. Dr. Sybille Schwarz1. Explain the term production function and insert the missing values ​​into the table.
2. Why does the supply curve of a polypolistic company correspond to the marginal cost curve?
3. Describe two factors influencing the supply function and their relationship with the quantity supplied.
4. Is the following statement true or false: The long-term supply is more price-elastic than the short-term supply for most products. Give reasons for your answer!
. Variable factor input Total product Marginal product Average product
1 5
2 11
3 18 Offenburg University of Applied Sciences
Part 3: Answers to the self-test
58 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 4: Market equilibrium
Key points:
•Market equilibrium
•Effects of supply and demand shifts
•Markets and price formation
•The welfare effect of markets: pension considerations
59 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
The market equilibrium
60 Prof. Dr. Sybille SchwarzA market is in equilibrium when, at a certain price, the total quantity supplied and the total quantity demanded are the same. The corresponding price is called the equilibrium price, the corresponding quantity is called the equilibrium quantity. A market in equilibrium has neither excess demand nor excess supply; it is "cleared". The price provides the signals required for an efficient allocation of scarce resources and reflects the respective opportunity costs of a decision. Free prices control who uses resources, which goods are provided and which consumers purchase them. The scarce resources are used where they are most urgently needed - measured in terms of willingness to pay, i.e. the willingness to forego other goods. The price system ensures efficient use of scarce resources. See Roth (2016), pp. 106 -124. Offenburg University of Applied Sciences
The market equilibrium
61 Prof. Dr. Sybille Schwarz Roth (2016), p. 107. Offenburg University of Applied Sciences
Market clearing in equilibrium
62 Prof. Dr. Sybille Schwarz Roth (2016), p. 108. Offenburg University of Applied Sciences
Effects of supply and demand shifts
63 Prof. Dr. Sybille SchwarzIf demand remains unchanged, an increase in supply (shift of the supply curve to the right) causes a price drop and a decrease in supply (shift of the supply curve to the left) causes a price increase.
If supply remains unchanged, an increase in demand (shift of the demand curve to the right) causes a price increase and a decrease in demand (shift of the demand curve to the left) causes a price drop.
The comparative-static analysis is a tool for assessing the effects:
1.Does an event lead to a shift in the supply and/or demand curve?
2.Right or left shift?
3.Comparison of the original equilibrium with the new equilibrium in terms of equilibrium price and quantity.
Cf. Mankiw/ Taylo (2018), pp. 80 -87. Offenburg University of Applied Sciences
Exercise: Effects of supply and demand shifts
64 Prof. Dr. Sybille SchwarzCase 1: How does a hot summer change the demand for milk?
Case 2: A drought drives up the price of animal feed. How does this affect the market for milk?
Case 3: Combination of 1 and 2 (heat wave and increase in the price of animal feed) Offenburg University of Applied Sciences
Markets and price formation
65 Prof. Dr. Sybille SchwarzThe market acts as a “clearing house” where buyers indicate the quantity they are willing to buy at certain prices and suppliers indicate the quantity they are willing to provide at certain prices. Although all market participants draw up their plans independently, the market enables the optimal coordination of the plans of suppliers and buyers through the price formation process. Similar to an auction, the buyer with the highest willingness to pay is sought and found (see exercise Gengenwin AG). Offenburg University of Applied Sciences
Example Gengenwin AG
66 Prof. Dr. Sybille SchwarzPrice Buy orders Sell orders
Best 26
120 15 2
121 5 6
122 3 16
123 16 4
124 6 7
125 3 10
126 4
Cheapest 25
Based on Bofinger (2015) Offenburg University of Applied Sciences
Exercise Gengenwin AG
67 Prof. Dr. Sybille SchwarzCourses Demanded
QuantitySupplied
QuantitySales
Under 120
120
121
122
123
124
125
126
Over 126 Offenburg University of Applied Sciences
The welfare effect of markets: rent considerations
68 Prof. Dr. Sybille Schwarz A welfare economic rent is understood to be the utility gain that a market participant derives from market transactions. 
The consumer rent captures the utility gain of a consumer as the difference 
between the individual willingness to pay and the price actually to be paid 
Graphically, it represents the area above the price and below the demand curve. 
The producer rent measures the difference between the revenue and the costs of 
production. Graphically, it represents the area below the market price and above the 
supply curve. 
The sum of consumer rent and producer rent is called total rent or 
social surplus (maximum in equilibrium → Pareto-efficient result). 
See Roth (2016), pp. 130 -139. Offenburg University of Applied Sciences
Example: Jakob's consumer surplus
69 Prof. Dr. Sybille Schwarz Roth (2016), p. 131. Jakob's willingness to payCosts
(=price)Benefit surplus
1st bar €1.50 €0.50 €1.00
2nd bar €1.25 €0.50 €0.75
3rd bar €1.00 €0.50 €0.50
4th bar €0.75 €0.50 €0.25
5th bar €0.50 €0.50 €0.00
Total €5.00 €2.50 €2.50 Offenburg University of Applied Sciences
Jakobs consumer surplus
70 Prof. Dr. Sybille Schwarz Roth (2016), p. 132. Offenburg University of Applied Sciences
Welfare loss of a supply surplus
71 Prof. Dr. Sybille Schwarz Roth (2016), p. 135. Offenburg University of Applied Sciences
Welfare loss from excess demand
72 Prof. Dr. Sybille Schwarz Roth (2016), p. 136. Offenburg University of Applied Sciences
Part 4: Questions for self-testing
73 Prof. Dr. Sybille Schwarz1. What happens if there is a higher price in relation to the equilibrium price?
2.How does a hot summer affect the quantity demanded and the price of ice cream?
3.Explain the following statement with reference to the supply-demand diagram:
"When a cold spell hits Spain, the price of orange juice rises."
4.Look at the market for eggs and clarify the effects on supply and demand as well as on supply and demand quantities for the events given.
➢The price of chicken feed rises.
➢A study shows that consuming eggs can be harmful to health.
5.What is meant by consumer surplus?
. Offenburg University of Applied Sciences
Part 4: Answers to the self-test
74 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 5: The market failure theory and special pricing situations in practice
Key points:
•Market failure and prisoner's dilemma
•Market failure situations
➢Public goods, external effects, structural problems of competition (e.g. 
natural monopoly), asymmetric information 
•State influence on pricing
➢Direct quantity regulations, maximum and minimum prices, subsidies or 
taxation
•Pricing in monopolistic competition and oligopolies
•Competition policy
•Distribution policy
•Regulatory policy
75 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Market failure
76 Prof. Dr. Sybille SchwarzFrom an economic perspective, government intervention in the competitive allocation of resources can be justified under certain circumstances. The market failure theory provides possible reasons. Government intervention promotes welfare when it can be plausibly expected that it will lead to a Pareto-superior state in relation to the market result. Market failure occurs in situations in which the allocation by the markets does not work. Individually rational behavior then leads to collectively irrational results, i.e. the criterion of Pareto efficiency (no waste of scarce resources and no individual can be better off without making another individual worse off) is not guaranteed. Examples of market failure are: existence of public goods, existence of external effects, structural problems of competition, lack of transparency for consumers. See Roth (2016), pp. 149 -152. Offenburg University of Applied Sciences
The prisoner's dilemma
77 Prof. Dr. Sybille SchwarzThe story of the prisoner's dilemma is not an economic one, but has been established in 
game theory to depict the divergence of individual and collective 
rationality. These are interdependent decision-making situations in which the consequences of decisions made by one side depend on decisions made by the other side. 
For example, two people are suspected of a crime. The following options for action 
exist in separate interrogations: one person confesses to the crime they committed together, the other denies it, both confess or both deny it. It can be shown that individual confession leads to the worst collective situation. 
See Roth (2016), pp. 152 -155. Offenburg University of Applied Sciences
The prisoner's dilemma
78 Prof. Dr. Sybille SchwarzMatilda confesses Matilda denies
Frieda confesses 5 years in prison
5 years in prison6 years in prison
Acquittal
Frieda denies Acquittal
6 years in prison1 year in prison
1 year in prison Offenburg University of Applied Sciences
Market failure situation: Public goods
79 Prof. Dr. Sybille SchwarzPrivate goods are goods that meet the following criteria:
➢Excludability from consumption,
➢Rivalry in consumption.

Public goods do not meet these two criteria. They can be consumed without having to pay for them, as they have no price.

So-called "common goods" have rivalrous uses, but are not excludable, whereas club goods are excludable but not rivalrous.
Efficient provision of public goods could be achieved if everyone contributed to financing according to their individual willingness to pay. Market allocation threatens to fail if all individuals do not act selflessly and honestly reveal their respective willingness to pay (freerider behavior or free rider behavior). See Roth (2016), pp. 155 -162. Offenburg University of Applied Sciences
Market failure situation: External effects
80 Prof. Dr. Sybille SchwarzExternal effects (externalities) arise from the impact of economic action on the welfare of an uninvolved third party, for which no one pays or receives compensation. If the effect is damaging, it is referred to as a negative external effect (e.g. exhaust fumes from cars); if it is beneficial, it is referred to as a positive external effect (e.g. restored historic buildings). The reason for this is often inadequately defined or insufficiently enforceable property rights. In the case of negative external effects, the social marginal benefit is lower than private demand suggests. The economically efficient result is missed as long as the effects of private actions on other members of society are not taken into account (internalization). See Roth (2016), pp. 163 -168 and Mankiw/Taylor (2018), pp. 322 -326. Offenburg University of Applied Sciences
Discussion: External effects
81 Prof. Dr. Sybille SchwarzTo what extent can inventions and vaccinations generate positive external effects? Offenburg University of Applied Sciences
Theoretical solutions for external effects
82 Prof. Dr. Sybille SchwarzThe private negotiation solution according to the Coase theorem: Private negotiations can lead to an efficient result. In practice, private negotiations do not always work, e.g. due to transaction costs (costs in the course of negotiating and implementing an agreement), negotiation difficulties, coordination problems, asymmetric information and the assumption of rational behavior. If individual negotiations are not possible, the state can solve the externality problem. The Pigou tax is a market-based political measure to correct negative external effects. In theory, the tax increases the costs to be borne privately to the extent necessary to compensate for the social marginal costs. The solution through certificates: The legislator defines a target quantity. A corresponding quantity of certificates is issued, the possession of which entitles the holder to damage the environment. The prices for the certificates are determined on the market.
See Roth (2016), pp. 168 -179. Offenburg University of Applied Sciences
Case study on the Coase theorem
83 Prof. Dr. Sybille Schwarz Nora has a dog called Brandy that occasionally barks and disturbs the neighbor Lukas (negative externality). Should Nora be forced to give Brandy away, or should Lukas have to endure sleepless nights because of the barking? Which outcome would be more economically efficient? If Nora's benefit from the dog exceeds the external costs for Lukas, it is more efficient overall if Nora keeps the dog and Lukas learns to live with the barking. If Lukas' costs exceed the benefit for Nora, Brandy should "theoretically" be given away. The problem, however, is to evaluate costs and benefits accordingly. Otherwise (assuming that market participants act rationally), Lucas could offer Nora money to give up the dog. Nora would give up the dog if the amount of money offered exceeded the discounted future benefit of keeping the dog. Would the outcome be different if Lukas had a legally protected right to peace and quiet? According to Coase, the original distribution of rights plays no role in the market's ability to reach an efficient result. However, the legal situation ultimately decides who pays whom. 
See Mankiw/Taylor (2018), pp. 334 -337. Offenburg University of Applied Sciences
Case study on the Pigou tax
84 Prof. Dr. Sybille SchwarzAssume that two factories, a paper factory and a steel factory, each discharge 500 tons of wastewater into a river each year. There are two alternatives to reduce pollution:
1. Regulation: Companies are required by the state to reduce their wastewater discharge into the river to 300 tons per year.
2. Pigou tax: Companies are charged a tax of 50,000 euros per ton of wastewater.

Regulation would prescribe a maximum level of pollution, and the Pigou tax would provide an incentive to reduce pollution to the point where the marginal cost of pollution prevention (the cost of the last unit of pollution not emitted, i.e. avoided) is equal to the tax rate.

The regulatory solution is not necessarily the cheapest and most effective for improving water quality. It is possible, for example, that the paper factory can reduce the amount of wastewater more easily and cheaply than the steel factory. It could significantly reduce the amount of wastewater discharged in order to avoid paying taxes, while the steel factory reduced pollution to a lesser extent and paid the taxes. However, the design of the Pigou tax depends on the respective framework conditions of the tax system; in practice it is often difficult to determine the appropriate tax rate. See Mankiw/Taylor (2018), pp. 338 -339. Offenburg University of Applied Sciences
Case study on the certificate solution
85 Prof. Dr. Sybille SchwarzSuppose the paper factory and the steel factory are each required to reduce their wastewater discharge to 300 tons per year. Both have fulfilled the requirement, but then come up with the idea that the steel factory increases the discharge by 100 tons, while the paper factory reduces the discharge by exactly that amount and receives 5 million from the steel factory in return. From the point of view of economic efficiency, this would be a good solution. By allowing the paper factory to sell its right to pollute to the steel factory, welfare is increased. Those companies that can only reduce their emissions at high costs will pay the most for the certificates. Companies that can reduce emissions at low costs will sell their certificates. Both Pigouvian taxes and environmental permits internalize the external effects of pollution by making it costly for companies. By levying a Pigouvian tax, the government sets the price of pollution and the demand curve determines the amount of pollution. By issuing a limited number of government environmental permits, on the other hand, the amount of pollution is set and the demand curve determines the price of pollution. In both cases, the price and amount of pollution are the same. See Mankiw/Taylor (2018), pp. 340 -342. Offenburg University of Applied Sciences
Exercise on the Pigou tax and environmental certificates
86 Prof. Dr. Sybille Schwarz1.Can you illustrate the market-based measure “Pigou tax” using a diagram?
2.Can you illustrate the market-based measure “environmental certificate” using a diagram? Offenburg University of Applied Sciences
Discussion: Emission certificates (CO2 certificates)
87 Prof. Dr. Sybille SchwarzSince 2005, companies within the EU have been trading emission certificates. The 
total volume of greenhouse gases that may be emitted is set by an upper limit 
(cap) within which companies can receive, purchase and trade certificates. 
"Unpopular success model" (FAZ, January 4, 2021) 
➢Expansion of the trading system to include transport and buildings? Offenburg University of Applied Sciences
Market failure situation: Natural monopoly
88 Prof. Dr. Sybille SchwarzIf there is insufficient competition, inefficiencies are to be expected (welfare losses due to 
lack of innovation and quantity restrictions). In contrast to the price takers 
(quantity adjusters) in the polypoly, the monopolist faces the entire market demand alone 
(price setters). The price-sales function of the monopolist is identical to the market demand curve. 
Temporary or state-protected monopolies do not constitute market failure. 
The situation is different with natural monopolies. Here, even in free markets 
there is no competition due to other suppliers entering the market. Natural monopolies arise 
because a single company can produce a certain good for the entire market at lower costs than 
two or more companies. When producing a good, there are "subadditive" 
cost structures, e.g. B. for network-dependent goods such as water, electricity, rail, 
telephone. Connecting additional users to a network only causes low costs. One 
supplier is therefore able to meet the entire demand at lower costs than several 
suppliers. 
See Roth (2016), pp. 180 -190. Offenburg University of Applied Sciences
Monopoly: Entrepreneurs as price setters
89 Prof. Dr. Sybille SchwarzRoth (2016), p. 184. Offenburg University of Applied Sciences
Monopoly: Welfare loss
90 Prof. Dr. Sybille SchwarzRoth (2016), p. 185. Offenburg University of Applied Sciences
Market failure situation: Asymmetric information
91 Prof. Dr. Sybille SchwarzWith an asymmetric distribution of information, one side of the market is better informed than the other. This information advantage can cause external effects, e.g. through:
➢Adverse selection: A market can result in poor quality or risk selection if one side of the market has crucial information before the contract is concluded that is hidden from the other (e.g. used car market).
➢Moral hazard: After the contract is concluded, one party to the contract can change their behavior in a way that harms the other (e.g. insurance).
There are options such as signaling (e.g. guarantee promises) or screening (e.g. insurance with deductibles) to counteract market failure. 
See Roth (2016), pp. 190 -199. Offenburg University of Applied Sciences
State influence on price formation
92 Prof. Dr. Sybille SchwarzThe state has various means of intervening in free price formation, e.g.
•Direct quantity regulations, such as the legal setting of quotas, these 
restrict supply. A higher price is formed than with free price formation.
•Setting maximum and minimum prices: These lead to a demand 
excess at maximum prices or a supply surplus at minimum prices. 
•Subsidies or taxation: This influences the financial framework for individual economic production and consumption decisions. Offenburg University of Applied Sciences
Exercise: State influence on price formation
93 Prof. Dr. Sybille SchwarzGraphically show the effects of the following interventions:
1.Setting a quota
2.Setting a rent control Offenburg University of Applied Sciences
Pricing in monopolistic competition
94 Prof. Dr. Sybille SchwarzIn monopolistic competition (heterogeneous polypoly), many suppliers offer heterogeneous, 
but similar products. There is scope for price setting within certain limits. Instead of pure quantity adjustment, a strategy of 
price fixing is used. For example, the strategy of product differentiation is a relatively simple 
way of creating limited scope for pricing. 
Price differentiation is based on the idea of ​​charging each buyer the maximum price that they would be willing to pay (maximum possible consumer surplus). In practice, different buyer groups are often separated from one another or markets are segmented.
Discuss the following statement: With a sophisticated marketing strategy, a monopolistic position can be achieved relatively easily! Offenburg University of Applied Sciences
Exercise: Price formation in monopolistic competition
95 Prof. Dr. Sybille SchwarzExplain and show graphically how you can gain a “monopoly-like” position with a brand name! Offenburg University of Applied Sciences
Price formation in oligopoly
96 Prof. Dr. Sybille SchwarzOligopoly on imperfect markets is characterized by the fact that there are many

customers but only a few suppliers. In oligopoly, price formation depends not only on

the reaction of the customers, but also on the reaction of other oligopolists (interdependent decision situations). In practice, various

behaviors can be observed, e.g.

➢quality competition,

➢market leadership,

➢cutting-edge competition,

➢coordinated behavior.

In practice, cooperation with cooperative partners is also common and

non-cooperative behavior is punished. Offenburg University of Applied Sciences
Exercise: Price formation in different market forms
97 Prof. Dr. Sybille SchwarzCompare the oligopoly and the heterogeneous polypoly in terms of price formation!
Briefly describe two possible behavioral strategies of an oligopolistic provider. Offenburg University of Applied Sciences
Competition policy
Competition is not an end in itself, but is intended to ensure freedom of entrepreneurial activity, freedom of consumer choice and freedom of choice of job. It is also intended to ensure the optimal allocation of resources. The protection of competition is anchored in the ➢Law against Restraints of Competition (e.g. principle of prohibition of cartels, ➢abuse control of market-dominating companies, merger control) and other laws such as the ➢Law against Unfair Competition, Trademark Act, Patent Act.
The model of functional competition is, for example, the benchmark for the decisions of the Federal Cartel Office. The model of freedom of competition is represented among economists primarily by Friedrich August von Hayek (1899-1992).
98 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Distribution policy
Even if free competitive markets enable efficient use of scarce resources
and distribution based on performance (exchange justice), there are often other concepts of justice (e.g. distributive justice, equal opportunities) that are used to justify distribution policy interventions. 
Requirements for efficient redistribution:
➢Redistribution activities should relate to individuals, be based on material need and be carried out efficiently. 
➢Redistribution activities through direct market interventions (“transfer in kind”) are inefficient from an economic point of view. Distribution policy should start with the initial endowment via the tax-transfer system (“transfer in cash”) and then leave price formation to the free play of supply and demand. 
➢The reduction of performance incentives as the upper limit of desirable redistribution policy should be taken into account.
See Roth (2016), pp. 201 -216. 99 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Regulatory policy versus process policy
Should political decisions be limited to basic framework conditions or should they contain specific requirements?
Regulatory policy aims to shape the economic order in a way that promotes the common good and thus the rules of the game under which participants in economic life operate.
Process policy, on the other hand, intervenes directly in individual economic processes and thus attempts to control economic activity promptly and specifically. In practice, it is often difficult to identify these situations in a timely and clear manner (information problem). In addition, there are often uncontrollable feedback effects or the risk of influence from lobbyists. 
See Roth (2016), pp. 221 -224.
100 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Part 5: Questions for the self-test
101 Prof. Dr. Sybille Schwarz1.Describe two market failure situations.
2.Describe a possible solution for externalities.
3.Graphically depict the imposition of a Pigou tax or the issuing of environmental certificates.
4.Describe an essential characteristic of the monopolist in comparison to the
polypolist.
5.Graphically depict how the following interventions affect price formation:
•direct quantity regulations,
•setting of maximum prices.
6.Describe the idea of ​​price differentiation.
. Offenburg University of Applied Sciences
Part 5: Answers to the self-test
102 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 6: Macroeconomic data
Key points:
•National accounts
➢Circular diagram, gross domestic product (nominal, real), GDP deflator, 
Economic indicators, GDP as a measure of prosperity
•Measuring the cost of living
➢Consumer price index, inflation rate, inflation adjustment
103 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
National Accounts
104 Prof. Dr. Sybille SchwarzWhile microeconomics analyses the decision-making behavior of consumers and producers and their interaction on the markets, macroeconomics examines macroeconomic phenomena. 
Circular analysis presents the flow of money and goods in a compressed form. To do this, the economic entities are grouped together into groups (sectors) and their relationships (transactions) are presented. Four sectors are distinguished: companies, private households, the state, and the rest of the world.
National accounts in the narrower sense record the transactions between the sectors in the form of an income/expenditure account (“national accounting”). 
In addition, the assets of an economy, credit relationships and transactions with foreign countries (balance of payments) are also recorded. Offenburg University of Applied Sciences
The circular flow diagram
105 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 623. Offenburg University of Applied Sciences
Excursus: Balance of payments
The balance of payments records the performance and financial transactions that have taken place between domestic and foreign countries over a period of time. It is structured according to the principle of double-entry accounting and records all transactions in two sub-balances:
1. Current account: essentially records foreign trade, services, earned and property income, current transfers.
2. Capital account: essentially records direct investments, securities investments, loans, foreign exchange, derivatives, other capital investments.
Example: A Porsche is exported to the USA. In the current account, this is recorded as an export, in the capital account, the short-term loans item increases if one assumes that the delivery to a US importer is made on credit.
106 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Discussion
“The Yin and the Yang of the Current Account” (FAZ, April 3, 2017)
➢The Yin: Germany had a current account surplus of €261 billion in 2016
➢The Yang: Germany consumed €261 billion less or invested domestically in 2016 than would have been possible. More capital left the country than flowed into it.

107 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Measurement concepts for economic growth
The domestic product (domestic concept) is measured as the result of production within the
geographical borders of a country. The gross domestic product is the 
value expression for the quantity of all goods and services produced in an 
economy within a certain period (only the value of the 
final products). 
The national income (domestic concept, formerly national product) measures the value 
of all goods and services generated in a year by domestic companies, 
households and the state. 
108 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Gross domestic product (GDP)
109 Prof. Dr. Sybille SchwarzThe GDP is the market value of all goods and services intended for final consumption that are produced in a country in a certain period of time.
The GDP (= Y) can be broken down into four components: consumption (C), investments (I),
government spending (G), net exports (NX) → Y = C + I + G + NX
The production of goods and services valued at current prices is called
nominal GDP. In order to obtain a measure that is not influenced by price changes,
real GDP is used, which values ​​the production of goods and services at
constant prices.
The GDP deflator measures the prices of goods and services, calculated as the ratio of nominal and real GDP times 100. 
See Mankiw/Taylor (2018), pp. 624 -641. Offenburg University of Applied Sciences
Exercise: Nominal, real GDP, GDP deflator
110 Prof. Dr. Sybille SchwarzIn 2019, an economy produced 100 keychains that are sold for 2 euros each. In 2020, this economy produces 200 keychains that are sold for 3 euros each.
Calculate the nominal GDP for the years 2019 and 2020.
Calculate the real GDP and the GDP deflator for the year 2020. By what percentage do the nominal GDP and the price level increase from one year to the next? Offenburg University of Applied Sciences
GDP components Germany 2017
111 Prof. Dr. Sybille SchwarzSource: Federal Statistical Office (ed.), as of February 2018, cited in: Mankiw /Taylor (2018), p. 631.GDP component Total (billion €) Per capita (€) Share (%)
Gross domestic product 3,263.35 39,469 100.0
Consumption 1,734.99 20,979 53.2
Investments 641.37 7,755 19.6
Government spending 638.66 7,723 19.6
Net exports 248.33 3,003 7.6 Offenburg University of Applied Sciences
Gross domestic product
112 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Economic indicators
113 Prof. Dr. Sybille SchwarzProductivity indicators are a measure of the performance of the production factors labor and capital, e.g. labor productivity is measured as the quotient of GDP and the number of employed persons or capital productivity as the quotient of GDP and gross fixed assets (capital stock).
Unit labor costs are a measure of the cost competitiveness of an economy. They relate labor costs per employee to the labor productivity provided and thus calculate the proportion of labor costs that fall on a product unit. When comparing countries, differences in labor costs are often offset by differences in productivity levels.
Other interesting indicators are the share of government consumption (government consumption/GDP), the government share (government expenditure/GDP) and the tax rate.
(see, for example, the annual report of the German Council of Economic Experts on the assessment of overall economic development). Offenburg University of Applied Sciences
Gross domestic product as a measure of prosperity
114 Prof. Dr. Sybille SchwarzThe real GDP per capita is considered the best available single measure of the economic prosperity of the population. But there are also things that contribute to the quality of life that are left out, e.g. the value of leisure time, housework, volunteer work, quality of the environment. 
See Mankiw /Taylor (2018), pp. 637 -638.

"The standard of living is a measure of life and not the sum of goods. 
It includes the freedom of choice as well as the care for others."
(Amartya Sen, received the Nobel Prize for Economics in 1998)
Discussion: Critically assess the domestic product as an indicator of prosperity! Offenburg University of Applied Sciences
Consumer Price Index
115 Prof. Dr. Sybille SchwarzThe consumer price index is used to measure the cost of living. This is a measure of the development of the price level of an economy. It measures the price changes of goods and services purchased by an "average consumer" and is determined monthly by the Federal Statistical Office in order to determine the change in the cost of living over time. First, the basket of goods is determined by means of a consumer survey, then the prices for each good in the basket are determined and the price of the basket is calculated. A base year is determined which serves as a comparison standard. The price of the basket of goods in each year is divided by the price of the basket of goods in the base year and multiplied by 100. Finally, the consumer price index is used to calculate the inflation rate as a percentage change in the price index compared to the previous period. See Mankiw / Taylor (2018), pp. 650 -658. Offenburg University of Applied Sciences
Weighting in the consumer price index
116 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Exercise: Consumer Price Index
117 Prof. Dr. Sybille SchwarzA shopping basket consists of four hot dogs and two hamburgers. In the base year 2016, the price of a hot dog was 1 euro and that of a hamburger was 2 euros. The prices have risen by one euro each in the next two years. Calculate the consumer price indices and the inflation rates for 2017 and 2018! Offenburg University of Applied Sciences
Discussion: Consumer Price Index
118 Prof. Dr. Sybille SchwarzDiscussion: What problems could distort the consumer price index? Offenburg University of Applied Sciences
Inflation adjustment

119 Prof. Dr. Sybille SchwarzPrice indices are used to eliminate the effects of inflation when comparing

money amounts at different points in time. The contractually or legally stipulated automatic inflation adjustment of monetary values ​​is called

indexation.

Adjusting for the effects of inflation is particularly important with regard to interest rates. The interest rate that the bank pays is called the nominal interest rate, and the interest rate adjusted for the

inflation rate is called the real interest rate. The real interest rate

is therefore the difference between the nominal interest rate and the inflation rate.

See Mankiw / Taylor (2018), pp. 659 -664. Offenburg University of Applied Sciences
Exercise on adjusting for inflation
120 Prof. Dr. Sybille SchwarzThe price of a chocolate bar rose from €0.10 to €0.60 over a longer period of time. The consumer price index rose from 150 to 300 in the same period. By how many
cents did the price rise after adjusting for inflation? Offenburg University of Applied Sciences
Part 6: Questions for the self-test
121 Prof. Dr. Sybille Schwarz1.Why is a high GDP desirable for a country? Give an example of something that increases GDP but is not desirable.
2.Which sectors are distinguished in the circular flow analysis?
3.How do nominal and real GDP differ?
4.How does the nominal interest rate differ from the real interest rate?
5.Which components of GDP are affected by the following transactions?
➢A family buys a new washing machine.
➢You buy a bottle of French red wine.
6.Using the following situation, explain a problem in determining the consumer price index: Purchases of tablets are increasing as a result of the price drop. Offenburg University of Applied Sciences
Part 6: Answers to the self-test
122 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 7: Long-term real economic development
Key points:
•Production and growth
➢Determinants of productivity, the importance of savings and 
investment
•Unemployment
➢Types, recording 
123 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Production and growth
124 Prof. Dr. Sybille SchwarzProductivity is an important determinant of the standard of living, 
measured by real GDP per capita. It refers to the amount of 
goods and services that a worker can produce in a certain period of time. 
Determinants of productivity are
•real capital per worker (stock of means of production produced),
•human capital (knowledge and skills of the workforce),
•natural resources (inputs provided by nature),
•technological knowledge (knowledge of the best production methods).

There is a positive correlation between investment and growth.
An increase in the stock of real capital can also be achieved through investment from abroad, e.g. through foreign direct investment and foreign portfolio investment. 
See Mankiw / Taylor (2018), pp. 673 -676. Offenburg University of Applied Sciences
Growth and Investments/1
125 Prof. Dr. Sybille SchwarzMankiw /Taylor (2016), p. 685.. Offenburg University of Applied Sciences
Growth and Investments/2
126 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2016), p. 685.. Offenburg University of Applied Sciences
Discussion: Growth rates and compounding
127 Prof. Dr. Sybille SchwarzSuppose one country has an average growth rate of 1 percent per year, while another country grows by 3 percent. Is the difference really that significant?
Example: Suppose two university graduates – Lena and Max – take their first job at the age of 25 and both earn 30,000 euros per year. Max lives in an economy in which all incomes grow by 1 percent per year, whereas Lena's grows by 3 percent. Max therefore earns 30,000 euros in the first year, 30,300 euros in the second year, 30,603 euros in the third year, etc. Lena already earns 30,900 euros in the second year, 31,827 euros in the third year, etc. Forty years later, when both are 65, Max earns 45,000 euros per year, while Lena earns 98,000 euros. A rule of thumb is helpful for understanding compounding effects. If a quantity grows at a rate of x percent per year, this quantity doubles in approximately 70/x years (for Max it takes 70 years, for Lena only around 23 years). See Mankiw/Taylor (2018), p. 672. Offenburg University of Applied Sciences
The importance of savings and investment
128 Prof. Dr. Sybille SchwarzIf an economy produces a lot of new capital goods today, it will have a larger capital stock tomorrow and be able to produce more goods and services. Conversely, this means that fewer resources can be used for current consumption and the savings rate must increase. As savings increase, fewer resources are needed to produce consumer goods or more resources are available for producing capital goods. Due to diminishing marginal returns, growth slows down. In the long term, a higher savings rate leads to a higher level of productivity and income, but not to faster growth. From a given starting point, poor countries tend to achieve faster growth than rich countries (catch-up effect). See Mankiw / Taylor (2018), pp. 687 -689. Offenburg University of Applied Sciences
Diminishing marginal returns and the catch-up effect
129 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2016), p. 686.. Offenburg University of Applied Sciences
Discussion
"Germany is suffering as a business location" (FAZ, January 12, 2021)
➢According to a ZEW study, Germany is among the laggards among the 21 industrial nations examined.
➢Weaknesses are seen primarily in the areas of tax policy, the quality of infrastructure, labor costs, productivity and human capital. The financial stability of the state and private sector is highlighted positively.
➢In recent years, Germany has focused heavily on the distribution of wealth, and now Germany must become more competitive again.
➢In the World Economic Forum's Global Competitiveness Report, Germany fell four places in 2019, but at 7th place it was still in the top 10 of the 141 countries examined. “The welfare state is reaching its limits” (FAZ, 30.12.2020)
130 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Unemployment
131 Prof. Dr. Sybille SchwarzThe problem of unemployment is viewed as a long-term and short-term phenomenon (e.g. 
seasonal unemployment).
The natural unemployment rate describes the normal level of long-term 
unemployment in an economy, which does not disappear and around which the 
unemployment rates fluctuate cyclically (cyclical unemployment). 
Approaches to explaining the natural unemployment rate include, for example,
•the search for a job (frictional unemployment),
•structural reasons that lead to a gap between labor supply and labor demand.
See Mankiw /Taylor (2018), pp. 709 -722. Offenburg University of Applied Sciences
Recording unemployment
132 Prof. Dr. Sybille SchwarzThe number of unemployed (jobless) is the number of people who are able to work and are available to the labor market at the prevailing wage rate, but who do not have a job. The unemployment rate expresses the number of unemployed in relation to the number of employed persons (labor force potential), which corresponds to the sum of employed and unemployed persons.

Unemployment is measured either as the number of people registered as unemployed (Federal Employment Agency) or with the help of unemployment statistics (Federal Statistical Office), which are based on the recording system of the International Labor Organization (ILO).

Unemployment that is not recorded in statistics is referred to as hidden unemployment.

See Mankiw/Taylor (2018), pp. 704 -707. Offenburg University of Applied Sciences
Part 7: Questions for the self-test
133 Prof. Dr. Sybille Schwarz1.To what extent does a university degree represent a form of capital?
2.How can differences in development between countries be explained?
3.Explain how higher savings lead to a higher standard of living.
4.In the 1980s, Japanese investors made significant direct and portfolio investments in the USA. In what ways was this beneficial for the USA? In what ways would it have been better if the Americans had made these investments themselves?
5.We assume that there are 40 million employed people, 2 million unemployed and 40 million non-employed people in Germany. What is the number of employed people, the employment rate and the unemployment rate?
. Offenburg University of Applied Sciences
Part 7: Answers to the self-test
134 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 8: Interest rates, money and prices
Key points:
•Saving, investing and the financial system
➢Financial markets, financial intermediaries, overall economic savings, analysis of the
credit market
•The monetary system
➢Importance of money, role of central banks, monetary policy instruments, the
central bank, banks and the money supply
•Money supply growth and inflation
➢Money supply, money demand, monetary equilibrium, quantity theory of

money, velocity of money
135 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
The financial system 
136 Prof. Dr. Sybille SchwarzThe financial system consists of institutions that help match one person's savings with another person's investment wishes. 
Financial markets are those institutions through which a person who wants to save can pass on funds directly to a person who wants to borrow money. The most important financial markets are the bond and stock markets. The basic functions of financial markets are capital allocation, risk transfer and information generation. 
Financial intermediaries are institutions through which savers indirectly provide funds to debtors (banks and investment companies). An investment company is an institution that issues shares to the public and uses the proceeds from them to buy a portfolio of shares and bonds (mutual funds). 
See Mankiw/Taylor (2018), pp. 735 -743. Offenburg University of Applied Sciences
Saving, investing and total income
137 Prof. Dr. Sybille Schwarz The overall economic savings (S) are determined by the total income of an economy that remains after deducting expenditure on consumption and government consumption. 
It is made up of private savings (income that remains for households after deducting taxes and consumption expenditure) and public savings (amount of tax revenue that remains for the state after paying its expenditure). 
In a closed economy, Y = C + I + G. If Y -C -G is replaced by S , S = I follows. T denotes taxes. Accordingly, the overall economic savings are: S = (Y -T -C) + (T -G). If (T -G) > 0, this is referred to as a budget surplus, if (T -G) < 0, this is referred to as a budget deficit. See Mankiw / Taylor (2018), pp. 746 -749. Offenburg University of Applied Sciences
Analysis of the credit market
138 Prof. Dr. Sybille SchwarzAssumptions are made that savings and investments are coordinated on the credit market, which is determined by supply and demand. Those who want to save offer funds and those who want to invest demand funds. The interest rate is the price of a loan. The amount of credit funds demanded falls as the interest rate rises. The analysis of the credit market can be used to examine different government measures that influence the savings and investments of an economy. The impact analysis in the sense of a comparative-static analysis is carried out in three steps: 1. Does the fiscal policy measure shift the supply or demand curve? 2. In which direction does the respective curve shift? 3. How does the equilibrium on the credit market change? See Mankiw / Taylor (2018), pp. 753 -757. Offenburg University of Applied Sciences
The credit market
139 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 751. Offenburg University of Applied Sciences
Taxes and savings
140 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 754. Offenburg University of Applied Sciences
Taxes and Investments
141 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 755. Offenburg University of Applied Sciences
State budget deficits
142 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 757 Offenburg University of Applied Sciences
The monetary system 
143 Prof. Dr. Sybille SchwarzWithout money, people would have to rely on barter. Money has three 
functions: It is 
•Means of exchange: Money enables transactions between buyers and sellers , 
•Unit of account: Money is a measure for setting prices and specifying debts and makes 
values ​​comparable and 
•Store of value: With the help of money, purchasing power can be 
shifted from today into the future. 
The amount of money circulating in an economy includes cash and demand deposits 
(book money) in the checking account, i.e. deposits that customers can withdraw immediately. Furthermore, 
a distinction is made between the three measures M1, M2 and M3 in order to define the money supply. 
See Mankiw /Taylor (2018), pp. 803 -808. Offenburg University of Applied Sciences
The monetary system
144 Prof. Dr. Sybille SchwarzThree measures for the money supply in the European Monetary Union
See Mankiw /Taylor (2018), p. 808..Description Amount Dec. 2017 (billion €) Components
M1 7,748 Daily deposits and
cash in circulation
M2 11,202 M1
+ Deposits with a notice period
of up to three months
+ Deposits with a term of up to
two years
M3 11,863 M2
+ Securities repurchase agreements
+ Money market fund shares
+ Money market paper
+ Bonds up to
two years Offenburg University of Applied Sciences
The role of central banks
145 Prof. Dr. Sybille SchwarzA central bank has the task of monitoring the banking system and controlling the money supply, 
the amount of money available in an economy. Corresponding measures 
are referred to as monetary policy. 
The European System of Central Banks (ESCB) consists of the European 
Central Bank (ECB) and the national central banks of the member states. The association 
between the ECB and the central banks that implement a common monetary policy is 
called the Eurosystem. 
The primary task of the ECB is to ensure price stability (inflation rate 2%), 
as well as supporting the general economic policy of the EU, conducting 
foreign exchange transactions, managing currency reserves, securing payment systems and participating in the supervision of banks. 
See Mankiw / Taylor (2018), pp. 809 -813. Offenburg University of Applied Sciences
Monetary policy instruments of the central bank
146 Prof. Dr. Sybille SchwarzThree instruments are mainly available:
1.In open market transactions, securities are bought or sold by the central bank. The money supply is increased when fixed-interest securities are bought on the bond market. The money supply is reduced when securities owned by the central bank are sold on the open market.
2.A refinancing rate is set at which the central bank is willing to provide liquidity in the short term. An increase in the refinancing rate increases the short-term procurement of liquidity, so that banks restrict their lending and the money supply is reduced.
3.By changing the minimum reserve requirements, the minimum amount of reserves that banks must hold on their deposits is set. An increase in the requirements reduces the money supply. See Mankiw / Taylor (2018), p. 810. Offenburg University of Applied Sciences
Banks, the money supply and money creation
147 Prof. Dr. Sybille SchwarzDeposits that banks receive but do not lend out are called reserves. If banks hold all of the deposits as reserves, the money supply does not change. However, if only a certain percentage of the deposits (reserve ratio) is held and the rest is given out as loans, the money supply increases through money creation. For example, if a bank receives a deposit of €100, keeps €10 as a reserve and lends out €90, the money supply has grown to €190: €100 in deposits plus €90 in cash held by the borrower. This flows to the second bank, which in turn lends out 90 percent of the deposits (€81) and holds 10 percent in reserves (€9). The process 
repeats and the sum of newly created money will be € 
1000 (100 + 90 + 81 + ...).

The amount of money that banks create from each euro of original deposits is called the 
money creation multiplier.

See Mankiw /Taylor (2018), pp. 813 -818. Offenburg University of Applied Sciences
The classical inflation theory
148 Prof. Dr. Sybille SchwarzThe increase in the general price level is called inflation. The general price level adjusts to bring the money supply and money demand into balance. The money supply is controlled by the central bank. The money demand depends on the price level. A higher price level (lower money value) increases the amount of money demanded.

According to the quantity theory of money (Milton Friedman, 1912 -2006), the existing amount of money determines the money value, and the growth of the money supply is the main cause of inflation. Changes in the money supply do affect nominal quantities that are expressed in monetary units, but not real quantities that are expressed in quantity units (neutrality of money).
According to the Fisher effect (Irving Fischer, 1867-1947), an increase in the inflation rate leads to a corresponding increase in the nominal interest rate, but the real interest rate remains unchanged. See Mankiw/Taylor (2018), pp. 834-839. Offenburg University of Applied Sciences
Monetary equilibrium
149 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 837. Offenburg University of Applied Sciences
Increase in the money supply
150 Prof. Dr. Sybille SchwarzSee Mankiw /Taylor (2018), p. 838. Offenburg University of Applied Sciences
Money circulation velocity and quantity equation 
151 Prof. Dr. Sybille SchwarzThe money circulation velocity (V) measures the speed at which a euro changes hands in the 
economy. The relationship between the money supply (M), the circulation velocity (V) and the nominal value of the goods produced (P x Y) is expressed by the 
quantity equation: M x V = P x Y. It shows that an increase in the 
money supply must be reflected in one of the three other quantities: either the 
price level must rise, the production level must increase or the circulation velocity 
decrease. 
Example: In an economy, 100 pizzas are sold per year at a price of €10, 
the money supply is €50. V = (10 € x 100)/ 50 € = 20. Since a total of 1000 € per year is spent on pizza with a money supply of only 50 €, each € must change hands on average 20 times per year. 
See Mankiw /Taylor (2018), pp. 842 -846. Offenburg University of Applied Sciences
Part 8: Questions for the self-test
152 Prof. Dr. Sybille Schwarz1. The GDP is 5 trillion euros, tax revenues 1.5 trillion euros, private savings 0.5 trillion euros and public savings 0.2 trillion euros. Calculate the amount of 
consumer spending, government spending, savings and investments.
2. How does the central bank proceed when it wants to increase the money supply through open market operations?
3. To what extent is inflation a type of tax?
4. Hyperinflation (exceptionally high inflation rates) rarely occur in countries with a central bank that is independent of the government. What could be the reason for this?
5. Suppose the money supply of an economy is 500 billion euros, the nominal GDP is 10 trillion euros and the real GDP is 5 trillion euros. What is the price level? What is the velocity of circulation of money? Offenburg University of Applied Sciences
Part 8: Answers to the self-test
153 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 9: Economic fluctuations
Key points:
•Economic cycles
•Economic models
154 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Economic cycles
155 Prof. Dr. Sybille SchwarzThe growth rate of GDP shows a sequence of highs and lows, of phases with rising and falling growth rates and even years of negative growth. At the peak of the economic development, economic activity reaches its peak and the level of production begins to fall. If overall economic development increases quickly, this is referred to as a boom; if the level of production falls, the economy shrinks. At the bottom, economic activity reaches its lowest point. Periods with shrinking incomes and rising unemployment are referred to as recessions; negative growth rates are referred to as depressions. Fluctuations in economic growth around a growth trend are referred to as the economic cycle. See Mankiw/Taylor (2018), pp. 907 -911. Offenburg University of Applied Sciences
156Phases of the economic cycle
Economic fluctuations are cyclical growth fluctuations around the growth trend, 
where the increase in the domestic product is used as a measure. 
The basic pattern of an economic cycle distinguishes four phases:
1.Economic boom: high investment activity, price increases,
2.Downturn: decline in demand, production and investment, 
3.Crisis, trough: high unemployment, low capacity utilization, 
4.Upswing: initially slow, then accelerating increase in production, 
sales increase, unemployment decreases.
In order to obtain information on economic development, leading indicators, 
lagging indicators or presence indicators are recorded, which reflect future, past or 
current economic development (e.g. ifo business climate index, 
capacity utilization, price indices, range of order backlogs). Offenburg University of Applied Sciences
Phases of an economic cycle
157 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Economic cycles as a deviation of GDP from the trend
158 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Economic models
159 Prof. Dr. Sybille SchwarzEconomic models provide different explanations for economic cycles. 
There are models that assume that markets are quickly cleared in the event of imbalances (flexible prices and wages) and return to equilibrium (New Classical Macroeconomics). 
Other models assume that markets adapt only slowly, so that for a certain period of time the welfare of all those involved is not maximized (Keynesian models, 
cf. John Maynard Keynes, 1883 -1946). 
Schumpeter's explanation of the economic cycle (Joseph Schumpeter, 1883 -1950) distinguishes 
between invention and innovation. While inventions occur evenly, innovations spread in cyclical fluctuations. "Pioneer entrepreneurs" are needed to test new inventions. Investment activity begins, leading to a boom phase, 
the end of the upswing is reached when investment activity declines again. 
See Mankiw /Taylor (2018), pp. 919 -927. Offenburg University of Applied Sciences
Kondratieff cycles (theory of long waves)
160 Prof. Dr. Sybille SchwarzIn 1926, the Russian economist Nikolai Kondratieff first described 
economic fluctuations that occur in long waves of 40 to 60 years. 
Schumpeter drew on these findings and determined that at the beginning of 
every cycle there is a basic technical innovation that profoundly changes the economy, leads to increases in productivity, until the technical added value is exhausted 
and an economic downturn occurs. 
This "creative destruction" in the sense of a displacement of established technologies by 
new innovations is, according to Schumpeter, the prerequisite for the start of a new 
cycle. Shorter economic cycles move around the long waves, which can be influenced, for example, by monetary policy decisions or fiscal policy measures. The long waves, on the other hand, develop largely independently. Offenburg University of Applied Sciences
161Important building blocks of Keynesian theory
The starting point for Keynesian theory is the demand for goods, which determines production. According to Keynes, the price mechanism does not necessarily lead to stabilization, but requires economic policy measures to stimulate demand. Important building blocks for this are consumption and investments:
Consumption: (consumption function: C = Ca+ cY) 
Consumer spending is determined by current income (Y) and autonomous 
consumer spending (Ca), which is independent of the level of income. 
The marginal propensity to consume (c) indicates by how many euros households increase their 
consumer spending when income increases by one euro. 
Investments increase the capital stock and thus the production potential and 
enable an increase in production, income and employment. According to Keynes, if the propensity to invest is weak, the state must stimulate overall demand (keyword: anti-cyclical fiscal policy, risk of a “crowding out” in the sense of a displacement effect). Offenburg University of Applied Sciences

Anticyclical fiscal policy according to Keynes

162 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
163The multiplier effect according to Keynes
According to Keynes, a demand stimulus through an expansive fiscal policy increases income and thus consumption with the help of the multiplier effect.
Argumentation:
➢If the state increases spending, companies adjust their supply of goods to the additional demand.
➢Production and income increase due to this stimulus effect.
➢Households base their consumption on income.
➢Accordingly, they demand more goods, which increases production and income again (process of consumption-induced income increase)
See Mankiw/Taylor (2018), p. 938. Offenburg University of Applied Sciences
164Exercise on the multiplier effect
Assumption: Y = 100 + 0.75Y + 300 + 400; where YN = 3200, C = 100, I = 300, G = 400
In period 1, government spending is increased by 100. What effect does this stimulating effect have
according to Keynes in the next period and the period after that? Offenburg University of Applied Sciences
Part 9: Questions for the self-test
165 Prof. Dr. Sybille Schwarz1.Companies notice that their inventories have increased. How can this happen, how will companies react to it and what impact will this reaction have on the overall economic production level?
2.Your roommate reads in the FAZ that the order intake index for German industry has increased significantly, but at the same time the unemployment rate has also increased. Is there a misprint?
. Offenburg University of Applied Sciences
Part 9: Answers to the self-test
166 Prof. Dr. Sybille Schwarz. Offenburg University of Applied Sciences
Part 10: Selected key topics
Contents (optional):
•Interdependence and trade advantages
•Discussion of current economic topics and questions, e.g.
Cryptocurrencies, climate policy…
167 Prof. Dr. Sybille Schwarz Offenburg University of Applied Sciences
Interdependence and trade advantages
168 Prof. Dr. Sybille SchwarzInterdependence and trade are desirable because they enable a greater variety of goods. Trade increases the welfare of nations, even if one of the two countries can produce all products more cheaply. What is crucial are the different production cost ratios, not the absolute cost advantages and thus the specialization in the production of those goods that are produced with the lowest comparative costs (opportunity costs).
Cf. Mankiw /Taylor (2018), pp. 579 -597. Offenburg University of Applied Sciences
Trade restrictions
169 Prof. Dr. Sybille SchwarzImport tariffs restrict trade. It is equivalent to a tax on goods produced abroad and thereby reduces the amount of imports. The tariff causes a loss of welfare. It does increase the price of goods achieved by domestic producers, who expand their production. At the same time, it increases the price for consumers, who reduce consumption.

Import quotas limit the permitted import quantity. They also reduce the amount of imports, increase the domestic price of a good and the welfare of the producers.
On the other hand, they reduce the welfare of consumers and lead to a net loss of welfare in the economy as a whole.

There are also non-tariff trade barriers, e.g. strict norms and standards for imported products or bureaucratic obstacles.

Trade restrictions are justified in various ways, e.g. with the employment argument, the security argument or the argument of unfair competition.
See Mankiw / Taylor (2018), pp. 592 -604. Offenburg University of Applied Sciences
Exchange rates
170 Prof. Dr. Sybille SchwarzThe two most important international prices are the nominal and the real exchange rate.
The nominal exchange rate is the ratio at which the currency of one country is exchanged for the currency of another country. The quantity quotation indicates, for example, the quantity of foreign currency units received for one unit of domestic currency. If, for example, fewer units of domestic currency are needed to buy one unit of foreign currency than before, this is referred to as an appreciation of the domestic currency.
The real exchange rate is the ratio at which goods and services from one country are exchanged for those from another country. It measures the price of a domestic bundle of goods in relation to a foreign bundle of goods. For example, a devaluation of the domestic currency leads to a relative reduction in the price of domestic goods compared to foreign goods, which leads to an increase in net exports. 
See Mankiw/Taylor (2018), pp. 869 -878. Offenburg University of Applied Sciences
Cryptocurrencies
171 Prof. Dr. Sybille SchwarzCryptocurrencies such as Bitcoin are designed as a means of payment and a payment system.
Bitcoin is a system of transactions that are stored in a blockchain. A central role is played by the “miners”, who confirm the correctness of transactions,
but also create Bitcoin units. There is a fixed supply function that cannot be deviated from, so that a maximum of 21 million Bitcoin can be created.
Bitcoin is scarce, the Bitcoin price arises from the interaction of supply and
demand. The Bitcoin supply is programmed. On the demand side there are
private investors all over the world, and in some cases companies in the
USA have already exchanged part of their cash balance from dollars to Bitcoin. Offenburg University of Applied Sciences
Part 10: Questions for the self-test
172 Prof. Dr. Sybille Schwarz1.If a Japanese car costs 500,000 yen and a comparable German car costs 10,000 euros and at the same time one euro is worth 100 yen, what are the nominal and real exchange rates?
2.A can of beer costs 0.75 dollars in the USA and 0.60 euros in Germany. What is the nominal exchange rate? What effects would it have if an expansionary monetary policy in the USA led to the price of a can of beer doubling?
. Offenburg University of Applied Sciences
Part 10: Answers to the self-test
173 Prof. Dr. Sybille Schwarz. 
Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University of Applied Sciences Organizational details
•Procedure: lecture + internship
–Date: Thursday 8:00 a.m. - 11:15 a.m.
–Division of lecture/internship varies
(usually 1st block lecture, 2nd block internship)
→will be announced on Moodle
•Lecture:
–Slide presentation
–Freehand notes
–Code examples
•Exam: oral (20 min)
–All of the above is relevant to the exam 3 Offenburg University Organization
•Internship
–Exercises on the lecture material
•Programming
•Understanding, analysis, calculation
–Usually submission of
•Program code 
•1-page “paper” on the tasks
•Usually every 14 days
–Programming in 
•Java 
•CUDA/C
• … 4 Offenburg University
Parallel Programming
Concepts and Practice
Bertil Schmidt, Jorge González-
Domínguez, Christian Hundt, Moritz 
Schlarb
2018 Elsevier (Morgan Kaufmann)
approx. 61.50 EUR
Language: C++ 11
Methods
C++11 Multithreading, OpenMP, CUDA, 
MPI, UPC++Literature 5 Offenburg University
Concurrent programming with Java
Concepts and programming models for 
multicore systems
Jörg Hettel, ManhTien Tran
2016 dpunkt.verlag
approx. 35.50 EUR
Language: Java
Methods
Multithreading, thread pools, atomic, 
Fork-Join, locks, secure data structures, ...Literature 6 Offenburg UniversityTopics today
•Motivation 
–Modeling perspective
–Architecture perspective
•Basic concepts
–Parallel, concurrent, distributed programs
•Threads
–Example: Java
•Parallelization of algorithms
–2 examples 7 Offenburg University of Applied SciencesParallelism in computer science
•Why parallel programming?
–Modeling perspective (software): “The world is parallel”
Goal: mapping this parallelism in modelling
–Architectural perspective (hardware): processor systems are parallel
Goal: using computer resources 8 Offenburg University of Applied SciencesModeling perspective
•Individual application
–Subtasks to be processed simultaneously in a software, e.g.
•Calculations, queries
•User interface (display, interaction)
–Shared access to resources
•Booking systems
•Traffic systems
•Operating system
–Processes to be processed simultaneously, e.g.
•Active applications
•Background processes (virus scan, etc.)
•System processes
→Concurrency:
Parts of the system must (be able to) run alongside each other 9 Offenburg University Why parallel/concurrent 
programming?
•Program structure better reflects the problem
•Throughput
–Less waiting, e.g. for I/O processes
•Response
–Interactions can be handled with the desired priority
•Increase in execution speed
–Use of as many available computing resources as possible 10 Offenburg UniversityArchitectural perspective:
Moore's Law (1960)
"Processor performance* doubles approximately every 18 -24 months"
(*actually: transistor density)
→Newer hardware automatically speeds up software
Really?
This has no longer been the case since the 2000s!
"The free lunch is over" (H. Sutter, 2005)
The number of transistors continues to increase, but no longer
on one processor (core) 11 Offenburg UniversityMoore's Law in Key Figures 12 Offenburg University of Applied SciencesArchitecture perspective
•Multicore processors (4, 8, 16, ... cores)
–Increase in performance required (Moore's Law)
–Limits of miniaturization and higher clock speeds
•Heat generation, cooling
•Principle physical limits
• "Manycore" architectures (>hundreds of cores)
–Graphics processors (GPU)
–Tensor processors (TPU)
•Computer clusters, supercomputers 13 Offenburg University of Applied Sciences “Multicore crisis”
•Increase in performance is (initially) only theoretical
–Good for already concurrent systems (independent parts are distributed 
across cores)
–But does not automatically make individual applications faster
–It is not uncommon for them to even become slower!
•In practice, there is often a critical, resource-hungry application that needs full performance
–Server application
–Complex/intensive calculations
→Parallelization of applications/calculations necessary! 14 Offenburg University of Applied Sciences Tightening: Many-Core
•Hundreds or thousands of cores
–e.g. modern graphics cards (GPUs), vector/tensor processors
–Usually limited instruction set
–Lower performance of individual cores
–Tight physical parallelism 
(SIMD model: “single instruction, multiple data)
→Forces parallel processing 
(even of actually sequential problems)
•Parallel algorithms
–Parallelized variants of “classic” or new algorithms
–Building blocks (“parallel building blocks”) 15 Offenburg UniversityBasic concepts
•Concurrent program
–Multiple threads of action
–Logically simultaneous
–Low dependency
•(Really) parallel program
–Multiple processors
–(also) physically simultaneous
–Strong dependency 
(for overall solution)
•Distributed program
–Multiple computers
•Multiple processors
•Multiple threads of action
•Separate resources•Coordinate (relatively) 
independent activities
•Try to divide a given 
activity into 
parallel sub-steps
•Coordinate activities 
only via messages 18 Offenburg University Threads for parallelization
•Dividing a task into tasks that can be completed (as far as possible)

independently of each other and thus simultaneously

•Each thread completes one task (or several)
•The threads can (hopefully) use all available

processors/cores

→Acceleration of the program flow is possible 19 Offenburg UniversityExample: Array Count
•Determine the number of elements in an array that meet a 
specific condition.
•Algorithmic building block often required in 
programming (e.g. filtering, selecting, etc.)
•Very easy to parallelize with kprocessors by 
dividing the array into kparts
•Each part is processed by a thread101111001101011111011011 20 Offenburg University Array count: Thread
classCountThread extends Thread {
private boolean [] array;
private int lo;
private int hi;
private int result;
publicCountThread( boolean[] array, intlo, inthi) {
this.array= array;
this.lo= lo;
this.hi= hi;
}
public void run() {
for(inti = lo; i <= hi; i++)
if(array[i]) // condition
result++;
}
public int getResult() {
returnresult;
}
}Pass parameters 
in the constructor!
Pass result 
using method! 21 Hochschule OffenburgArray-Count: Aufteilung
public static void main(String[] args) {
...
CountThread[] counter = newCountThread[ NUMBER_OF_THREADS ];
intlo = 0;
inthi;
intsize = ARRAY_SIZE / NUMBER_OF_THREADS ;
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
if(i < NUMBER_OF_THREADS - 1) 
hi = lo + size - 1;
else
hi = ARRAY_SIZE - 1;
counter[i] = newCountThread(array, lo, hi);
counter[i].start();
lo = hi + 1;
}
... 22 Offenburg University Array count: 
Merge results
// Synchronization (wait for all threads)
try{
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
counter[i].join();
}
} catch(InterruptedException e) {
e.printStackTrace();
}
// Calculate total result from partial results
intresult = 0;
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
result += counter[i].getResult();
}join() merges the 
thread for which
it is called with the
calling thread. 23 Offenburg University Array count: 
Merge results
// Synchronization (wait for all threads)
intresult = 0;
try{
for(inti = 0; i < NUMBER_OF_THREADS ; i++) {
counter[i].join();
result += counter[i].getResult();
}
} catch(InterruptedException e) {
e.printStackTrace();
} 24 Offenburg UniversityAcceleration through parallelization 27 Offenburg University Speedup for array count 
with k processors
•Array is broken down into k parts (1 ≤ k≤ n)
•The more parts, the higher the parallelization
–Effort per thread: O( n/k)
•But: the more parts, the higher the effort when 
merging the results
–Effort: O( k)
•Total effort: O(n/k) + O(k) 28 Offenburg University of Applied SciencesParallel algorithms
•Previous implicit assumption: 
–k fixed (constant) by the number of processors in the system
•New perspective:
–We consider k as selectable depending on the input size n
–We call k= P(n) the processor complexity of the algorithm
–Preliminary assumption: “We can use as many processors 
as we need” (to achieve an optimal result)
–We will deal with the actual speedup on a specific system 
with p processors later ☺ 29 Offenburg UniversitySpeedup for array count?
•The more parts, the higher the parallelization
–Effort per thread: O( n/k)
•But: the more parts, the higher the effort when merging the results
–Effort: O( k)
•Total effort: O(n/k) + O(k)
–What would be a good/optimal value for kin depending on n? 31 Offenburg University of Applied SciencesAdvanced example: Editing distance
Problem:
For two given strings A and B, find the minimum
number of editing operations to convert A into B.
Editing operations:
- Inserting a character
- Deleting a character
- Replacing a character 32 Offenburg University Editing distance
Mini-example: A= SONG B= HONEY
2 editing operations necessary/sufficient:
- Replace S with H: SONG→HONG
- Insert I: HONG→HONEY
i.e. the editing distance d(A,B) is 2.
Observations:
(1) 0 ≤d(A,B) ≤ max(| A|,|B|)
(2)d(A,B) = d(B,A) 33 Offenburg UniversityApplication areas
•Search engines
–Alternative suggestions for typos
•Word processing
–Recognition of spelling mistakes
–Automatic correction
•Plagiarism detection
–Finding slightly changed texts
•Databases
–Recognition of duplicates
•Bioinformatics
–Sequence alignment 34 Offenburg University Calculating the editing distance
•Idea:
If the editing distance for partial words (prefixes) is already 
known, the editing distance for longer partial words can be easily determined.
•Dynamic programming method:
Result is built up step by step from partial results 35 Offenburg University of Applied SciencesDynamic programming
•Goal: Calculate a ( m+1)×(n+1) matrix d with the 
editing distances of all prefix combinations of A and B
•Editing distance of A and B is in d[m,n]
•Determine the editing operations by backtracking H O N I G 
0 1 2 3 4 5
S11 2 3 4 5
O22 1 2 3 4
N33 2 1 2 3
G44 3 2 2 2 36 Offenburg University of Applied SciencesCalculating the matrix
// Calculates cell d[ i,j] for i,j> 0
calcCell (i,j)
if(A[i] == B[j]) 
d[i,j] = d[i-1,j-1] // nothing to do
else
d[i,j] = 1 + min( d[i-1,j], // insertion
d[i,j-1], // deletion
d[i-1,j-1] ) // replacement
H O N I G 
0 1 2 3 4 5
S1
O2
N3
G41 2 3 4 5
2
3
41 2 3 4
2 1 2 3
3 2 2 2 37 Offenburg UniversityAlgorithm
intm= A.length;
intn= B.length;
intd[][] = new int[m+1][n+1]; // Create matrix
for(inti=0; i<=m; i++) // Initialize column 0
d[i,0] = i;
for(intj=0; j<=n; j++) // Initialize row 0
d[0,j] = j;
for(inti=1; i<=m; i++) // Fill all other cells
for(intj=1; j<=n; j++)
calcCell (i,j);
returnd[m,n]; // Return result 38 Offenburg University of Applied SciencesEfficiency
•The algorithm runs in O(n·m): 
–(Essentially) n·m cells have to be calculated. 
–Every single cell can be calculated in a constant amount of time.
–The cells are calculated one after the other. 
•For long strings (e.g. in bioinformatics) this is very 
inefficient
–String length in the millions and more
•Is there a possibility for parallelization 
i.e. can cells be calculated simultaneously? 39 Offenburg University Observations
•The order of the nested for loop is irrelevant 
–Calculation by row or column is possible
•BUT: Calculating the cells in any order is not possible 
–Every row or column requires the previous row or column.
–Every row or column must also be calculated step by step.
–More precisely: 
Every cell requires, among other things, the cell that was just previously calculated (above or to the left) for the calculation. 40 Offenburg University of Applied SciencesParallelization
•Different calculation order:
–Not row or column by row, but diagonally
•All cells on a diagonal can be calculated independently of each other, i.e. in parallel
•BUT: before each new diagonal, the previous two diagonals must be calculated
–Synchronization necessary! S A M T A Y
0 1 2 3 4 5 6 7
S10 1 2 3 
T21 12
A321
R43 41 Hochschule OffenburgAnzahl und Größe der Diagonalen
S  A  M  S  T  A  G
0  1  2  3  4  5  6  7
S10  1  2 3  4  5  6
T21  12  3  3  4 5
A321  2  3  4 3  4
R43  2  2  3 4  4  4for(k=1; k<m); k++)
for(t=1; t<=k; t++)
calcCell (k-t+1, t)
for(k=m; k<=n; k++) 
for(t=k-m+1; t<=k; t++)
calcCell (k-t+1, t)
for(k=n+1; k<n+m; k++) 
for(t=k-m+1; t<=n; t++)
calcCell (k-t+1, t) 42 Offenburg University of Applied SciencesParallel algorithm (abstract)
Not very practical, but useful, e.g. for estimating runtime:
If we have m processors, the inner loop is processed in 1 
step (each processor calculates one cell).
This gives us n+m-1 steps.
As a reminder: The sequential algorithm requires n∙m steps.for(k=1; k<n+m; k++) { 
fort=max(1,k-m+1) tot<=min(k,n) in parallel {
calcCell (k-t+1, t)
}
synchronize
} 43 Offenburg University Calculation with mThreads
Each thread processes exactly one row.
Each thread works 1 column away from its neighbors.
After each step, all threads must be synchronized.
→We need a synchronization barrier. S A T I N G A Y
0 1 2 3 4 5 6 7
S1
T2
A3
R40 1 2 3 4 5 6
1 12 3 3 4 5
2 12 3 4 3 4
32 2 3 4 4 4Thread 0
Thread 1
Thread 2
Thread 3
for(intc=1-id; c<n+m-id; c++) {
if(c>=1 &&c<=n)
calcCell (id+1, c);
// There must be a BARRIER here!
} 44 Offenburg UniversityThe Java class CyclicBarrier
•Provides a barrier for a specified number m of 
threads (more precisely: runnables)
•When the await() method is called, 
each thread waits until the barrier is lifted.
•This happens when all m threads have 
arrived at the barrier (via an internal countdown).
•Optionally, a runnable can always be executed directly before 
the barrier is lifted (e.g. for sequential intermediate steps in the main program). 45 Hochschule OffenburgJava-Rahmen mit Barriere
class EditDistance {
final int m,n;
final int [][] d;
finalCyclicBarrier barrier;
publicEditDistance( char[] A, char[] B) {
m= A.length;
n= B.length;
d = new int[ m+1][n+1];
barrier = newCyclicBarrier ( m,
newRunnable() {
public void run() {
// optionale Aktionen
}
} );
initMatrix();
for(int i=0; i< m; i++) {
newThread(newCalcThread(i)).start();   
}
}
} 46 Hochschule OffenburgBenutzung in einem Thread
class CalcThread implements Runnable {
intid;    // 0 <= id < m
CalcThread( intthreadID) { 
id = threadID; 
}
public void run() {
for(intc=1-id; c<n+m-id; c++) {
if(c>=1 &&c<=n)
calcCell (id+1, c);
try{
barrier.await();
} catch(...) { };
}
}
} 47 Offenburg University of Applied SciencesWhat is the benefit of parallelization?
•If we have m processors, each diagonal is processed in 
a single step.
–This gives us a total of n+m-1 steps.
–As a reminder: 
The sequential algorithm requires n∙m steps.
•If n=20, m=16 on a 16-core computer: 
320 vs. 35 steps
•If n=800, m=768 on a GPU (NVIDIA GTX-1050): 
614,400 vs. 1567 steps.
•If the number of processors p< m, the 
number of steps is ( n+m-1) ∙m/p
•The actual speedup depends on other factors. Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences Topics
2 Offenburg University•Perfect parallelizability
–Example: Monte Carlo simulation
•Threading in Java
•Thread safety of objects
•Synchronization of threads
–Java language tools
–Libraries Parallelization
4 Offenburg University of Applied Sciences•There are problems that can be trivially parallelized:
–Completely independent steps
–No communication required
–No (or hardly any) synchronization
–Examples:
•Array count (see lecture 1)
•Graphics rendering
•Many problems in image processing
•Monte Carlo simulations
–“embarrassingly parallel”, “perfectly parallel” Monte Carlo Simulation
5 Offenburg University•Suitable for:
–Problems with unknown exact solutions
•Financial world
•Physics, chemistry
•Probabilities
–Processes/procedures can be easily simulated
•Idea:
–Run as many simulations as possible
→Law of large numbers
–Individual simulations are completely independent
→Perfectly parallelizable Simple example
•Determining π
–Area of ​​the unit circle:
F = πr² = π
–Determine as many random points as possible in the unit square
–Count how many of these points are in the unit (quarter) circle
–A value for π can be approximated from the proportion
6 Offenburg University Repetition: Threads in Java
8 Offenburg University of Applied Sciences•Instance of the Thread class or a subclass 
of it, which is activated as a hardware thread using the start() method (only then does the thread become “alive”).
•2 methods for creating threads:
–Direct derivation of Thread
–Implementation of Runnable Derivation of Thread
9 Offenburg University class MyThread extends Thread{
...
public void run(){
// whatever this thread does
}
...
}
// Create thread object 
MyThread t = newMyThread(...);
// Activate thread 
t.start(); Implementation of Runnable
10 Offenburg University class MyClass implements Runnable {
...
public void run(){
// whatever this thread does
}
...
}
// Create thread object
Thread t = new Thread(newMyClass(...));
// Activate thread 
t.start(); Implementation of Runnable
11 Offenburg University class MyClass extends OtherClass implements Runnable {
...
public void run(){
// whatever this thread does
}
...
}
// Create thread object
Thread t = new Thread(newMyClass(...));
// Activate thread 
t.start(); Simultaneity
16 Offenburg University•Threads logically run at the same time
•In reality: ???
•Scheduler (JVM) changes states of threads
–decides which thread can run (“running”) and which
has to wait (“runnable” / “suspended”) →Competition!
–doesn’t always have to be “fair”
–“arbitrary” switching back and forth (switching)
–(almost) no influence from programming
→Program as if all threads ready to run

were actually running at the same time (and leave the rest to the system) Java Threads: Life cycle
Quelle: Th. Letschert
T is selected by scheduler
and activated
T cannot be continued, because it is &
- waiting for data
- waiting to enter a synchronized method
- sleeping
T is selected by scheduler and
deactivatedT can be continued because &
- the expected event has happened
- data is available
-synchronized methode is unlocked
- sleeping time is over Thread Scheduling
20 Offenburg University •Threads run logically at the same time
•Scheduler (JVM) changes the states of the threads
–decides which thread can run (“running”) and which has to wait (“runnable” / “suspended”) →Competition!
–doesn’t always have to be “fair”
–“arbitrary” switching back and forth
–(almost) no influence from programming
•Threads are executed in an interleaved or physically parallel manner
(or both) What does thread scheduling mean for the
program?
•Without threads: deterministic
•With threads:
–Transitions:
•If we program as if all tasks 
that can be executed simultaneously are actually executed simultaneously, this 
nondeterminism cannot cause any harm.
•What if the threads are not completely independent?
21 Offenburg Universitynot (completely) deterministic
ready < –> running unknown! Multi-threading
23 Offenburg University•The activities of the threads are often independent
of each other
→Existence of threads and scheduling can be ignored
•BUT: Sometimes threads come into conflict:
–Parts of the program that can be used by one thread without any problems
cause problems when multiple threads
use them What can happen?
24 Offenburg University • Race condition
– Multiple threads modify the same object; the result varies depending on the scheduling
• Deadlock
– Multiple threads block each other because they are waiting for resources that are currently being held by another thread
• Thread starvation
– A thread is waiting for a resource that is never allocated to it Race Condition –Example
25 Offenburg University•Our array counter (see lecture 1)
•So far:
–Main thread waits for the end of all threads, then fetches the
results and sums them
–Problem: threads that have already ended have to wait
•Alternative idea:
–As soon as each thread has calculated its sum, it adds its
result to the overall result
–As soon as the last thread has ended, you immediately have the
result101111001101011111011011 Previous
26 Offenburg Universityclass CountThread extends Thread { 
private boolean [] array; 
private int lo;
private int hi; 
private int result;
public CountThread( boolean[] array, int lo, int hi) { 
this.array = array;
this.lo = lo; 
this.hi = hi;
}
public void run() {
for(int i = lo; i <= hi; i++) 
if(array[i])
result++;
}
public int getResult() { 
returnresult;
}
}Pass parameters 
in the constructor!
Pass result 
using a method! Previous
27 Offenburg University // Synchronization (wait for all threads)
try{
for(int i = 0; i < NUMBER_OF_THREADS ; i++) { 
counter[i].join();
}
} catch(InterruptedException e) { 
e.printStackTrace();
}
// Calculate total result from partial results 
int result = 0;
for(int i = 0; i < NUMBER_OF_THREADS ; i++) { 
result += counter[i].getResult();
}join() merges the 
thread for which 
it is called with the 
calling thread. Now
28 Offenburg Universityclass CountThread extends Thread { 
private boolean [] array; 
private int lo;
private int hi; 
private int result;
static int overall_result = 0;// Common variable for overall result
public CountThread( boolean[] array, int lo, int hi) { 
this.array = array;
this.lo = lo; 
this.hi = hi;
}
public void run() {
for(int i = lo; i <= hi; i++) 
if(array[i])
result++;
overall_result +=result;
}Add result to 
overall result. Array count: result
30 Offenburg University // Synchronization (wait for all threads)
try{
for(int i = 0; i < NUMBER_OF_THREADS ; i++) { 
counter[i].join();
}
} catch(InterruptedException e) { 
e.printStackTrace();
}
System.out.println( "Result = " + CountThread. overall_result );
Does this work? Race conditions
31 Offenburg University of Applied Sciences•Different implementations lead to
different and incorrect results
•can make classes thread-unsafe
•are generally to be avoided (there are exceptions) Access to shared resources
32 Offenburg University of Applied Sciences•Synchronization necessary to avoid race conditions
(partial results are “forgotten”)
•The resource to be changed must be protected from simultaneous
changes Problem
read add
33 Offenburg University write•Adding is not an atomic operation
overall_result += result corresponds to:
–Read overall_result into a register x
–Add the value of result to x
–Write the result back into overall_result
•A context switch (thread switching) can take place between the sub-operations
read add write
Thread 1
Thread 2 Atomicity
•An operation is atomic if it is only executed by one thread.
34 Offenburg University Atomicity in Java
•Simply adding is not thread-safe
(because it is not atomic)
•Atomic is for “small” primitive data types
(int, char, byte, short, float, boolean):
–Simple reading
–Simple writing
•but not for long, double
•For general objects (reference types) there are no guaranteed
atomic operations
•Responsibility of the programmerNot both together!
35 Offenburg University Thread safety
36 Offenburg University of Applied Sciences•A class is thread-safe if it remains correct even if its code is executed by multiple threads, regardless of
–scheduling of threads
–other entanglements
–executing code affects all publicly accessible
•static fields and methods
•methods and fields of any instance
–Correct means: behavior according to specification How can you achieve thread safety?
37 Offenburg University •Make operations atomic
–by “locking out” other threads during execution
•Statelessness of classes
–No shared resources
–Each method provides its own variables/objects Statelessness
38 Offenburg University of Applied Sciences•Guarantee that each thread gets its own instances
of all fields used
–Class or object has no state that can change
(immutability)
–e.g. for classes that function as a service and perform a calculation
that is based only on the input of the threads
–in practice
•often not possible
•even more often not sensible Synchronization
39 Offenburg University•Guarantee, ...
–that only one thread can change the state of an 
object (or class) at any time (mutual exclusion)
and
–that when reading, a sensible, i.e. consistent, state (according to specification) always prevails Synchronization
40 Offenburg University•Eliminates non-determinism due to

concurrency
•Exclude undesirable sequences
•Two types:
–Competitive synchronization
•Dealing with competition
•Means: mutual exclusion
–Conditional synchronization
•Enabling cooperation
•Means: conditions, barriers Cooperative multitasking
41 Offenburg University•Goal:
–not to protect one thread from another, but rather:
–the entire program runs sensibly / correctly / faster
•Threads make programs non-deterministic:
–it is not the program that decides on the exact sequence, the
scheduler (part of the JVM) also decides on it
–non-determinism is usually desired
–occasionally the non-determinism must be reduced again:
•In synchronization situations the arbitrariness of the scheduler must be
limited:
→the program influences (synchronizes) the thread execution! Cooperative multitasking
Totally non-deterministic program execution (no synchronization)
Threading, restricted non-determinism:
Atomic actions are not interrupted
everything within a thread is executed sequentially
Synchronization constructs:
Additional restriction of non-determinism
(only as much as necessary, as little as possible)
Totally deterministic program execution (completely sequential)
42 Offenburg University Contest synchronization
43 Offenburg University of Applied Sciences•Guarantee atomicity
•Options:
–Use atomic (through Java specification) constructs
–Libraries, e.g. java.util.concurrent.atomic
–Enforce atomicity through locks (locks, monitor)
•Explicitly programmed: synchronized
•Implicitly through libraries, e.g. java.util.concurrent.atomic Explicit locks
44 Offenburg University•In Java with the keyword synchronized
•Can be used for
–Methods:
–Code blockssynchronized void setValue(… x) {
this.x = x;
}
void setValue(… x) {
…
synchronized (o) {
this.x = x;
}
}
•A lock (Monitor or Lock) is set in each case synchronized
45 Offenburg University •For non-static methods, the monitor of the object whose synchronized method is called is used. •For static methods, the monitor of the class in which the synchronized method is defined is used. •For code blocks, an (arbitrarily selectable) object must be specified whose monitor is used. Monitor
•Every Java object has a monitor
–either locked (held by a thread)
–or free
•When calling a synchronized method/block
–If monitor is free, then
•Monitor is locked (held by this thread)
•Method/block entered and executed
–If monitor is locked, then
•Thread is set to blocked state by the scheduler
(Exception: the same thread already has the monitor)
46 Offenburg University Monitor
•Every object has a monitor
–either locked (held by a thread)
–or free
•When leaving a synchronized method/block
–All threads blocked because of this monitor

go into the ready state
–One of them gets the monitor, locks it and
enters the method/block
–All others are immediately blocked again
47 Offenburg University Properties of monitors/locks
48 Offenburg University Properties of monitors/locks
49 Offenburg University Additional waiting conditions

Sometimes it is necessary for threads to wait even though no monitor is occupied because the object cannot be processed for other reasons.

Typical example: Producer-Consumer pattern
•Data structure with limited capacity (see Lab 2)
•Insert only when there is space again
•Remove only when there is something to remove

At the same time, you want to avoid the waiting costing computer resources (busy waiting) Waiting threads
Options
(1) Thread waits “outside” the object, e.g.
while(stack.isFull()) {
... 
}
stack.push(...);
Problems:
Busywaiting, costs computer resources (solution: sleep)
Thread may never get a turn
Context switch after while and before push→Racecondition
(2) Waiting within the object in the synchronized method Waiting within the method
1st attempt:
synchronized void push(longk) {
while(isFull()) {
...
}
h[end++] = k;
}
Even worse:
The method will never be left if isFull()== true Waiting within the method
2nd attempt: push no longer synchronized
voidpush(longk) {
while(isFull()) {} // isFull synchronized
h[end++] = k;
}
Also not correct:
Method content not atomic!
Context switch between instructions possible. Solution: wait / notify
Methods of java.lang.Object
can only be called within synchronized methods or blocks
wait() can throw an InterruptedException
wait()
causes the current thread to wait (the scheduler blocks it
→no busy waiting!)
At the same time, the object's monitor is released (otherwise no
other thread could do anything in the object that is also synchronized) Warten mit wait()
Korrekt:
synchronized void push(longk) {
while(isFull()) {
try{ 
wait(); 
} catch(InterruptedException e) { ... }    
}
h[end++] = k;
} wait / notify
notify() 
•causes the scheduler to notify one of the threads that is waiting for this object
•The notified thread must first acquire the monitor again before it can continue!
•only at the end of the method from which notify was called is the lock released by the current thread
notifyAll()
•Like notify, but all threads that are waiting for the object are notified
•Can always be used instead of notify, but not the other way around!
•less efficient if there are many threads Notification with notify()
Notify:
synchronized long pop() {
longresult = h[--end];
notifyAll();
returnresult;
} Kombination wait/notify
Komplett:
synchronized void push(longk) {
while(isFull()) {
try{ 
wait(); 
} catch(InterruptedException e) { ... }    
}
h[end++] = k;
notifyAll();
} Kombination wait/notify
Notify:
synchronized long pop() {
while(isEmpty()) {
try{
wait();
} catch(InterruptedException e) { ... }
}
notifyAll();
returnh[--end];
} Thread-safe data structures
Thread-safe versions of various data structures are available in the 
package java.util.concurrent
Examples of blocking data structures:
ArrayBlockingQueue
LinkedBlockingQueue
SynchronousQueue
Non-blocking but thread-safe data structures
ConcurrentLinkedQueue
& Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg UniversityTopics
•Limits of parallelization
•The PRAM model
•Analysis of parallel algorithms 4 Offenburg UniversityLimits of acceleration
•How much can an application be accelerated by
parallelization?
•Idealized assumptions
–we have as many processors as we want
–our parallelized portion is p times faster with pprocessors than with one
•What is the upper limit for the speedup?
–Remember: Speedup S(p) = tseq/ tpar(p) 5 Offenburg UniversityAmdahl's Law (Gene Amdahl, 1922-2015)
•Every application also has a non-parallelizable part, 
e.g. loading, initializing variables, etc.
•Let 0 < Par< 1 be the parallelizable part of the runtime tseq
•If 1 is the total runtime, then the time for the non-parallelizable part is ( 1 –Par), since 1 = Par+ (1 –Par)
•Only Par can be accelerated: with p processors, the time can be reduced to at most Par/ p.
•The total time would then be ( 1 –Par) + Par/ p
•This means that we can never be faster than ( 1 –Par), no matter how many processors we have!
•Speedup S(p) = 1 / ( 1 –Par + Par/p) ≤ 1 / ( 1 –Par) 6 Offenburg University Amdahl's Law: Example
•The non-parallel part of a program takes up 10% of its runtime, 90% is (perfectly) parallelizable ( Par = 0.9 ; 1-Par = 0.1 )
–With 2 processors we need
•0.45 + 0.1= 0.55 of the time
Speedup: 1.8x
–With 4 processors it is
•0.225 + 0.1= 0.325
Speedup: 3.1x
–With 16 processors
•0.056 + 0.1= 0.156
Speedup: 6.4x
–With 64 processors
•0.014 + 0.1= 0.114
Speedup: 8.8x
Source: Wikipedia 7 Offenburg University Amdahl = bad news?
•Yes, for problems of a fixed size
–These can only be accelerated up to a certain point with more and more processors
• Amdahl's Law assumes that the parallelizable part Par is independent of the number p of processors
•In reality, things often look different:
–You want to solve a larger problem in the same amount of time and use more processors for this
–In this case, Par is usually also significantly larger because the non-parallelizable part of the program is often almost constant 8 Offenburg UniversityGustafson's Law (John Gustafson, 1988)
•Another perspective on acceleration (speedup): 
With more processors, the problem size can
increase
•Gustafson: S(p) = (1 –Par) + p∙ Par
“How much more can I do in the same amount of time with pprocessors than with 
one?”
•See Amdahl: S(p) = 1 / ( 1 –Par + Par/p)
“How much faster does it take to solve the same problem with pprocessors than with 
one?” 9 Offenburg UniversityGustafson's Law: Example
•The non-parallel part of a program takes up 10% of its work, 90% can be (perfectly) parallelized (Par = 0.9; 1-Par = 0.1)
–With 2 processors we can achieve
•0.1+ 2* 0.9= 1.9 times
Speedup: 1.9x
–With 4 processors it is
•0.1+ 4* 0.9= 3.7 times
Speedup: 3.7x
–With 16 processors it is
•0.1+ 16* 0.9= 14.5 times
Speedup: 14.5x
–With 64 processors it is
•0.1+ 64* 0.9= 57.7 times
Speedup: 57.7x 11 Offenburg UniversityParallel computing models
•Sequential computing model
–Von Neumann architecture:
A program (machine code) is created in memory and 
executed by passing one instruction after the other to the CPU.

–There is only one memory for data and commands
–Relatively close relationship between hardware, software and 
theoretical modelsMemory
CPU 12 Offenburg University of Applied SciencesParallel computing models
•Various parallel computing and computer models
•Each is programmed differently
•Therefore, when designing algorithms, one always has a 
specific model in mind (e.g. editing distance)
•Different parallel machines are suitable for 
solving different problems 13 Offenburg UniversityThe PRAM model
•Parallel Random Access Machine
–n processors, connected to a
shared memory
–Each processor can access any location in the
memory in each clock cycle
–Access conflicts can occur. The memory management

of the hardware must regulate this.Shared Memory
P1P2P3 Pn 14 Offenburg University PRAM
•Theoretical model
•There is no hardware that implements it exactly like this
•ignores many specific things
–e.g. cache, …
•still useful
–No separate algorithm needed for every specific architecture
–Can be adapted to various architectures with slight changes 15 Offenburg University of Applied SciencesParallel architectures
•Classification of parallel architectures (according to Flynn, 1966):
•Examples: ?Single instruction Multiple instruction
Single Data SISD MISD*
Multiple Data SIMD MIMD 16 Offenburg University MIMD
•The processors process various programs/ 
algorithms that communicate from time to time
•Suitable for solving problems without a special 
internal structure
–Various tasks
–Exchange/communication at irregular intervals 17 Offenburg UniversitySIMD
•The same algorithm runs on each processor, but with 
different inputs in each case
•Suitable for solving problems with regular 
internal structure
–Similar tasks on a (divisible) set of data
–Communication/synchronization at regular intervals
–There are many such problems 
(e.g. graphics rendering, array count, editing distance, ...) 18 Offenburg UniversitySIMD Properties
•An n-processor SIMD computer has the following 
properties:

–Each processor can store both instructions and data in 
its local memory (register).

–Each processor has an identical copy of the same program in 
its memory.

–In each clock cycle, each processor executes the same instruction 
of this program. However, the data 
(input) of the different processors differs.

–The processors communicate either over a network or 
through a shared memory. 19 Offenburg UniversitySIMD with shared memory
•Classification of access (PRAM):
–Exclusive Read Exclusive Write (EREW )
Each memory location can only be accessed by one processor at a time.
–Concurrent Read Exclusive Write (CREW )
Multiple processors can read from the same memory location at the same time, but only one processor can write at a time.
–Concurrent Read Concurrent Write (CRCW )
Multiple processors can read or write to the same memory location at the same time.
•How is simultaneous writing regulated? 20 Offenburg UniversityCRCW variants
•Common CRCW
–All processors that want to write must write the same value
•Arbitrary CRCW
–One of the processors “wins” (its value is at the end of the memory location). We don’t know which one!
•Priority CRCW
–Processors are assigned priorities; the one with the highest priority writes its valueEREW
CREW 21 Offenburg UniversityParallel algorithms 
•Simple example: Add n numbers in parallel
•Different from before:
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3
Time: O(log n) Processors: n/2 = O(n) 22 Offenburg University Analysis of parallel algorithms
•Time complexity Tp(n)= #time steps with pprocessors
•Processor complexity P(n)= #processors (depending on n)
•Work W(n)= number of primitive operations that are executed on all processors together.
→corresponds to T1(n), i.e. sequential execution
•Span (also: depth, critical path length): number of primitive operations on the longest sequential path
→corresponds to T∞(n), i.e. execution on an idealized machine with an infinite number of processors
•Cost: Cp(n) = p ∙ Tp(n)= cost of execution 23 Offenburg UniversityAnalysis of parallel algorithms
•Our example:
–Tp(n) = 
–P(n) =
–W(n) =
–T∞(n) =
–Cp(n) = 24 Offenburg University Properties
•Work law: p∙Tpg T1
–The cost always corresponds to at least the work . (Because pprocessors 
can only perform poperations in parallel).
•Span law: Tpg T∞
–With a finite number of processors you cannot be faster than 
with an infinite number (but possibly equally fast!) 25 Offenburg University Properties
•Speedup Sp= T1/ Tp
•The work law follows: T1/ Tpf p
→Sp is limited by p
•Sp/ p is also known as efficiency
•Parallelism T1/ T∞ is the highest possible speedup ( S∞) 26 Offenburg University Properties
•Slackness: T1/ pT∞ ≤T1/ pTp≤T1/ T1 =1
span law work law
•If slackness < 1 →Speedup Sp<p
–No perfect parallelizability 27 Offenburg University of Applied SciencesOptimality in parallel algorithms
•For sequential algorithms:
–Determine lower asymptotic bound T(n) for a problem P 
of size n
•Example: T(n) = Ω(nlog n) for sorting any n numbers
–If algorithm A solves the problem for all n in time T(n), A is 
optimal for P
•Example: Merge sort, heapsort, some variants of quicksort
•Consider a parallel algorithm that solves Pof size 
nwith work W(n) in time Tp(n). 28 Offenburg University of Applied SciencesOptimality in parallel algorithms
Let Tseq(n) be the running time of the best sequential algorithm
•A parallel algorithm is called work-optimal if
–W(n) = T1(n) = O(Tseq(n))
•It is also called work-time-optimal if
–Tp(n) cannot be further improved
•A parallel algorithm is called cost-optimal if
–c(n) = p ∙Tp(n) = O(Tseq(n)) 29 Offenburg UniversityOur example 
•Add n numbers in parallel
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3 30 Offenburg University of Applied SciencesKey figures using the example
•Span: T∞(n) = O(log n)
•Work: T1(n) = O(n) ( n-1 additions)
•Parallelism: T1(n) / T∞(n) = O(n) / O(log n) = O(n / log n)
With p= n/2 processors:
•Time: Tp(n) = O(log n)
•Cost: p∙Tp(n) = n/2 ∙ O(log n) = O(nlog n)
•Speedup: Sn/2= T1/ Tn/2= O(n / log n)
•Efficiency: Sp/ p= Sn/2/ (n/2)= O(1 / log n) 31 Offenburg UniversityOur example
• … is not cost-optimal!
–cost(n) = p ∙ Tp(n) = O(n) ∙ O(log n) = O(n log n)
–The sequential algorithm only takes O(n) time.
–Our previous parallel algorithm (array count) 
with a constant number p of processors was cost-optimal:
•T(n) = n/p+ p= O(n) + O(1) = O(n)
•p= O(1)
–It was also cost-optimal with n1/2 processors:
•T(n) = n / n1/2+ n1/2= O(n1/2)
•p= n1/2= O(n1/2)
•Can the new algorithm be made cost-optimal?
–Yes, you can reduce pre. 32 Offenburg University of Applied SciencesReducing the number of processors
•Idea –Step 1: 
–As with Array-Count:
•Give each processor a (larger) part of the array
•Each processor first adds its part sequentially
–But: 
•Make sure that the time per processor remains in O(log n)
log n log n
Total: n/ log nparts → n/ log nprocessors 33 Offenburg UniversityTime per processor
•Each of the n/log n processors first sums up log n
numbers
•After that, n/log n numbers remain
•These can be added with the n / log n processors using the
original algorithm in time
O(log(n/log n)).
•Total time: T(n) = O(log n) + O(log(n/log n)) 
= O(log n) 34 Offenburg UniversityNew Cost
•Before: Now:
–T(n) = O(log n) T(n) = O(log n)
–P(n) = O(n) P(n) = O(n/log n)
–C(n) = O(n log n) C(n) = O( n/log n) ∙ O(log n)
= O(n) 35 Offenburg University of Applied SciencesParallel prefix sum
•Problem:
–Given:
•Array A of n numbers
a0, a1, a2, …, an-1
•Binary associative operator (e.g. +, MAX, MIN)
–Required:
•Array
a0, (a0a1), (a0a1a2), …, ( a0a1…an-1) 36 Offenburg UniversityExample
•let the addition be and the input be 
5, 3, -6, 2, 7, 10, -2, 8
•Then the output is
5, 8, 2, 4, 11, 21, 19, 27
•Sequential computing time: 
O(n) 37 Offenburg UniversityHow do you calculate this in parallel?
•First of all: why at all?
–often required building block for other parallel algorithms
–“parallel primitive” 38 Offenburg University of Applied SciencesParallel prefix sum
•Two passes through the array
–Bottom-up: calculate the total
–Top-down: distribute partial sums
•Representation as a tree:
5 3 -6 2 7 10 -2 8 39 Offenburg UniversityParallel prefix sum
•First pass: bottom-up
–sum[v] = sum[v.left] + sum[v.right]
–Also works “in -place”
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3 40 Hochschule OffenburgPseudocode
ford=0; d<=log(n)-1; d++ do
fori=0; i<=n-1; i+=2d+1in parallel
a[i+ 2d+1-1] = a[ i+ 2d-1] + a[ i+ 2d+1-1]
5 3 -6 2 7 10 -2 8
5 8 -6 -4 7 17 -2 6
5 8 -6 4 7 17 -2 23
5 8 -6 4 7 17 -2 27 41 Offenburg University of Applied SciencesParallel prefix sum
•Second pass
•Idea: 
–Top-down calculation to generate all prefix sums
•Notation:
–pre[v] stands for the sum in each node v
•pre[root] = 0
–0 is the neutral element of +, i.e. x + 0 = x for all x
–For a different operator, the corresponding neutral 
element must be selected (e.g. -∞ for MAX or ∞ for MIN ) 42 Offenburg UniversityParallel prefix sum
•Second round: top-down
–pre[v.right] = sum[v.left] + pre[v]
–pre[v.left] = pre [v]
Input:
• This is “almost” the desired solution!5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 05 3 -6 2 7 10 -2 8
27 43 Offenburg UniversityInclusive vs. exclusive prefix sum
5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0 275 3 -6 2 7 10 -2 8
Inclusive prefix sum Exclusive prefix sum 44 Hochschule OffenburgPseudocode
a[n-1] = 0
ford=log(n)-1; d>=0; d-- do
fori=0; i<=n-1; i+=2d+1in parallel
temp= a[i+ 2d-1] 
a[i+ 2d-1] = a[ i+ 2d+1-1]
a[i+ 2d+1-1] = temp+ a[i+ 2d+1-1]
5 8 -6 4 7 17 -2 23
5 8 -6 0 7 17 -2 4
5 0 -6 8 7 4 -2 21
0 5 8 2 4 11 21 190 45 Offenburg University Complexity
•The algorithm requires 
–Time: O(log n)
–Processors: n/2 = O(n)
•This can be reduced to O(n/log n) Exercise! 46 Offenburg UniversityIs the algorithm correct?

Definition:

Node x is called the predecessor of node y if x is reached in the tree during a

preorder traversion (= depth-first search). 47 Offenburg UniversityCorrectness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that correspond to its predecessors in the 
original tree.
5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 48 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
Proof (by structural induction):
1. The statement holds for the root (with value 0) because it is the 
first visited node and has no predecessors.
2. To show: If the statement holds for any node v 
it also holds for its two children v.left and v.right . 49 Offenburg University Structural induction
•The left child node of v has exactly the same 
predecessor leaves as v itself, namely those in region A.
•And by definition, pre[v.left ] = pre[v]Av 50 Offenburg University Structural induction
•The right child node of v has two sets of 
predecessors
–Region A: corresponds to pre[ v]
–Region B: corresponds to sum[ v.left]
•By definition, pre[v.right ] = pre[v] + sum[v.left ]A Bv 51 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
If it is true for every node, it is especially true for the 
leaves.
This corresponds exactly to what we want to show.5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 3 Offenburg UniversityTheory vs. Practice
• “There’s nothing more practical than a good theory” 
(Kurt Lewin)
•Our previous assumptions:
–Time complexity is given as a function of the 
processor complexity
–We have as many processors “as we need” 
•e.g. O(n/ log n), O(n), …
•But:
–In practice, you usually only have a 
constant number p of processors available 4 Offenburg University of Applied SciencesParallelization with pprocessors
• Let ...
–T1the time for the sequential execution of the algorithm
–Tpthe time for the parallel execution with pprocessors
–T∞the time for the maximum parallel execution
•Then the following applies:
–Tpg T1/p (limited by linear speedup)
» The linear speedup would be Θ(p)
–Tpg T∞ (limited by maximum speedup)
» The maximum possible speedup is T1 /T∞ 5 Offenburg University Brent's Theorem (also: Brent's Law)
An algorithm A can be executed with p processors in parallel in time
㕇㕝≤㕇1
㕝+㕇∞
.
Proof of assignment of jobs to processors 6 Offenburg University of Applied Sciences 7 Offenburg UniversityExample
•Parallel summation (general reduction)
–T1= O(n)
–T∞= O(logn)
–Tpf O(logn)+n/p 8 Offenburg University Conclusion 1
•From the lower bounds Tp≥ T1/p and Tp≥ T∞
and Brent's theorem follows:
max㕇1
㕝, 㕇∞ ≤㕇㕝≤2∙ max㕇1
㕝, 㕇∞
㕐 ≤ 㕇㕝≤2∙ 㕐 9 Offenburg University Conclusion 2
•With the number p= O(T1/ T∞) of processors 
the (asymptotically) optimal time, 
i.e. a maximum speedup can be achieved
•Proof: 㕇㕝=㕇1
㕝+ 㕇 ∞=㕂(㕇 1)
㕂T1
T∞+ 㕇 ∞
= 㕂(㕇1
T1
T∞) + 㕂(㕇 ∞) = 㕂(㕇 ∞) 10 Offenburg UniversityParallel prefix sum
•Problem:
–Given:
•Array A of n numbers
a0, a1, a2, …, an-1
•Binary associative operator (e.g. +, MAX, MIN)
–Required:
•Array
a0, (a0a1), (a0a1a2), …, ( a0a1…an-1) 11 Offenburg UniversityExample
•let the addition be and the input be 
5, 3, -6, 2, 7, 10, -2, 8
•Then the output is
5, 8, 2, 4, 11, 21, 19, 27
•Sequential computing time: 
O(n) 12 Offenburg UniversityHow do you calculate this in parallel?
•First of all: why at all?
–often required building block for other parallel algorithms
–“parallel primitive” 13 Offenburg University of Applied SciencesParallel prefix sum
•Two passes through the array
–Bottom-up: calculate the total
–Top-down: distribute partial sums
•Representation as a tree:
5 3 -6 2 7 10 -2 8 14 Offenburg UniversityParallel prefix sum
•First pass: bottom-up
–sum[v] = sum[v.left] + sum[v.right]
–Also works “in -place”
5 3 -6 2 7 10 -2 88 -4 17 64 2327
P1P1P1
P2 P3 P4P3 15 Hochschule OffenburgPseudocode
ford=0; d<=log(n)-1; d++ do
fori=0; i<=n-1; i+=2d+1in parallel
a[i+ 2d+1-1] = a[ i+ 2d-1] + a[ i+ 2d+1-1]
5 3 -6 2 7 10 -2 8
5 8 -6 -4 7 17 -2 6
5 8 -6 4 7 17 -2 23
5 8 -6 4 7 17 -2 27 16 Offenburg University of Applied SciencesParallel prefix sum
•Second pass
•Idea: 
–Top-down calculation to generate all prefix sums
•Notation:
–pre[v] stands for the sum in each node v
•pre[root] = 0
–0 is the neutral element of +, i.e. x + 0 = x for all x
–For a different operator, the corresponding neutral 
element must be selected (e.g. -∞ for MAX or ∞ for MIN ) 17 Offenburg UniversityParallel prefix sum
•Second round: top-down
–pre[v.right] = sum[v.left] + pre[v]
–pre[v.left] = pre [v]
Input:
• This is “almost” the desired solution!5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 05 3 -6 2 7 10 -2 8
27 18 Offenburg UniversityInclusive vs. exclusive prefix sum
5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0 275 3 -6 2 7 10 -2 8
Inclusive prefix sum Exclusive prefix sum 19 Hochschule OffenburgPseudocode
a[n-1] = 0
ford=log(n)-1; d>=0; d-- do
fori=0; i<=n-1; i+=2d+1in parallel
temp= a[i+ 2d-1] 
a[i+ 2d-1] = a[ i+ 2d+1-1]
a[i+ 2d+1-1] = temp+ a[i+ 2d+1-1]
5 8 -6 4 7 17 -2 23
5 8 -6 0 7 17 -2 4
5 0 -6 8 7 4 -2 21
0 5 8 2 4 11 21 190 20 Offenburg University Complexity
•The algorithm requires 
–Time: O(log n)
–Processors: n/2 = O(n)
•This can be reduced to O(n/log n) Exercise! 21 Offenburg UniversityIs the algorithm correct?

Definition:

Node x is called the predecessor of node y if x is reached in the tree during a

preorder traversion (= depth-first search). 22 Offenburg UniversityCorrectness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that correspond to its predecessors in the 
original tree.
5 3 -6 2 7 10 -2 88 -4 17 64 23270
0 4
0 8 4 21
19 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 23 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
Proof (by structural induction):
1. The statement holds for the root (with value 0) because it is the 
first visited node and has no predecessors.
2. To show: If the statement holds for any node v 
it also holds for its two children v.left and v.right . 24 Offenburg University Structural induction
•The left child node of v has exactly the same 
predecessor leaves as v itself, namely those in region A.
•And by definition, pre[v.left ] = pre[v]Av 25 Offenburg University Structural induction
•The right child node of v has two sets of 
predecessors
–Region A: corresponds to pre[ v]
–Region B: corresponds to sum[ v.left]
•By definition, pre[v.right ] = pre[v] + sum[v.left ]A Bv 26 Offenburg University Correctness
Lemma:
After the second pass (top-down), each 
node contains the sum of the values ​​of all leaf nodes that are its predecessors in the 
original tree.
If it is true for every node, it is especially true for the 
leaves.
This corresponds exactly to what we want to show.5 3 -6 2 7 10 -2 819 21 11 4 2 8 5 0
5 3 -6 2 7 10 -2 8 Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University Topics
•Thread pools 3 Offenburg University of Applied SciencesParallelization: Tasks and threads
•Task, completed work unit
•To be completed (largely) independently of other tasks
•Examples
–Processing a request, e.g. in a web server
–Calculation, e.g. image processing
•Task structure of an application
–Identify tasks
–Break down tasks:
•To be processed sequentially
•To be processed in parallel 4 Offenburg UniversityTasks and threads
Task
•Task thread
• “Completer” of tasks
Assignment can be done in different ways:
•Sequential: one thread for all tasks
•Thread per task: a separate thread for each task
•Thread pool: a set of threads completes a set of
tasks
•Note: Application is at most as fast as its

longest-running thread (cf. span = “critical path length”) 5 Offenburg University Sequential assignment: 1 thread 
•Advantages
–Easy to implement
–No synchronization required in the code
–No overhead for thread management
•Disadvantages
–Poor throughput (e.g. with web servers)
–Tasks may have to wait for a long time unnecessarily
–Parts of the hardware may be unused unnecessarily 6 Offenburg University of Applied SciencesOwn thread per task
•Advantages:
–Improved throughput
–Hardware resources are used
•Disadvantages:
–Code must be thread-safe
–Overhead, especially with many tasks:
•Permanent creation/destruction of threads
•Resource consumption (e.g. stack area)
•See example merge sort, edit distance, heapify
–Possible stability problems 7 Offenburg University Compromise: Thread pool
•Idea: use a limited number of threads that is not exceeded
•Use the advantages of both approaches, minimize disadvantages
–uses resources (e.g. multi-core CPUs)
–conserves resources (e.g. memory for virtual address space)
•Disadvantage
–work to manage the pool
–code must be thread-safe
•The executor framework in java.util.concurrent
provides interfaces and classes for thread pools 8 Offenburg University Executor framework
•Interface that describes the execution of tasks
•An executor (i.e. a class that implements the interface) executes tasks using threads
–Can assign in any way (thread policy), e.g.
•Single-threaded
•Thread per task
•Thread pool
–Creation and reuse of threads is left to the executor
•Advantage: decoupling of the application from the thread policy
–Simple modification of the thread policy possible 
–Application does not have to worry about thread policy at all 9 Offenburg University of Applied SciencesExecutor framework as 
intermediate layer
•simple standardized means 
for creating, managing 
and scheduling threads
•supports the decoupling
–of tasks 
(application-specific)
from
–the configuration of the execution 
(depending on the 
runtime environment)
•preferable for software development over the 
direct use of threads
Task
Task
Task
Task
Task
Task
Executor framework
Java Virtual Machine
Operating System
Task
CPU CPUThreads 10 Offenburg UniversityExecutor
•Executor:
–public void execute(Runnable r)
•Example 1:
–Execution of each task in its own thread:
classThreadPerTaskExecutor implements Executor { 
public void execute(Runnable r) { 
newThread(r).start();
}
}
•Tasks must be programmed in a thread-safe manner! 11 Offenburg UniversityExecutor
•Example 2:
–Execution of the task in the calling thread:
classDirectExecutor implements Executor { 
public void execute(Runnable r) { 
r.run(); 
} 
}
•What about the thread safety of tasks here? 12 Offenburg UniversityExecutor
•Example 3:
–Serial execution of all tasks guaranteed in the same thread:
public class OneThreadExecutor extends Threadimplements Executor {
private final Queue<Runnable> tasks = new 
LinkedBlockingQueue<Runnable>();
public void run() {
while(true) // or condition for termination
try {
tasks.take().run();
} catch(InterruptedException e) { ... } 
}
public synchronized void execute(Runnable r) {
tasks.offer(r);
}
} 13 Hochschule OffenburgExecutor Framework
•Comes with most important types of Executor
–ThreadPoolExecutor
–ScheduledThreadPoolExecutor für 
•delayed or
•periodic execution of tasks
•Even better:  ExecutorService extends Executor
–Additional methods for managing, e.g.
•shutdown()
•isTerminated()
•submit()
•... 14 Hochschule OffenburgThreadPoolExecutor
•Components
–Queue Qfor incoming tasks
–Threads: take tasks from Qand execute them
•Number tof threads in Pool
–Core pool size : desired number of threads, only exceeded if
necessary
–Maximum pool size : maximum number of threads, will never be
exceeded 15 Hochschule OffenburgHow it works
•New task comes in:
–Ift< core pool size :
•Create a new thread for the task
–Ift≥ core pool size und t< maximum pool size
•Put task on Q
–Ift> core pool size und t< maximum pool size und Q voll
•Create new thread
–Ift≥ maximum pool size und Q voll
•?  (find out!)
•Ift> core pool size andQis empty
•Terminate next idle thread 16 Hochschule OffenburgTasks with result
•So far: tasks for Executor must be Runnable s
–No (explicit) result –public void run()
–Any result must be fetched manually (e.g. with get method)
•For tasks with result: Callable<V>
–Interface: publicVcall()
–Like Runnable, but returns result of type V
–Can be executed by an Executor
–Call withexecutor.submit(task) 
•How to retrieve the result? 17 Hochschule OffenburgResults
•Problem: 
–Result has not been calculated yet at the time of task submission
–Return value of executor.submit(task) must represent a 
future object of Type V
•Solution:Future<V>
–Interface: publicVget()
–Represents the result (of Type V) of the execution
•of a  Callable<V>
•by an Executor
–Call:future = executor.submit(task) 18 Hochschule OffenburgCallable and Future
•Application
–Submits a Callable<V> o anExecutorService
–Receives a Future<V> immediately
–Result can be retrieved from Future
•Executor
–Receives a Callable<V>
–Returns a Future<V> 
–Computes the task with the help of threads
–Hands the result to Future
•How to know when the result is there? 19 Hochschule OffenburgMethods of Future
–get()
•Blocks the calling thread until result is there
–get(long timeout, TimeUnit unit)
•Waits for result or until time is up (whatever comes first)
–isDone()
•true if and only if task has ended (no matter how!)
–isCancelled()
•true if and only if task has been cancelled
–boolean cancel(boolean b)
•Tries to cancel the corresponding task
(with/without interruption)
•true if and only if successful 21 Hochschule Offenburgjava.util.concurrent.atomic
•Atomic data types
–AtomicBoolean
–AtomicInteger undAtomicIntegerArray
–AtomicLong undAtomicLongArray
–AtomicReference<V> undAtomicReferenceArray<E>
– …
•Atomic methods for above types
–compareAndSet()
–addAndGet()
–addAndGet() und getAndAdd()
–incrementAndGet() undgetAndIncrement() 
– … 23 Offenburg Universityjava.util.concurrent.locks
•Interfaces:
–Lock
•tryLock (…)
•lock()
•unlock()
–ReadWriteLock
–Condition
•Classes:
–ReentrantLock
–ReentrantReadWriteLock 24 Hochschule OffenburgSynchronization classes
•CountDownLatch
–Barrier with countdown which must be counted down explicitly
(usingcountDown() )  Not reusable.
•CyclicBarrier
–Barrier for a number of threads, reusable.
•Exchanger
–Barriere for two threads to exchange data
•Semaphore
–Implementation of a typical counting semaphore with key
methods acquire (…) andrelease (…) Parallel programming and 
algorithms
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University Topics
•Thread pools
–Disadvantages of standard thread pools
–Fork/Join framework
•Parallel algorithms
–Parallel maximum determination 3 Offenburg University Thread pools
•Thread pools of the executor framework are not ideal for
–recursive tasks
•Potentially high number of threads
•Risk of deadlocks (see merge sort) if limited
–highly dependent tasks
•Either all or no tasks
•Risk of deadlocks if pool is too small
•One reason: queuing all tasks in one queue
–order is often crucial! 4 Offenburg University Alternative: Fork/Join Framework
•Suitable for algorithms that
–consist of a large number of sub-actions 
•that can be processed independently of one another (per “level”)

–contain as little synchronization code or blocking 
actions as possible (except for task-subtask dependencies)
•Typical applications
–Divide-and-Conquer algorithms
–Parallel algorithms on arrays 
•Worthwhile on very large arrays if you have a lot of 
processors/cores available 5 Offenburg UniversityFork/Join: Task constructs
•RecursiveAction
–Class for recursive action (without return of result)
–Method: voidcompute()
–Calls (independent) sub-actions of the same type
•Instances of the same class are used for this
–Start of the sub-actions (scheduling in the pool) with
voidinvokeAll(... sub-actions... )
•Method waits for return
–Alternative: asynchronous start of a single sub-action
sub-action .fork() 6 Hochschule OffenburgBeispiel
•Array-Increment: Erhöhe jedes Element eines Arrays um 1 
classIncrementAction extends RecursiveAction { 
final long [] array; 
final int lo, hi; 
IncrementAction (long[] array, intlo, inthi) { 
this.array = array; 
this.lo = lo; this.hi = hi; 
} 
protected void compute() { 
if(hi - lo < THRESHOLD ) { // eine sinnvoll gewählte Grenze
for(inti = lo; i < hi; ++i) array[i]++; 
} else{ 
intmid = (lo + hi) >>> 1; 
invokeAll( newIncrementAction (array, lo, mid), 
newIncrementAction (array, mid, hi)); 
} 
} 
} 7 Offenburg UniversityFork/Join: Task constructs
•RecursiveTask< V> 
–Class for recursive action with result return of type V
–Method: Vcompute()
–Calls (independent) sub-tasks of the same type
•Instances of the same class are used for this
–Start the sub-tasks (scheduling in the pool) with
voidinvokeAll(... Sub-Tasks... )
•Method waits for result return
–Alternative: asynchronous start of a single sub-action
subtask.fork()
–Fetch the results
Vjoin() 8 High School Offenburg Theater
classFibonacci extends RecursiveTask<Integer> { . 
final int n; 
Fibonacci(intn) { 
this.n = n; 
} } 
Integer compute() { 
if ( n <= 1 ) return n ; 
Fibonacci f1 = newFibonacci(n-1);
f1.fork(); 
Fibonacci f2 = newFibonacci(n-2); 
return f2 . compute ( ) + f1 . join ( ) ; 
} } 
} } 9 Offenburg University of Applied SciencesHow does the ForkJoinPool work?
• A ForkJoinPool looks “from the outside” like any other
thread pool:
–There is a task queue in which new tasks are submitted by applications…
–… and threads that pick up and execute these taskssubmit
submit
taketake 10 Offenburg University of Applied SciencesThread-specific task queues
•Special feature:
Each thread also has its own task queue
–Sub-tasks of the currently executed task are inserted here…
–… and also executed primarily by the thread!submit
submit
taketakepush
pushpop
pop 11 Offenburg University of Applied SciencesThread-specific queues
•Advantages
–A started (overall) task is completed before the thread starts a completely new task
–The central queue is not burdened with many small sub-tasks, 
but only contains (relatively) independent tasks
–This means fewer conflicts (blocking) between threads when accessing 
the queues (both central and thread-specific)
•Access is implemented like a stack (LIFO)
–The last task inserted is processed first
–Subtasks are processed before the parent tasks
–Avoidance of deadlocks 12 Offenburg University of Applied SciencesThread-specific queues
•BUT:
–A started (overall) task is completed before the thread starts a completely new task
•BUT: What do threads do that are already finished while others are still busy with sub-tasks?

–Central queue is not burdened with many small sub-tasks, but only contains (relatively) independent tasks
•BUT: What if there is only 1 large task that is split up (e.g. merge sort)???

–This means fewer conflicts (blocking) between threads when accessing the queues (both central and thread-specific)
•BUT: Danger of unemployed threads because central and own queues are empty! 13 Offenburg University of Applied SciencesUnemployed threads
• If a thread's own queue is empty...
–First look in the task queues of other threads...
–... and take work away from them if possible (work stealing).submit
submit
taketakepush
pushpop
popsteal 14 Offenburg University Work stealing
•Advantages:
–A started (entire) task is only completed before a completely new task is started – also by other threads
–Threads also complete fine-grained tasks using a division of labor
•Possible disadvantage:
–Access conflicts with the “owner” thread of the queue
–Fine-grained tasks are prioritized (LIFO), so many accesses are likely
•This can be prevented: 
–by implementing the thread’s own queue as a deque. 15 Offenburg UniversityDeque (double ended queue)
•Allows operations at the beginning (first) and at the end (last)
–Owner thread only works at the beginning of the queue
•push: addFirst(…)
•pop: removeFirst()
–Foreign threads only remove at the end 
•steal: removeLast()
push
pushpop
popsteal: removeLast() 16 Offenburg University Conclusion: Fork/Join
•Not necessarily more efficient than ThreadPoolExecuter!
–Additional overhead for managing queues and work-
stealing
–If the input tasks are very balanced, work-stealing often does not take place. 
–If this is known in advance, you can be more efficient with a “normal” 
thread pool or manual threading
•Work-stealing ensures finer granular load distribution
–Well suited if a large task would use up a thread while other threads have to wait
–e.g. with unevenly distributed input that can be broken down further
–Fork/Join is preferable for recursively formulated tasks 17 Offenburg University Parallel maximum
•Problem:
Find the largest number in an array A of n numbers
or the index (position) of the largest number.
•Sequential solution: O(n)
•Claim: We already know how to determine the maximum
of n numbers in parallel in time O(log n)
–MAX is like + a binary associative operator
–Therefore the same algorithm as for the simple sum can be used, only with MAX instead of + 18 Offenburg UniversityParallel maximum
•Time T(n) = O(log n)
•Can it be done faster?5 3 -6 2 7 10 -2 85 2 10 85 1010
P1P1P1
P2 P3 P4P3 19 Offenburg UniversityParallel maximum
•Claim:
You can even determine MAX in time O(1)!
•However: 
We need a lot of processors (and extra space) for this 20 Offenburg UniversityParallel maximum
A:
B:
•Start: 
–Create an array B of length n
–Initialize it with B[k] = 1 for all k
–This takes O(1), and we “only” need n processors5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1 21 Offenburg University of Applied SciencesParallel maximum
•Step 1
–Use n processors for each element A[ i] 
–We consider an element A[ i] and its n processors 
Pi,0, Pi,1, …, Pi,j, …, Pi,n-1
–Each processor Pi,j compares A[ i] with A[ j]:
if(A[i] < A[j])
B[i] = 0
else
// do nothing 22 Offenburg UniversityExample: P4,0
i= 4
5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1
Do this in parallel for all j. j = 0 23 Offenburg University of Applied SciencesExample: P4,jfor all j
i= 4
5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1 0
Do this in parallel for all i. 24 Offenburg UniversityExample: all Pi,j
5 3 -6 2 7 10 -2 8
1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 25 Offenburg University of Applied SciencesParallel maximum
•Step 2:
–Observation: 
At the end of step 1, B[k] = 1 if and only if A[k] 
is the maximum (or a maximum) element
–Use n processors, one for each entry in B
ifB[i] == 1
result_index = i (or result_max = A[i])
(depending on what is expected as the answer)
•Observation: In both steps, simultaneous 
writing (concurrent write) may be required 26 Offenburg University of Applied SciencesSimultaneous writing
•Can there be writing conflicts?
–In step 1:
If writing is done simultaneously, all processors write 
the same thing (“false” race condition)
→possible from Common CRCW
–In step 2:
Since the index (position) of the maximum is written, 
different threads could write different things
→possible only from Arbitrary CRCW (real race condition)
–However, if the value of the maximum is written directly, 
this step also goes with Common CRCW 27 Offenburg University Complexity
•Time: T(n) = O(1)
–It doesn’t get any better than this! ☺
•Processors: p(n) = O(n²)
–Not good at all! 
•Costs: c(n) = O(n²)
–Not cost-optimal  28 Offenburg University Can it be done better?
•Goals:
–More realistic number of processors (less than n)
–Still runtime better than O(logn)
–Cost optimality
•Approach:
–Combine the two previous algorithms
1. T(n) = O(log n) c(n) = O(n)
2. T(n) = O(1) c(n) = O(n²)
–Goal
•T(n) = O(log log n) c(n) = O(n) 29 Offenburg UniversityAlgorithm with time O(log log n)
5 3 -6 2 71
0-2 8 41
56 -5 9 11
1-3 30 Offenburg UniversityAlgorithm with time O(log log n)
•Example: k= 2, n= 222= 16
•Depth of the tree: k + 1
•Since n=22k, the following applies: k= O(log log n)
•The number of nodes on level i is 22k-2k-ifor 0 ≤ i≤ k5 3 -6 2 71
0-2 8 41
56 -5 9 11
1-3 31 Offenburg UniversityAlgorithm
•Proceed level by level
–Start at the nodes directly above the leaves
–In each level: 
•Calculate the maximum of all child nodes for each node in 
constant time using the O(1) algorithm
–Use this to calculate the entire level in parallel in constant time 
•The only important thing is: How many processors do you need for this?
•Then the total time is proportional to the number of 
levels (depth of the tree), i.e. in O(log log n) 32 Offenburg UniversityCosts per level
•Every node on level ihas c= 22k-i-1children
•The processor effort per node is O(c²) = O((22k-i-1)²)
•On level ithere are a total of 22k-2k-inodes that we want to calculate in parallel (→time O(1) per level)
•This means that the total costs Ci(n)for level i are
O((22k-i-1)²) ∙ 22k-2k-i= O(22k) = O(n)
•Observation: The costs Ci(n) are independent of i,

so the same for every level! 33 Hochschule OffenburgNebenrechnung
(22k-i-1)2· 22k–2k-i
= 22·2k-i-1· 22k–2k-i
= 22k-i-1+1· 22k–2k-i
= 22k-i· 22k–2k-i
= 22k-i+ 2k–2k-i
= 22k
=n 34 Offenburg University Number of processors
•Start from the top
–Level 0 (root)
•1 node with n1/2 children →1 · (n1/2))2= n processors
–Level 1
•n1/2 nodes with n1/4 children each →n1/2· (n1/4))2= n processors
–Level 2
•n3/4 nodes with n1/8 children each →n3/4· (n1/8))2= n processors
–Level 3
•n7/8 nodes with n1/16 children each →n7/8· (n1/16))2= n processors
–…
–Level above the leaves:
•n/2 nodes with 2 children →works with n/2 processors 35 Offenburg UniversityExample 1
•n= 16 (k= 2)
•Start from the bottom
–Level above the leaves:
•8 nodes/processors for a maximum of 2 leaves each
–Level above:
•4 nodes with 2 children each → 16 processors
(4 processors)
–Level above:
•1 node with 4 children → 16 processors 36 Offenburg UniversityExample 2
•n= 256 (k= 3)
•Start from the bottom
–Level above the leaves:
•128 nodes with 2 children (leaves) → 128 processors
–Level above:
•64 nodes with 2 children → 256 processors
(or: 64 processors)
–Level above:
•16 nodes with 4 children → 256 processors
–Level above
•1 node with 16 children → 256 processors 37 Offenburg University Total costs
•For O(log log n) levels with cost O(n) each, the
total effort (costs)
c(n) = O(nlog log n) →not cost-optimal
•Improvement by reducing the input
(algorithmic cascading)
–We already know this from sum, parallel prefix etc.
–First divide the input into suitably large parts
–Calculate the maximum for each part sequentially

(but ​​all parts in parallel)
–Use the algorithm just presented on the result 38 Offenburg UniversityPhase 1: Input reduction
Reminder: Brent's theorem: 㕇㕝≤㕇1
㕝+㕇∞
Conclusion: With the number p= O(T1/ T∞) of processors, the 
(asymptotically) optimal time, i.e. a maximum speedup, can be achieved! 39 Offenburg UniversityPhase 2 40 Offenburg University Total effort
•Phase 2:
–Cost O(n)
–Time O(log log n)
•Phase 1:
–Cost O(n)
–Time O(log log n)
Conclusion (theorem):
The maximum of n elements can be calculated in time O(log log n) 
and cost O(n) on a CRCW PRAM.
(It can be shown that this is also the optimum.) 41 Offenburg University of Applied SciencesParallel algorithms from Java 1.8
•Parallel methods for arrays:
–Sorting, setting values, etc.
–implemented in java.util.Arrays
•Java 1.8 has a default fork/join pool that the above

parallel methods use
–Access: ForkJoinPool.commonPool()
•Streams: flexible calculations on large data sets
–Supports parallelization: ParallelStream class
–Caution: not everything there is actually calculated in parallel! GPU 
Programming 
Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous 
Offenburg University 
Source: Nvidia Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Motivation 
2 Mathematics 
Numerics 
Computational Finance 
Simulations (Monte Carlo) 
Risk analysis 
Derivative valuation 
Signal processing 
Video analysis and processing 
Sensor data 
Deep learning 
Training neural networks Physics 
Particle simulations 
Biology, bioinformatics 
Sequence alignment 
Protein folding 
Cryptology 
Password security 
Meteorology 
Weather and climate 
simulations 
Mining 
Bitcoin etc. 
and much more. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPGPU Motivation 
3 General Purpose Computation on Graphics Processing Unit 

GPUs offer very high computing power per euro 
(also due to lower energy consumption per FLOP/s). 

GPUs can execute MAC (multiply-accumulate) operations in one cycle (number cruncher). 

Sophisticated SIMD hardware for data-parallel applications 

GPUs are accessible and built into many workstations and laptops. Every specialist retailer has them in their range. 

Many computationally intensive applications can be significantly 
accelerated with GPUs. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Review 
4 Shader programming with OpenGL 1992, Glide 1996 or other 
shader programming languages ​​or specifications are cumbersome. 

In 2007, CUDA (Compute Unified Device Architecture) was released by Nvidia for 
exclusively Nvidia GPUs and makes programming much easier. 

In 2009, OpenCL (Khronos Group) was released as a standard, so 
it is possible to program on non-Nvidia graphics cards. 

In 2011, OpenACC (PGI, Cray, Nvidia, CAPS) was released so that 
CPU and GPU programming is portable and can be used in heterogeneous environments 
(various manufacturers: CPU, CPU-GPU, GPU). Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPUs Today 
5 •“General-Purpose Computation on Graphics 
Processing Units< 
•2009 
•First GPU Technology Conference from NVIDIA 
•2010 
•Fastest supercomputer “Tianhe -1A< 
with 7,168 GPUs (NVIDIA Tesla M2050) 
•2012 
•Fastest supercomputer “Titan< 
with 18,688 GPUs (NVIDIA Tesla K20X) 
•2014 
•GPUs in 17 of the top 100 
supercomputers 
•2019 
•Summit supercomputer with 27,648 GPUs and 9,216 CPUs Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPGPU and algorithms 
6 First opportunity to implement massively (data-)parallel 
algorithms on inexpensive hardware 
Theory of parallel algorithms goes back 
to the 1980s and beyond 
At that time only a few special computers 
Scalability of the number of processors with the data size 
in principle possible 
e.g. through multi-GPU 

Relatively simple implementation 
Primarily through CUDA and OpenCL Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPU structure 
7 Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University GPU structure 
8 Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Hardware: Überblick (GTX 280) 
9 Double Precision Special Function Unit (SFU) 
TP Array Shared Memory Streaming Processor (SP) 
Thread Processor (TP) 
FP / Integer Multi-banked  
Register File 
Special 
Ops Streaming Multiprocessor (SM) 
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared Memory
Double PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryDouble PrecisionSpecial Function Unit (SFU)
TP Array Shared MemoryMain 
Memory 
Thread Manager 
30 SMs pro chip Core Processor 
Chip Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Thread processor (CUDA Core) 
10 •Floating point / Integer 
Unit 

•Move, compare, logic, 
branch 

•Local registers 

•"Stripped down< 
processor core or 
highly developed ALU FP / Integer Multi-banked 
Register File 
Special 
Ops Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor 
11 •8 / 32 / 192 / 128 / 64 thread processors (CUDA cores) 
•Double-precision unit 
(IEEE-754 compliant) 
•16 KB / 64 KB shared memory 
•Shared instruction unit and instruction cache 
•Simultaneous management of a large number of threads 
•Hundreds of threads simultaneously 
•Hardware scheduler with context switching with almost no overhead 
•NVIDIA GTX 280: 30 multiprocessors (8 CUDA cores each) 
•NVIDIA Tesla P100: 56 multiprocessors (64 CUDA cores each) Double Precision Special Function Unit (SFU) 
TP Array Shared Memory Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor – Maxwell generation 
12 •Up to 16 multiprocessors per GPU 
(e.g. GeForce GTX 980) 
•Per multiprocessor: 
•128 thread processors (CUDA cores) 
•64-96 KB shared memory 
•64 KB register file 
•Up to 64 active warps (32 threads each) 
 32768 active threads Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor – Pascal generation 
13 •Up to 60 multiprocessors per GPU 
(e.g. GeForce GTX 1080) 
•Per multiprocessor: 
•64 thread processors (CUDA cores) 
•64 KB shared memory 
•64 KB register file 
•Up to 64 active warps (32 threads each), 
 122880 active threads Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Multiprocessor – Turing generation 
14 •Up to 72 multiprocessors per GPU 
(e.g. Quadro RTX 6000) 
•Per multiprocessor: 
•64 thread processors (CUDA cores) 
•64 KB shared memory 
•64 KB register file Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Processor array 
15 •Scalable by design 
•Multiprocessors form a scalable processor array 
•Varying number of multiprocessors depending on GPU 
(automatic scaling execution) 

•Architecture has an impact on programming 
•Each multiprocessor is a SIMD-like 
unit Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University SIMD – Single instruction, multiple data 
16 •All processors execute the same instruction in each step 
but on different data 

•see MIMD (multiple instruction, multiple data) 
•Every processor can execute any program 
•Standard multicore architecture (dual core, quad core, ...) 

•If everyone always executes the same instruction, how can 
one then implement branches (if ... else)? Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Conditionals/branches in SIMD 
17 Elimination of branching 
•Parallel evaluation of the 
predicate for all elements 
•Parallel calculation of the 
TRUE branch for all 
elements 
•Parallel calculation of the 
FALSE branch for all 
elements 
•Parallel selection of the 
correct value for all 
elements (how?) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University of Applied Sciences Storage system 
18 Up to 24 GB memory supported 
(e.g. Turing Quadro P6000) 

Generic load/store model 
(Concurrent Read, Concurrent Write) 
ATTENTION: arbitrary CRCW 
Every processor can write to any memory location 
Conflicts (race condition) possible 
  Programmer is responsible! Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory types 
19 Global memory 
Write/Read 
Large: hundreds of MB to 32 GB/GPU 
Slow (600+ cycles) 

Texture memory 
Physically the same as global memory 
Read-only 
Cached for streaming (2D neighborhoods) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory types 
20 Constant memory 
Read-only 
64kB per chip 
Very fast (1-4 cycles) 

Shared memory (partly shared with L1 cache) 
Read/write 
16kB - 64kB per multiprocessor 
Very fast, provided DRAM bank conflicts are avoided 

Register 
Read/write 
16kB-64kB per multiprocessor; max. 255 registers per thread 
Fastest memory Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory types 
21 The performance of applications depends crucially on 
the efficient use of the memory hierarchy! 

For example 
Shared memory as a cache for global memory 
Constant memory for transferring larger parameter sets 
Efficient use of registers 
… Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Programming model: CUDA 
(Compute-Unified Device Architecture) 
22 Scaling to 
Hundreds to thousands of cores 
Tens of thousands to millions of threads 

Programmer should be able to concentrate on parallel algorithms 
No special parallel programming language 
C for CUDA plus runtime API 

Support for heterogeneous systems (i.e. CPU+GPU) 
CPU & GPU are separate devices with separate DRAMs 
Today transparent (unified address space, unified memory) 
CPU = <Host=, GPU = <Device= Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Programmiermodell: CUDA 
23 NVCC C/C++ CUDA 
Application 
PTX to Target 
Translator 
   GPU    …     GPU  
Target device code PTX Code Generic 
Specialized CPU Code Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Parallel abstractions in CUDA 
24 
Hierarchy of concurrent threads 

Lightweight synchronization primitives 

Shared memory model for cooperating 
threads Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Hierarchy of concurrent threads 
25 Parallel kernels with many threads 
All threads execute the same sequential 
program 

Threads are grouped in thread blocks 
Blocks are arranged 1-, 2- or 3-dimensionally 
Threads in the same block can cooperate 

Threads/blocks have unique IDs 

Thread blocks are grouped in a grid 
Grid is arranged 1-, 2- or 3-dimensionally Thread t 
t0 t1 … tB Block b Prof. Dr. rer. nat. Tobias Lauer Dr.-Ing. Alexander Vondrous Offenburg University Grid structure 
26 Thread blocks are arranged as a 
1- or 2-dimensional 
grid 

This makes it easy to adapt the grid structure to 
the problem structure 

Each thread is defined by 
its position in the block and 
the position of the Blocks in the 
Grid clearly 
identifiable t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB t0 t1 … tB Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University CUDA programs 
27 GPU is coprocessor 
Host code and device code 
Application-controlled calls to parallel functions 
No "strict< SIMD, but 
SPMD – Single Program, Multiple Data 
Between different warps 
(Warp = thread bundle that is scheduled as a unit) 
The same program (but not necessarily the same instruction) is 
executed simultaneously on all data 
SIMT – Single Instruction, Multiple Thread 
Within a warp 
Each thread has its own registers Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Typical CUDA program structure 
28 
Application is split into sequential 
and parallel computable 
parts/tasks 

For each parallelizable part 
Memory is allocated to GPU(s) 
Required data is copied from RAM to GPU memory 
CUDA kernel(s) started 
Result(s) copied back to host 
Memory released Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Beispiel: Vektoraddition 
29 // Compute vector sum C = A+B 
// Each thread performs one pair-wise addition 
__global__  void vecAdd(float* A, float* B, float* C) 
{ 
    int i = threadIdx.x  + blockDim.x  * blockIdx.x ; 
    C[i] = A[i] + B[i]; 
} 
 
int main() 
{ 
    // Run N/256 blocks of 256 threads each 
    vecAdd<<< N/256, 256>>> (d_A, d_B, d_C); 
} Device Code Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Beispiel: Vektoraddition 
30 // Compute vector sum C = A+B 
// Each thread performs one pair-wise addition 
__global__  void vecAdd(float* A, float* B, float* C) 
{ 
    int i = threadIdx.x  + blockDim.x  * blockIdx.x ; 
    C[i] = A[i] + B[i]; 
} 
 
int main() 
{ 
    // Run N/256 blocks of 256 threads each 
    vecAdd<<< N/256, 256>>> (d_A, d_B, d_C); 
} Host Code Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Synchronization 
31 Threads within a block can be synchronized with barriers 
 … Step 1 … 
__syncthreads(); 
… Step 2 … 

Coordination using atomic memory access operations 
e.g. incrementing a shared pointer with atomicInc() 

Implicit barrier between dependent kernels 
 vec_minus<<<nblocks, blksize>>>(a, b, c); 
vec_dot<<<nblocks, blksize>>>(c, c); 

ATTENTION: Differences between architectures (compute capability)! Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Compute Capabilities 
32 CC1.1 atomic int32 operations in global memory 
CC1.2 atomic int32 operations in shared memory 
atomic int64 operations in global memory 
CC1.3 double-precision floating-point arithmetic 
CC2.x atomic int64 operations in shared memory 
atomicAdd on 32-bit floats 
new synchronization mechanisms 
CC3.5 Dynamic parallelism 
shuffle instruction (threads exchange data) 
… 
CC7.5 Tensor Cores for training DNN Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University What is a thread? 
33 Independent execution thread 
Has its own program counter, variables (registers), processor 
status, etc. 
No influence on thread scheduling 

CUDA threads can be physical threads 
"regular< on NVIDIA GPUs 

CUDA threads can be virtual threads 
e.g. 1 block = 1 physical thread on multicore CPUs 
(as in MCUDA) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University What is a thread block? 
34 Thread block = virtualized multiprocessor 
Freely selectable <processor size = for data volume 
Can be adjusted for each kernel start 

Thread block = a (data) parallel task 
All blocks in a kernel have the same entry point 
But can execute any code 

Thread blocks of a kernel must be independent tasks 
Block executions can be "interleaved" as desired Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Blocks must be independent 
35 Arbitrary interleaving of blocks should lead to correct 
results 
Run without a predetermined order 
Can be executed concurrently OR sequentially 

Blocks can coordinate but not synchronize 
Shared queue pointer: OK 
Shared lock: BAD ... risk of deadlocks 

Independence requirement guarantees scalability 
And makes hardware implementation manageable Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Levels of parallelism 
36 Thread parallelism 
Each thread is an independent thread of execution 

Data parallelism 
Between threads in a block 
Between blocks in a kernel 

Task parallelism 
Different blocks are independent 
Kernels are independent Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
37 thread 
Per-thread 
Local memory block 
Per-block 
Shared 
Memory Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
38 Kernel 0 
. . . 
Per-device 
Global 
Memory 
. . . Kernel 1 Sequential 
Kernels Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
39 Device 0 
memory 
Device 1 
memory Host memory cudaMemcpy() Prof. Dr. rer. nat. Tobias Lauer Dr.-Ing. Alexander Vondrous Offenburg University Memory model 
40 Every thread can 
read/write its registers 
read/write its local memory 
read/write in the shared memory of its block 
read/write global memory 
only read constant memory 
only read texture memory 
The host ( CPU) can 
Read/write global memory 
Read/write constant memory 
Read/write texture memory 
Grid 
Constant 
Memory 
Texture 
Memory Global 
Memory Block (0, 0) 
Shared Memory 
Local 
Memory Thread (0, 0) Registers 
Local 
Memory Thread (1 , 0) Registers Block (1, 0) 
Shared Memory 
Local 
Memory Thread (0, 0) Registers 
Local 
Memory Thread (1, 0) Registers Host Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Warps and half-warps 
41 thread 
Block multiprocessor 32 threads 
32 threads 
32 threads ... 
Warps 
16 
Half warps 16 DRAM 

Global 
Local A thread block consists of warps 
with 32 threads each. 

A warp is physically executed in parallel on 
the same multiprocessor 
. 
Device 
Memory = 
A half-warp of 16 threads can 
access the global memory as a 
single transaction 
(coalescing) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University C for CUDA: Minimal extensions 
43 Declarations (indicate where things are) 
__global__ void KernelFunc(...); // Kernel can be called from the host 
__device__ void DeviceFunc(...); // Function can be called on the device 
__device__ int GlobalVar; // Variable in the device memory 
__shared__ int SharedVar; // Variable in the block-internal shared memory 

Extended syntax for calling parallel kernels 
KernelFunc<<<500, 128>>>(...); // 500 blocks, 128 threads per block 
KernelFunc<<<500, 128, 1024>>>(...); // ... 1024B shared memory per block 

Special variables for thread identification in kernels 
dim3 threadIdx ; dim3 blockIdx ; dim3 blockDim ; 

Intrinsics for specific operations in kernel code 
__syncthreads(); // Synchronization barrier Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Runtime Support 
44 •Explicit memory allocation, returns pointer to GPU memory 
cudaMalloc() , cudaFree() 

•Explicit copying for host device, device device 
(since CUDA 4.0 also between different devices) 
cudaMemcpy() , cudaMemcpyPeer() , ... 

•Interoperability with OpenGL & DirectX 
cudaGLMapBufferObject() , cudaD3D9MapVertexBuffer() , ... Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Summary 
45 CUDA = C + simple extensions 
Easy start when writing basic 
parallel programs 

Three important abstractions: 
1. Hierarchy of parallel threads 
2. Corresponding levels of synchronization 
3. Corresponding memory locations 

Supports massive parallelism of many-core GPUs Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Example: Matrix multiplication 
46 Two matrices A and B are 
multiplied: 
The vector scalar product of the i-th 
row of A with the j-th column 
of B results in the entry Ci,j in the 
result matrix C 

A and B must be correctly 
dimensioned in order to be multiplied. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Example: Matrix multiplication ( n × n) 
47 Two ( n × n) matrices A and B are multiplied: 
The vector scalar product of the i-th row of A with the j-th 
column of B results in the entry Ci,j in the result matrix C 

Vector scalar product: a ∙ b = a0 ∙ b0 + a1 ∙ b1 + … + an-1 ∙ bn-1 

Sequential algorithm 

for (i = rows of A) 

for (j = columns of B) 

for ( k = value in row i, column j) 
C[i,j] += a[i, k] * b[k, j] Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Representation 
48 
Matrix as a one-dimensional array 
in row-major representation Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Repräsentation 
49 for (int i=0; i<width; i++) { 
 for (int j=0; j<width; j++) { 
    float scalar = 0; 
    for (int k=0; k<width; k++) { 
   float a = A[i*width + k]; 
   float b = B[k*width + j]; 
   scalar += a * b; 
    } 
    C[i*width + j] = scalar; 
 } 
} Explizite row-major  Darstellung: 
 
Aus M[i][j] wird M[i*width + j] 
 
Sequenzieller Algorithmus: Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Matrix multiplication 
50 Observations: 
Each entry of the result matrix can be calculated independently 
of the others 
Order of all loops is irrelevant 

Idea: 
One thread for each entry (i, j) of the result matrix 
"Embarrassingly parallel < (no dependencies) 

Theoretically, the innermost loop 
(scalar product) could also be parallelized Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University One thread per matrix entry 
51 Make thread block 2-dimensional to reflect the structure of the 
matrix. 
Thread position in block: (x, y) 

Thread at position ( i, j) in block calculates entry ( i, j) 
of the result matrix: 
int i = ThreadIdx.x ; 
int j = ThreadIdx.y ; 
float scalar = 0; 
for (k=0; k<width; k++) { 
float a = A[i*width + k]; 
float b = B[k*width + j]; 
scalar += a * b; 
} 
C[i*width + j] = scalar; Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Implementation in CUDA C 
52 Initialization 
Memory allocation on the host side, reading the input 
Memory allocation on the device side: cudaMalloc 
Copying the input from the host to the device: cudaMemcpy 

Starting the GPU kernel 
MatrixMulKernel<<<blocksize, threads>>>(...) 

Conclusion 
Copying the output from the device to the host: cudaMemcpy 
Releasing memory: cudaFree Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Initialization (Host) 
53 // Memory allocation for matrices M and N (Host) 
unsigned int size = width * width * sizeof(float) ; 
float* hostM = ( float*) malloc(size); 
float* hostN = ( float*) malloc(size); 

// Fill matrices with values ​​
... 

// Memory allocation for result matrix P (Host) 
float* hostP = ( float*) malloc(size); Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Initialization (Device) 
54 // Memory allocation for matrix M (Device) 
float* deviceM; 
cudaMalloc ((void**)&deviceM, size); 

First parameter of cudaMalloc : 
Address of a pointer that points to the allocated object after allocation 
Cast to (void**) is recommended because a generic pointer is expected 
(and not to a specific type) 
Second parameter: 
Number of bytes to be allocated Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Copying from host to device 
55 // Memory allocation for matrix M (device) 
float* deviceM; 
cudaMalloc ((void**)&deviceM, size); 

// Copy matrix M from host to device 
cudaMemcpy (deviceM, hostM, size, cudaMemcpyHostToDevice); 

Parameters of cudaMemcpy: 
Pointer to target location (device) 
Pointer to source object (host) 
Number of bytes to be copied 
Memory locations involved (source, target) Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Implementation in CUDA C 
56 Initialization 
Memory allocation on the host side, reading the input 
Memory allocation on the device side: cudaMalloc 
Copying the input from the host to the device: cudaMemcpy 

Starting the GPU kernel 
MatrixMulKernel<<<grid, block>>>(...) 

Conclusion 
Copying the output from the device to the host: cudaMemcpy 
Releasing memory: cudaFree Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Start kernel 
57 // Define CUDA execution configuration 
dim3 dimBlock( width, width); 
dim3 dimGrid( 1, 1); // Warning, unrealistic! 

// Call kernel 
MatrixMulKernel <<<dimGrid, dimBlock >>>(deviceM, deviceN, 
deviceP, width); 

Kernel call is made by: 
Name of kernel function 
<<<CUDA configuration >>> 
Parameters of kernel function 

dim3: Struct for displaying the dimensions of blocks and grids. Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Configuration 
58 // Define CUDA execution configuration 
dim3 dimBlock( width, width); 
dim3 dimGrid( 1, 1); 

Restrictions for block and grid configuration (CC 1.x): 
Block: max. dimensions 512 x 512 x 64 
and total size ≤ 512 
and total size multiple of 32 (CC 1.x) 
or 1024 x 1024 x 64 
and total size < 1024 (CC 2.0+) 
Grid: max. dimensions 65535 x 65535 x 1 (CC 1.x) 
or 65535 x 65535 x 65535 (CC 2.x) 
or 231-1 x 65535 x 65535 (CC 3.0+) 

Source: http://en.wikipedia.org/wiki/CUDA Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg Kernel-Code 
59 // Kernel-Funktion, wird von jedem Thread ausgeführ t 
__global__  void MatrixMulKernel (float* A, 
       float* B, 
       float* C, 
       int width) 
{ 
 int i = ThreadIdx.x ;  // x-Koordinate des Threads im 
Block 
 int j = ThreadIdx.y ;  // y-Koordinate des Threads im 
Block 
 float scalar = 0; 
 for ( k=0; k<width; k++) { 
    float a = A[ i*width + k]; 
    float b = B[ k*width + j]; 
    scalar += a * b; 
 } 
 C[i*width + j] = scalar; 
} Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University of Applied Sciences CUDA Keywords 
60 Keyword Execution Call 

__global__ void KernelFct() Device Host 

__device__ float DevFct() Device Device 

__host__ float HostFct() Host Host Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Implementation in CUDA C 
61 Initialization 
Memory allocation on the host side, reading the input 
Memory allocation on the device side: cudaMalloc 
Copying the input from the host to the device: cudaMemcpy 

Starting the GPU kernel 
MatrixMulKernel<<<grid, block>>>(...) 

Conclusion 
Copying the output from the device to the host: cudaMemcpy 
Releasing memory: cudaFree Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Degree 
62 
// Copy result matrix from device to host 
cudaMemcpy (hostP, deviceP, size, cudaMemcpyDeviceToHost); 

// Free memory on device 
cudaFree (deviceM); 
cudaFree (deviceN); 
cudaFree (deviceP); Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Offenburg University Example is simplified 
63 Requirement for matrices 
Square (n x n) 
Size must fit in a block and be a multiple of 32 
n ≤ 16 

Only 1 thread block 
Limits size to 16 x 16 

Extension to general matrices? Prof. Dr. rer. nat. Tobias Lauer 
Dr.-Ing. Alexander Vondrous Hochschule Offenburg CUDA-Programme 
64 
CUDACC 
 CPU Compiler 
C for CUDA 
Kernels 
CUDA object 
files 
Rest of C 
Application 
CPU object 
files 
CPU-GPU 
Executable 
NVCC 
C for CUDA 
Application 
Linker Combined CPU-GPU Code Parallel programming and 
algorithms
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg University Topics
•Parallel sorting
–Part I: Quicksort
–Part II: Radix sort
–Part III: Mergesort (final)
–Part IV: Bitonic sort (later) 3 Offenburg University of Applied SciencesParallel Sort
•Sorting data
–important building block, omnipresent
•Databases
•Search engines (ranking)
• …
–performance bottleneck
• Ω(nlog n) for general sequential sorting methods 
•Time grows more than linearly with the amount of data
→Good candidate for parallelization 4 Offenburg University of Applied SciencesParallel Quicksort
•Quicksort sequential with input length n:
–In-place (no extra space required)
–Complexity 
•O(nlog n) in the best and average case
•O(n²) in the worst case (we ignore here)
–Algorithm: Time T (n):
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to the pivot
•Sort A and Crecursive 2 · T(n/2) 5 Offenburg University Parallelization
•Algorithm: Time (average)
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to pivot
•Sort A and C cursively 2 · T(n/2)
•Obvious:
–Carry out the two sorts of A and C in parallel 6 Offenburg University of Applied Sciences 7 Offenburg University of Applied SciencesParallelization
•Algorithm: Time (average)
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to pivot
•Sort A and C cursively in parallel 1 · T(n/2)
•Complexity:
–Time required (span): T∞(n) = O(n) + 1· T(n/2) = ? 8 Offenburg University of Applied Sciences 9 Offenburg University of Applied SciencesParallelization
•Algorithm: Time (average)
•Choose a pivot element O(1)
•Divide the array into: O(n)
–A: All elements smaller than pivot
–B: Pivot
–C: All elements greater than/equal to pivot
•Sort A and C cursively in parallel 1 · T(n/2)
•Complexity:
–Time required (span): T∞(n) = O(n) + 1· T(n/2) = O(n)
–Speedup is in the range O(log n)
–How many processors are needed? 10 Offenburg University of Applied SciencesParallel Quicksort
•This means that a billion (= 109) elements would be sorted about 30x
faster...
•... with O(n) (e.g. 1/2 billion) processors... 
•Can we parallelize even better, 
e.g. also the more complex partitioning step?
→Yes, if we allow extra space! 11 Offenburg UniversityPartition
•Reminder
–A: elements smaller than pivot
–B: pivot
–C: elements greater than/equal to pivot
•Sequential time: O(n)
•Can you filter A and C from the array faster?
•Yes: execute pack twice! 12 Offenburg UniversityPack
•Pack 1:
–Condition: a[i] < pivot
•Pack 2:
–Condition: a[i] ≥ pivot5 3 10 2 8 6 4 7
5 3 2 6 4
5 3 10 2 8 6 4 7
5 3 2 6 4 10 8 13 Offenburg UniversityReminder: Pack (Compaction) 14 Offenburg University Extra storage space
•Array to remember whether the condition is fulfilled
–Prefix sum on top to calculate the target position
•Auxiliary array for the results of pack 1 and 2
–A and C can write their results into the same auxiliary array 
(insert pivot in between!)
–Pack 1 and 2 can also be carried out together
(does not change the asymptotic complexity)
•At the end, swap the auxiliary array with the original array
(always sort back and forth between 2 arrays) 15 Offenburg University Complexity
•Partition
–2x Pack
–Time (span): O(log n)
–Processors: O(n / log n)
•Total:
–Time (span): T(n)= O(log n) + 1· T(n/2) = 
–Processors: O( n / log n) 16 Offenburg University of Applied SciencesParallel Quicksort: Conclusion
•Quicksort can be parallelized in a cost-optimal way
•There are several other parallelization variants 17 Offenburg UniversityRadix Sort
•Fastest known parallel sorting method in
practice (on GPUs)
–especially for large numbers of elements …
–… and not too large elements
•Not only based on comparing and swapping
–Not a general sorting method
–makes use of the number format used! 18 Offenburg UniversityIdea
• Sort by iterative distribution into "compartments", where there is a compartment for each digit of the number representation.
–Start with the last digit (far right)
–Keep the order of the elements per "compartment" the same
•Was previously used for manual or mechanical sorting (e.g. postcodes, punch cards) 19 Offenburg UniversityExample (board) 20 Offenburg University of Applied Sciences Implementation
1. Choose a "suitable" base d
2. Start with the lowest value i
3. Partition elements according to the value at position i
–Do not change the existing order between the elements within the group
4. Attach the partitions to one another 
→ new global arrangement
5. Increase ium 1 
6. If not yet sorted, go to step 3. 21 Offenburg University Complexity
•Number of runs:
–up to e (e is the exponent of the representation basis)
–e is constant for the specific application (but relevant)
–can be selected within a certain framework
–determines potential additional space requirements
•Number of steps per run:
–view and distribute n elements
•O( e·n) 22 Offenburg University of Applied SciencesParallelization
•First: d= 2 (binary representation)
•Example: 4, 7, 2, 6, 3, 5, 1, 0 (3 bits required)
–Consider the first run:
–We split the sequence into two parts:
•Elements that end with 0
•Elements that end with 1
–How can we do that? 23 Offenburg UniversityParallel Primitive: Split
•Partition a sequence into two subsequences based on a condition
•We already know this!
•Essentially corresponds to 2x Pack 24 Offenburg UniversitySplit
•Select the relevant bit in the input
•Store the inverse of the considered 
bit in temporary array e
•Prefix sum on e →f: f contains the 
target addresses of the elements with 0
•Store the total number of 
0 values ​​in a global variable
•Another temporary array t for 
target position of the 1 values
•Select d from t or f, depending on the bit value
•Copy input elements at position d in 
output 25 Offenburg University Radix Sort with Split
•For complete sorting:
–Repeat split with the next highest bit …
–… until the sequence is sorted.
–When is that?
•At the latest after the split with the most significant bit
•If the keys are not too large, possibly much earlier
•Complexity:
–Time per split: O(log n) 26 Offenburg University Radix Sort with Split
•For complete sorting:
–Repeat split with the next highest bit …
–… until the sequence is sorted.
–When is that?
•At the latest after the split with the most significant bit
•If the keys are not too large, possibly earlier
•Still a problem: lots of passes!
–Solution: see lab 27 Offenburg University of Applied SciencesPractical aspects
•Implementation for very large input?
–Division into blocks the size of a SIMD array
–Sorting per block
–Merging the sorted blocks
→Parallel merge 28 Offenburg University of Applied SciencesMerge-Sort in practice
•Problem:
Tree-like division into ever smaller subsequences
•Leads to different scenarios:
–More merges than threads in the lower part of the tree
–Merges ~ threads in a middle region
–More threads than merges in the upper part
•Idea: 
Use different approaches depending on the region in the tree 29 Offenburg University Merge methods
•In the lower part:
–Sequential:
Each thread merges small subsequences sequentially 
(or sorts them sequentially in another way)
–Middle part:
Parallelization according to architecture (e.g. GPU multiprocessors)
•1 or a few merges per thread 
•enables optimal parallel calculation of independent subsequences 
(e.g. per processor group)
–Upper part:
•Parallelize individual merge steps
(→see later: Parallel Merge) 30 Offenburg University Sorting networks
•Property of many sorting algorithms
–Data dependency
–Next action depends on the value of the key in question
–Threads are unevenly loaded / diverge
–Not optimal for SIMD-like architectures
•Better suited algorithms
–As similar a task as possible for all threads
→“Sorting Networks” 31 Offenburg UniversityBitonic Sort
•Similarities to Merge Sort
–Recursive approach
•Halving the sequence in each step
–Sorting the halves again with Bitonic Sort
•New: Merging the sorted subsequences is also recursive
– “Bitonic Merge” 
–but: in-place (no extra space required)
•No copying, just swapping
•Uses an important property of bitonic sequences 32 Offenburg UniversityBitonic number sequences
•Bitonic (cf. monotonic):
–Sequence consists of a monotonically increasing and a monotonically 
decreasing part
–Example:
7, 10, 24, 21, 18, 10, 4, 3 33 Offenburg UniversityDefinition
•A sequence e0, e1, …, is called bitonic if there is an 
index i(0 < i< n) such that 
–either e0, …, eimonotonically increasing and ei, …, enmonotonically decreasing 
–or there is a cyclical shift of the sequence for which the above 
applies.
•Example:
Image source: Philipp Slusallek 34 Offenburg UniversityComparison Network
•Element-wise comparison of two episodes (here: sorted) 35 Offenburg UniversityComparison Network
•Element comparisons
–Completely independent of each other
–Always the same number (namely n/2)
–Always structured the same (regardless of data quality)
→Perfectly parallelizable (“embarrassingly parallel”)
→Very well suited for SIMD, e.g. for GPUs 36 Offenburg University Observation 1
•Decomposition of a bitonic sequence into 2 subsequences (of the same length):
•Comparison step: both subsequences are then also 
bitonic again (proof not trivial!)
•All elements of the right subsequence are larger than all elements of the 
left subsequence 37 Offenburg UniversityBitonic Split
•Splits a sequence using a comparison network: 38 Hochschule OffenburgBitonicSplit
bitonicSplit( intlo, inthi) {
if(lo>= hi) return;
intm= (hi+lo)/2;
intstride= (hi-lo+1)/2;  // Abstand verglichener Elemente
for(inti=lo; i<=m; i++) in parallel {
if(input[i] > input[i+stride]) {
swap(i, i+stride);
}
}
} 39 Offenburg University Observation 2
•By repeated (recursive) splitting, a bitonic sequence is completely sorted (→“BitonicMerge”) 40 High School OffenburgBitonicMerge (recursive)
bitonicMerge( house, house) {
if ( lo >= hi ) return ;
bitonicSplit( no, hi);
intm= (hi+lo)/2;
bitonicMerge ( lo , m ) ;
bitonicMerge( m+1, hi);
} } 41 Offenburg University of Applied SciencesWhat to do with non-bitonic sequences?
•Unsorted sequences are usually not bitonic
•Can you make them bitonic?
–Idea: 
•Divide the unsorted sequence into two equal halves
•Sort the left half in ascending order and the right half in descending order 
(how? →recursively with BitonicSort)
•The sequence is now bitonic and can now be sorted with a BitonicMerge. 42 Offenburg UniversityBitonicSort
•BitonicSort( n) 43 Hochschule OffenburgBitonicBlack (recursive)
static void bitonicSort( home, home) {
if ( lo >= hi ) return ;
// Split the input
intm= (hi+lo)/2;
// Sort lower half in same order
bitonicSort ( lo , m ) ;
// Sort upper half in reverse order
bitonicSort_reverse( m+1, hi);
// Sort bitonic sequence lo..hi
bitonicMerge( lo , hi );
} } 44 Offenburg University of Applied SciencesFor GPU: Resolve recursion
•Iterative view (example with 16 elements):
k=2 k=4 k=8 k=16s=2s=1 s=2s=1 s=4 s=2s=1 s=4 s=8 s=1
k: current size of the subsequence(s) s: distance between two compared elements 46 Offenburg University of Applied SciencesIterative algorithm
bitonicSort_iterative (int[] input) {
final int NUM= input.length;
for(intk=2; k<=NUM; k*=2) { // log2(n) iterations
for(ints=k/2; s>0; s/=2) { // <= log2(n) iterations
for(inttid=0; tid<NUM; tid++) in parallel {
intixj= tid^ s; // XOR // Determine exchange partner
if(ixj> tid) { // only 1 exchange per 2 elements
if((tid& k) == 0) { // In “even k-group” ...
if(input[tid] > input[ixj]) // ... sort in ascending order
swap(input, tid, ixj);
} else{ // else...
if(input[tid] < input[ixj]) // ... sort descending
swap(input, tid, ixj);
}
}
} // sync
}
}
} 47 Offenburg University Complexity
•Cost: O(nlog² n)
•Time/Span: O(log² n)
•Processors: O(n) 49 Offenburg University Summary
•Parallelization can speed up sorting significantly
•General sorting methods (quicksort, merge sort) are 
flexible in terms of the elements to be sorted
•Special methods such as radix sort can be very fast 
(~1 billion 32-bit numbers per second on a GPU).
•Bitonic sort has no data dependency and therefore 
minimal thread divergence Parallel Programming and Algorithms
Tobias Lauer
Offenburg University of Applied Sciences 2 Offenburg UniversityTopics
•Parallel algorithms 
–Parallel merge
(combining 2 large sorted arrays) 3 Offenburg University of Applied SciencesParallel Merge
•We already know merging from merge-sort
•Input: 
Two sorted arrays A and B of lengths m and n
–Here: not necessarily the same/similar length
–We assume that m≥ n (otherwise swap A and B)
•Output: 
An array C of length m+n that contains the elements 
of A and B sorted 4 Offenburg UniversityExample
A=( 2, 8, 11, 13, 17, 20) B=( 3, 6, 10, 15, 16, 73)
→ C= (2, 3, 6, 8, 10, 11, 13, 15, 16, 17, 20, 73)
•Sequential algorithm (see merge sort)
–Go through A and B simultaneously using two pointers
–In each step:
•Copy the smaller of the two elements from branch B
to the current position in C
•Increment the corresponding pointer and the current position in C
–The complexity is O(m+n) 5 Offenburg University of Applied SciencesParallelization: 1st attempt
•Observation:
–You can perform the merging from the left (ascending) or from the right 
(descending)
–Can both be done at the same time?
•Yes, if you fill the target array exactly halfway 
(then there can be no write conflicts)
–Parallel merging with 2 threads/processors possible
•You can't do more with this approach!
•Is there even more parallelism possible? 6 Offenburg University of Applied SciencesParallel merging
Idea: 
Divide the arrays appropriately and run the sequential 
algorithm in parallel on the parts
Definition:
rank(ai: A) = number of elements in A 
that are less than or equal to ai ( aiєA)
rank(bi: A) = number of elements in A 
that are less than or equal to bis ( biєB) 7 Offenburg UniversityExample and remark
A=( 2, 8, 11, 13, 17, 20 ) B=( 3, 6, 10, 15, 16, 73 )
rank(11 : A) = 3 rank(11 : B) = 3
rank(16 : A) = 4 rank(16 : B) = 5
Remark:
The rank of an element efrom Aor Bin 
result array Ccorresponds to
rank(e: A) + rank(e: B)
(if you start counting at 1) 9 Offenburg University Division
• Divide array A into blocks so that each block consists of log m
elements (except possibly the last one).
• This gives us m/log m blocks
• The ith block ends at position i∙ log m(1 ≤ i< m/log m)
(and the last block ends at position m) a1… …… alog ma(log m)+1 … a2log m ………. 10 Offenburg University Division
•There is a unique assignment of each block Ai to 
a block in B
•We call such a pair of blocks a matching pair
•Problem: How do you find a matching pair? a1… ……alog ma(log m)+1 …a2log m ………. 11 Offenburg University of Applied SciencesParallel merging
Goal:
–Running time of the sequential algorithm for a matching pair
in O(logm+ logn)
–For this, the size of each piece should be in O (logm)
or O (logn)
–This division was easy for A; what about B?
Approach:

–Use the rank of elements to divide B
into suitable pieces
–Then run the sequential algorithm in parallel on one 
matching pair each 12 Offenburg University Processor allocation
•Use a processor for each block A of A
•For all i in parallel (except the last):
–Find a rank in B for the last element of A
i.e. determine rank(ai log m : B)
–How can you determine the rank efficiently?
•Binary search
–Time complexity for this step
•O(log n)
–Binary searches can take place in parallel (CREW)
•After this step, array B is also implicitly divided into 
m/log m pieces 13 Offenburg University Division
•There is a unique assignment of each block Ai to 
a block in B (blocks in B can also be empty)
•We call such a pair of blocks a matching pair a1… …… alog malog m + 1 … a2log m ………. 14 Offenburg University Division
•The rank of each element within a block Ai lies 
in the corresponding matching block Bi.
•Therefore, merging a matching pair is an 
independent subproblem.ai log m a(i+1) log m
rank(ai log m : B) rank(a(i+1)log m : B) 15 Offenburg University Merging Matching Pairs
Size of the blocks in B:
•Good case: 
–Size of each block is (approximately) in O(logn)
→Use the sequential algorithm for merging
•Bad case: 
–There are one or more long blocks
→We have to “fix” that (how?) 16 Offenburg University of Applied SciencesWorst case
•A single block Aivon Ahas the 
complete array B as a matching partner
•Idea: Use the same algorithm for Bund Ai 17 Offenburg UniversityGood case
•Use one processor per matching pair
•Merge the two blocks of the pair using the sequential algorithm
–The time for this is O (logm + logn)
•All pairs can be processed simultaneously 18 Offenburg University Complexity analysis
•Step 1: Division of array A
–The division is purely based on position (index)
–We have m/log m processors 
(one processor per block of size log m )
–The end element of each block can therefore be determined in 
constant time O(1)
–The cost is in O(m/log m) 19 Offenburg University Complexity analysis
•Step 2: Division of array B
–This division is done according to values ​​(rank( a: B))
–We use the binary search in B for this
This takes time in O(logn)
–Here too we have m/logm processors
–Cost: O( logn∙ (m/logm) ) 20 Offenburg University Assessment 21 Offenburg University Complexity analysis
•Step 2: Division of array B
–This division is done according to values ​​(rank( a: B))
–We use the binary search in B for this
This takes time in O(logn)
–Here too we have m/logm processors
–Cost: O( ( m/logm)∙ logn) = O( m+ n) 22 Offenburg University Complexity analysis
•Step 3: Sequential merging of matching pairs
–The time for this is O( log m + log n ) 
–Here too we have m/logm processors
–Cost: O( ( m/logm)∙ (logm+ logn) ) 
= O( m+ ( m/logm∙ logn) ) 
= O(m) + O( m+ n)
= O(m+ n) 23 Offenburg University Complexity analysis
•Summary:
Time Cost
Step 1: O(1) O( m/log m )
Step 2: O( logn) O( m+ n)
Step 3: O( log m+ logn) O( m+ n)
Total: O(logm+ logn) O( m+ n) 24 Offenburg University of Applied SciencesWhat about the bad cases?

•Worst case:
A single block Aivon Ahas the complete array B as a matching partner
•Then use the same algorithm for block Ai:
–Length of Ai= log m < m
–Length of B= n
–Total runtime for this part never worse than before
• For cases that lie “in between”, you can proceed analogously. Optimizing Parallel Reduction in CUDA
Mark Harris
NVIDIA Developer Technology 2
Parallel Reduction
Common and important data parallel primitive
Easy to implement in CUDA
Harder to get it right
Serves as a great optimization example
We’ll walk step by step through 7 different versions
Demonstrates several important optimization strategies 3
Parallel Reduction
Tree-based approach used within each thread block
Need to be able to use multiple thread blocks
To process very large arrays
To keep all multiprocessors on the GPU busy
Each thread block reduces a portion of the array
But how do we communicate partial results between 
thread blocks?4 7 5 9
11 14
253 1 7 0 4 1 6 3 4
Problem: Global Synchronization
If we could synchronize across all thread blocks, could easily 
reduce very large arrays, right?
Global sync after each block produces its result
Once all blocks reach sync, continue recursively
But CUDA has no global synchronization.  Why?
Expensive to build in hardware for GPUs with high processor 
count
Would force programmer to run fewer blocks (no more than # 
multiprocessors * # resident blocks / multiprocessor) to avoid 
deadlock, which may reduce overall efficiency 
Solution: decompose into multiple kernels
Kernel launch serves as a global synchronization point
Kernel launch has negligible HW overhead, low SW overhead 5
Solution: Kernel Decomposition
Avoid global sync by decomposing computation 
into multiple kernel invocations
In the case of reductions, code for all levels is the 
same
Recursive kernel invocation4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163
4 7 5 9
11 14
2531704163Level 0:
8 blocks
Level 1:
1 block 6
What is Our Optimization Goal?
We should strive to reach GPU peak performance
Choose the right metric:
GFLOP/s: for compute-bound kernels
Bandwidth: for memory-bound kernels
Reductions have very low arithmetic intensity
1 flop per element loaded (bandwidth-optimal)
Therefore we should strive for peak bandwidth
Will use G80 GPU for this example
384-bit memory interface, 900 MHz DDR
384 * 1800 / 8 = 86.4 GB/s 7
Reduction #1: Interleaved Addressing
__global__ void reduce0( int*g_idata, int*g_odata) {
extern __shared__ int sdata[];
// each thread loads one element from global to shared mem
unsigned int tid = threadIdx.x ;
unsigned int i = blockIdx.x *blockDim.x + threadIdx.x ;
sdata[tid] = g_idata[i];
__syncthreads() ;
// do reduction in shared mem
for(unsigned int s=1; s < blockDim.x ; s *= 2) {
if(tid % (2*s) == 0) {
sdata[tid] += sdata[tid + s];
}
__syncthreads() ;
}
// write result for this block to global mem
if(tid == 0) g_odata[ blockIdx.x ] = sdata[0];
} 8
Parallel Reduction: Interleaved Addressing
10 1 8 -1 0 -2 3 5 -2 -3 2 7 0 11 0 2 Values (shared memory)
0 2 4 6 8 10 12 14
11 1 7 -1 -2 -2 8 5 -5 -3 9 7 11 11 2 2 Values
0 4 8 12
18 1 7 -1 6 -2 8 5 4 -3 9 7 13 11 2 2 Values
0 8
24 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 Values
0
41 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 ValuesThread 
IDsStep 1 
Stride 1
Step 2 
Stride 2
Step 3 
Stride 4
Step 4 
Stride 8Thread 
IDs
Thread 
IDs
Thread 
IDs 9
Reduction #1: Interleaved Addressing
__global__ void reduce1( int*g_idata, int*g_odata) {
extern __shared__ int sdata[];
// each thread loads one element from global to shared mem
unsigned int tid = threadIdx.x ;
unsigned int i = blockIdx.x *blockDim.x + threadIdx.x ;
sdata[tid] = g_idata[i];
__syncthreads() ;
// do reduction in shared mem
for(unsigned int s=1; s < blockDim.x ; s *= 2) {
if(tid % (2*s) == 0) {
sdata[tid] += sdata[tid + s];
}
__syncthreads() ;
}
// write result for this block to global mem
if(tid == 0) g_odata[ blockIdx.x ] = sdata[0];
}Problem: highly divergent 
warps are very inefficient, and 
% operator is very slow 10
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Note: Block Size = 128 threads for all testsBandwidth Time (222 ints) 11for(unsigned int s=1; s < blockDim.x ; s *= 2) {
if(tid % (2*s) == 0) {
sdata[tid] += sdata[tid + s];
}
__syncthreads() ;
}
for(unsigned int s=1; s < blockDim. x; s *= 2) {
int index = 2 * s * tid;
if (index < blockDim. x) {
sdata[index] += sdata[index + s];
}
__syncthreads();
}
Reduction #2: Interleaved Addressing
Just replace divergent branch in inner loop:
With strided index and non-divergent branch: 12
Parallel Reduction: Interleaved Addressing
10 1 8 -1 0 -2 3 5 -2 -3 2 7 0 11 0 2 Values (shared memory)
0 1 2 3 4 5 6 7
11 1 7 -1 -2 -2 8 5 -5 -3 9 7 11 11 2 2 Values
0 1 2 3
18 1 7 -1 6 -2 8 5 4 -3 9 7 13 11 2 2 Values
0 1
24 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 Values
0
41 1 7 -1 6 -2 8 5 17 -3 9 7 13 11 2 2 ValuesThread 
IDsStep 1 
Stride 1
Step 2 
Stride 2
Step 3 
Stride 4
Step 4 
Stride 8Thread 
IDs
Thread 
IDs
Thread 
IDs
New Problem: Shared Memory Bank Conflicts 13
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 14
Parallel Reduction: Sequential Addressing
10 1 8 -1 0 -2 3 5 -2 -3 2 7 0 11 0 2 Values (shared memory)
01234567
8 -2 10 6 0 9 3 7 -2 -3 2 7 0 11 0 2 Values
0123
8 7 13 13 0 9 3 7 -2 -3 2 7 0 11 0 2 Values
01
21 20 13 13 0 9 3 7 -2 -3 2 7 0 11 0 2 Values
0
41 20 13 13 0 9 3 7 -2 -3 2 7 0 11 0 2 ValuesThread 
IDsStep 1 
Stride 8
Step 2 
Stride 4
Step 3 
Stride 2
Step 4 
Stride 1Thread 
IDs
Thread 
IDs
Thread 
IDs
Sequential addressing is conflict free 15for(unsigned int s=1; s < blockDim. x; s *= 2) {
int index = 2 * s * tid;
if (index < blockDim. x) {
sdata[index] += sdata[index + s];
}
__syncthreads();
}
for (unsigned int s=blockDim. x/2; s>0; s>>=1) {
if (tid < s) {
sdata[tid] += sdata[tid + s];
}
__syncthreads();
}
Reduction #3: Sequential Addressing
Just replace strided indexing in inner loop:
With reversed loop and threadID-based indexing: 16
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 17for (unsigned int s=blockDim.x/2 ; s>0; s>>=1) {
if (tid < s) {
sdata[tid] += sdata[tid + s];
}
__syncthreads();
}
Idle Threads
Problem: 
Half of the threads are idle on first loop iteration!
This is wasteful… 18// each thread loads one element from global to shared mem
unsigned int tid = threadIdx.x ;
unsigned int i = blockIdx.x *blockDim.x + threadIdx.x ;
sdata[tid] = g_idata[i];
__syncthreads() ;
// perform first level of reduction,
// reading from global memory, writing to shared memory
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockDim.x*2 ) + threadIdx. x;
sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];
__syncthreads();
Reduction #4: First Add During Load
Halve the number of blocks, and replace single load:
With two loads and first add of the reduction: 19
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 20
Instruction Bottleneck
At 17 GB/s, we’re far from bandwidth bound
And we know reduction has low arithmetic intensity
Therefore a likely bottleneck is instruction overhead
Ancillary instructions that are not loads, stores, or 
arithmetic for the core computation
In other words: address arithmetic and loop overhead
Strategy: unroll loops 21
Unrolling the Last Warp
As reduction proceeds, # <active= threads decreases
When s <= 32, we have only one warp left
Instructions are SIMD synchronous within a warp
That means when s <= 32:
We don’t need to __syncthreads()
We don’t need <if (tid < s)= because it doesn’t save any 
work
Let’s unroll the last 6 iterations of the inner loop __device__ void warpReduce( volatile int* sdata, int tid ) {
sdata[tid] += sdata[tid + 32]; 
sdata[tid] += sdata[tid + 16]; 
sdata[tid] += sdata[tid +  8]; 
sdata[tid] += sdata[tid +  4]; 
sdata[tid] += sdata[tid +  2]; 
sdata[tid] += sdata[tid +  1]; 
}
// later…
for(unsigned int s=blockDim. x/2; s>32 ; s>>=1) {
if (tid < s)
sdata[tid] += sdata[tid + s];
__syncthreads();
}
if (tid < 32) warpReduce(sdata, tid);
22
Reduction #5: Unroll the Last Warp
Note: This saves useless work in allwarps, not just the last one!
Without unrolling, all warps execute every iteration of the for loop and if statementIMPORTANT: 
For this to be correct,
we must use the 
<volatile= keyword! 23
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34x
Kernel 5:
unroll last warp0.536 ms 31.289 GB/s 1.8x 15.01xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 24
Complete Unrolling
If we knew the number of iterations at compile time, 
we could completely unroll the reduction
Luckily, the block size is limited by the GPU to 512 threads
Also, we are sticking to power-of-2 block sizes
So we can easily unroll for a fixed block size
But we need to be generic –how can we unroll for block 
sizes that we don’t know at compile time?
Templates to the rescue!
CUDA supports C++ template parameters on device and 
host functions 25
Unrolling with Templates
Specify block size as a function template parameter:
template <unsigned int blockSize>
__global__ void reduce5( int *g_idata, int *g_odata) 26
Reduction #6: Completely Unrolled
if (blockSize >= 512) {
if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }
if (blockSize >= 256) {
if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }
if (blockSize >= 128) {
if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); }
if (tid < 32) warpReduce< blockSize >(sdata, tid);
Note: all code in RED will be evaluated at compile time.
Results in a very efficient inner loop!Template < unsigned int blockSize >
__device__ void warpReduce( volatile int* sdata, int tid ) {
if (blockSize >= 64) sdata[tid] += sdata[tid + 32]; 
if (blockSize >= 32) sdata[tid] += sdata[tid + 16]; 
if (blockSize >= 16) sdata[tid] += sdata[tid +  8]; 
if (blockSize >=   8) sdata[tid] += sdata[tid +  4]; 
if (blockSize >=   4) sdata[tid] += sdata[tid +  2]; 
if (blockSize >=   2) sdata[tid] += sdata[tid +  1]; 
} 27
Invoking Template Kernels
Don’t we still need block size at compile time?
Nope, just a switch statement for 10 possible block sizes:
switch ( threads)
{
case 512:
reduce5<512><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 256:
reduce5<256><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 128:
reduce5<128><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 64:
reduce5< 64><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 32:
reduce5< 32><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case 16:
reduce5< 16><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  8:
reduce5<  8><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  4:
reduce5<  4><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  2:
reduce5< 2><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
case  1:
reduce5<  1><<< dimGrid, dimBlock, smemSize >>>(d_idata, d_odata); break;
} 28
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34x
Kernel 5:
unroll last warp0.536 ms 31.289 GB/s 1.8x 15.01x
Kernel 6:
completely unrolled0.381 ms 43.996 GB/s 1.41x 21.16xStep
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 29
Parallel Reduction Complexity
Log( N)parallel steps, each step Sdoes N/2S
independent ops
Step Complexity is O(log N)
For N=2D, performs S[1..D]2D-S= N-1 operations 
Work Complexity is O( N)–It is work-efficient
i.e. does not perform more operations than a sequential 
algorithm
With Pthreads physically in parallel ( Pprocessors), 
time complexity is O( N/P + log N) 
Compare to O( N) for sequential reduction
In a thread block, N=P, so O(log N) 30
What About Cost?
Cost of a parallel algorithm is processors time 
complexity
Allocate threads instead of processors: O( N) threads
Time complexity is O(log N), so cost is O( Nlog N) : not 
cost efficient!
Brent’s theorem suggests O( N/log N) threads
Each thread does O(log N) sequential work
Then all O( N/log N) threads cooperate for O(log N) steps
Cost = O(( N/log N) * log N) = O( N) cost efficient
Sometimes called algorithm cascading
Can lead to significant speedups in practice 31
Algorithm Cascading
Combine sequential and parallel reduction
Each thread loads and sums multiple elements into 
shared memory
Tree-based reduction in shared memory
Brent’s theorem says each thread should sum 
O(log n) elements
i.e. 1024 or 2048 elements per block vs. 256
In my experience, beneficial to push it even further
Possibly better latency hiding with more work per thread
More threads per block reduces levels in tree of recursive 
kernel invocations 
High kernel launch overhead in last levels with few blocks
On G80, best perf with 64-256 blocks of 128 threads
1024-4096 elements per thread 32unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockDim.x*2 ) + threadIdx. x;
sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];
__syncthreads();
Reduction #7: Multiple Adds / Thread
Replace load and add of two elements:
With a while loop to add as many as necessary:
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockSize*2) + threadIdx. x;
unsigned int gridSize = blockSize*2* gridDim. x;
sdata[tid] = 0;
while (i < n) {
sdata[tid] += g_idata[i] + g_idata[i+blockSize];
i += gridSize;
}
__syncthreads(); 33unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockDim.x*2 ) + threadIdx. x;
sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];
__syncthreads();
Reduction #7: Multiple Adds / Thread
Replace load and add of two elements:
With a while loop to add as many as necessary:
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockSize*2) + threadIdx. x;
unsigned int gridSize = blockSize*2* gridDim. x;
sdata[tid] = 0;
while (i < n) {
sdata[tid] += g_idata[i] + g_idata[i+blockSize];
i += gridSize;
}
__syncthreads();Note: gridSize loop stride 
to maintain coalescing! 34
Performance for 4M element reduction
Kernel 1: 
interleaved addressing
with divergent branching8.054 ms 2.083 GB/s
Kernel 2:
interleaved addressing
with bank conflicts3.456 ms 4.854 GB/s 2.33x 2.33x
Kernel 3:
sequential addressing1.722 ms 9.741 GB/s 2.01x 4.68x
Kernel 4:
first add during global load0.965 ms 17.377 GB/s 1.78x 8.34x
Kernel 5:
unroll last warp0.536 ms 31.289 GB/s 1.8x 15.01x
Kernel 6:
completely unrolled0.381 ms 43.996 GB/s 1.41x 21.16x
Kernel 7:
multiple elements per thread0.268 ms 62.671 GB/s 1.42x 30.04x
Kernel 7 on 32M elements: 73 GB/s!Step
Speedup Bandwidth Time (222 ints)Cumulative
Speedup 35template <unsigned int blockSiz e>
__device__ void warpReduce( volatile int *sdata , unsigned int tid) {
if (blockSize >=  64) sdata[tid] += sdata[tid + 32];
if (blockSize >=  32) sdata[tid] += sdata[tid + 16];
if (blockSize >=  16) sdata[tid] += sdata[tid +  8];
if(blockSize >=   8) sdata[tid] += sdata[tid +  4];
if (blockSize >=   4) sdata[tid] += sdata[tid +  2];
if (blockSize >=   2) sdata[tid] += sdata[tid +  1];
}
template <unsigned int blockSize>
__global__ void reduce6( int *g_idata, int *g_odata, unsigned int n) {
extern __shared__ int sdata[];
unsigned int tid = threadIdx. x;
unsigned int i = blockIdx. x*(blockSize*2) + tid;
unsigned int gridSize = blockSize*2* gridDim. x;
sdata[tid] = 0;
while (i < n) { sdata[tid] += g_idata[i] + g_idata[i+blockSize];  i += gridSize;  }
__syncthreads();
if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }
if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }
if (blockSize >= 128) { if (tid <  64) { sdata[tid] += sdata[tid +  64]; } __syncthreads(); }
if (tid < 32) warpReduce(sdata, tid);
if (tid == 0) g_odata[ blockIdx. x] = sdata[0];
}Final Optimized Kernel 36
Performance Comparison
0.010.1110
131072262144
524288
1048576
2097152
4194304
8388608
16777216
33554432
# ElementsTime (ms)1: Interleaved Addressing:
Divergent Branches
2: Interleaved Addressing:
Bank Conflicts
3: Sequential Addressing
4: First add during global
load
5: Unroll last warp
6: Completely unroll
7: Multiple elements per
thread (max 64 blocks) 37
Types of optimization
Interesting observation:
Algorithmic optimizations
Changes to addressing, algorithm cascading
11.84x speedup, combined!
Code optimizations
Loop unrolling
2.54x speedup, combined 38
Conclusion
Understand CUDA performance characteristics
Memory coalescing
Divergent branching
Bank conflicts
Latency hiding
Use peak performance metrics to guide optimization 
Understand parallel algorithm complexity theory
Know how to identify type of bottleneck
e.g. memory, core computation, or instruction overhead
Optimize your algorithm, then unroll loops
Use template parameters to generate optimal code
Questions: mharris@nvidia.com Parallel Computing
Tobias Lauer
Offenburg University of Applied Sciences 2 Hochschule OffenburgOverview
•CUDA-optimized algorithms
–Warp-internal functions for optimization
–Atomic operations on recent CUDA architectures 4 Hochschule OffenburgExample: Reduction (Redux)
•Aggregate an input of nvalues to one single output
value
–E.g. sum, min, max, product, count , …
•Sequential algorithm: O(n) time 
•Parallel algorithm with tree-like structure
–O(log n) time with O(n) processors
•Can be improved to O( n/ log n) processors
→Cost complexity O( n) →cost-optimal 
–Requires synchronization between each level of the tree
•In CUDA: __syncthreads() 5 Offenburg University of Applied SciencesParallel reduction algorithm 6 Hochschule OffenburgReduction
•Lots of potential for optimization (see Slides by Nvidia)
–Shared memory
–Avoiding memory bank conflicts
–Sequential computation per thread (algorithm cascading)
–…
•Today : use SIMD characteristics of CUDA
–Direct access to registers of neighboring threads
–No synchronisation required inside a warp
•Also: use atomic operations
–Very efficient on recent CUDA architectures (not on older ones) 7 Hochschule OffenburgRecap: Warps
•A warp is a bundle of 32 „neighboring “ threads
within a thread-block
–All threads operate in lockstep („im Gleichtakt“), 
i.e. they carry out the same instruction at the same clock cycle
•Exception: threads that don‘t have anything to do (because of a 
branch) do nothing
•SIMT (single instruction multiple thread ), „almost “ SIMD
–Inside the same warp, the thread IDs % 32 are 0..31 
•i.e. for the first thread of a warp, threadId.x % 32 == 0 8 Hochschule OffenburgWarp-internal operations
•Threads of the same warp have access to registers of
other threads in the same warp
–Can use other threads ‘ values of variables, share or broadcast their
own values , …
•How can this be done (from a language point of view)?
–Problem: variable names are the same in all threads
–Solution: Special warp-internal commands (intrinsics)
•__ballot()
•__shfl()
•… 9 Hochschule OffenburgThe shuffle command (SHFL)
•__shfl(intvar, intsourceThread, intwidth)
•__shfl_down (intvar, intthreadDelta, intwidth)
•__shfl_up (intvar, intthreadDelta, intwidth)
•__shfl_xor (intvar, intbitMask, intwidth)
Returns the value of var of the thread in the same (part of the) warp 
(of sizewidth ), whose ID is given by the second parameter. 10 Offenburg UniversityExample: Shuffle-down
inti = threadIdx.x % 32; 
intj = __shfl_down (i, 2, 8);
Source: https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/ 11 Hochschule OffenburgAdvantages
•Sequence of several shared memory instructions is 
replaced by one single instruction
–Increases the bandwidth
–Reduces latency
•Requires noshared memory
–can be used for other things (limited resource)
•Synchronization in warp is implicit in every instruction, 
hence noneed for__syncthreads()
–Less block-wide synchronization required, which otherwise would
decrease performance 12 Hochschule OffenburgOptimization of the reduction algorithm
•Idea:
–Reduction per warp
•using shuffle
–Reduction of all warp results in a block
•again using shuffle
–Reduction of all block results of the kernels
•by calling a second kernel
–The above limits the number of blocks (≤ 1024 = 32*32)
•Solution: start with a sequential reduction per thread 13 Hochschule OffenburgWarp reduction
__device__ int warpReduceSum (int val) { 
for (int offset = warpSize/2; offset > 0; offset /= 2) 
val += __shfl_down (val, offset); 
return val; 
} 14 Hochschule OffenburgBlock reduction using warp reduction
__device__ int blockReduceSum (int val) { 
static __shared__ int shared[32]; // Shared mem for 32 partial sums 
int lane = threadIdx.x % warpSize; // thread id inside warp
int wid = threadIdx.x / warpSize; // warp id inside block
val = warpReduceSum (val); // Each warp performs partial reduction 
if (lane==0) shared[wid]=val; // Write reduced value to shared memory 
__syncthreads(); // Wait for all partial reductions 
//read from shared memory only if that warp existed 
val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;   
if (wid==0) val = warpReduceSum (val); // Final reduce within first warp
return val; 
} 15 Hochschule OffenburgComplete kernel
__global__ void deviceReduceKernel (int *in, int* out, int N) { 
int sum = 0; 
// sequentially pre-reduce data to size of grid
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; 
i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread writes out block result
out[blockIdx.x]=sum; 
} 16 Hochschule OffenburgKernel call
void deviceReduce(int *in, int* out, int N) { 
int threads = 512; // enough to saturate GPU
int blocks  = min((N + threads - 1) / threads, 1024);
deviceReduceKernel<<<blocks, threads>>>(in, out, N);
// Second kernel call for final reduction
deviceReduceKernel<<<1, 1024>>>(out, out, blocks); 
} 17 Hochschule OffenburgFurther optimization?
Potentially inefficient:
•Second kernel call
–Under-occupies GPU (only one SM is used)
–Global synchronization between kernels
•Need to wait for every thread inevery block
•Block reduction uses __syncthreads()
–Block-wide synchronization
•Need to wait for every thread in that block 18 Hochschule OffenburgAtomic Add
•Thread-safe addition of a value to a variable
–Implicitly synchronized –no manual synchronization required
•Syntax:   atomicAdd(variable, valueToBeAdded);
•Attention: this is a blocking operation!
–Only 1 thread per cycle can add
–Threads in same warp will be serialized
–Danger of contention of threads („Staugefahr“) 19 Hochschule OffenburgPrevious kernel
__global__ void deviceReduceKernel (int *in, int* out, int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread writes out
out[blockIdx.x]=sum; // block result
} 20 Hochschule OffenburgNew kernel
__global__ void deviceReduceBlockAtomicKernel (int *in, int* out, 
int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread atomically adds
atomicAdd(out, sum); // block result to final result
} 21 Hochschule OffenburgWhat has changed?
• Don‘t need a second kernel call
–Each block directly adds its own partial result to final result
•Only 1 thread per block will callatomicAdd
–Danger of contention is small
•But: atomicAdd synchronizes implicitly
–Can this be more efficient?
•Yes! Throughput increased by up to 10%
•But only works on recent CUDA architectures (Kepler and later)
•Noticeable for Nbetween 200.000 and 100.000.000 22 Hochschule OffenburgFurther optimization?
Potentially inefficient:
•Second kernel call
–Under-occupies GPU (only one SM is used)
–Global synchronization between kernels
•Need to wait for every thread inevery block
•Block reduction uses __syncthreads()
–Block-wide synchronization
•Need to wait for every thread in that block
•Idea: even more atomics instead of block reduction 23 Hochschule OffenburgPrevious kernel
__global__ void deviceReduceBlockAtomicKernel (int *in, int* out, 
int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = blockReduceSum (sum); 
if (threadIdx.x==0)  // first thread atomically adds
atomicAdd(out, sum); // block result to final result
} 24 Hochschule OffenburgNew kernel
__global__ void deviceReduceWarpAtomicKernel (int *in, int* out, 
int N) { 
int sum = 0; 
for (int i = blockIdx.x * blockDim.x + threadIdx.x; 
i < N; i += blockDim.x * gridDim.x) { 
sum += in[i]; 
} 
sum = warpReduceSum (sum); 
if (threadIdx.x & (warpSize-1) == 0) // first thread in warp
atomicAdd(out, sum);   // adds warp result to final result
} 25 Hochschule OffenburgWhat has changed?
•No block reduction at all!
–Each warp directly adds its own partial result to final result
•1 Thread per warp will callatomicAdd
–Higher danger of contention than before!
•Still more efficient?
–Yes! Throughput increased by up to 50%
•But only works on recent CUDA architectures (Kepler and later)
•Noticeable for Nbetween 150.000 and 50.000.000 26 Hochschule OffenburgSummary
•Warp internal functions can use SIMD-like features to
avoid blockwide synchronization ( __syncthreads )
•Atomic operations
–Performance has been greatly improved in recent CUDA 
architectures!
–Can increasingly be used als alternative to explicitly synchronized
algorithms To appear : Workshop on High- Dimensional Data Mining (HDM 821)
im Rahmen der IEEE International Conference on Data Mining (ICDM 921)Accelerating Density-Based Subspace 
Clustering in High-Dimensional Data
Jürgen Prinzbach, Tobias Lauer, Nicolas Kiefer Data Clustering
▪Find (disjoint) subsets of similar data 
in large/complex data sets
▪Similarity defined by distance function
▪Euclidean distance, Manhattan distance, 
Hamming distance, ... 
▪Basic approaches:
▪Segment the space (e.g. k-means)
▪Divide the points based on density 
(e.g. DBSCAN)
oCluster = 
Set of g kPoints with distance < ε
(k,εselectable parameters)
oAlso supports outliers High-Dimensional Data
▪High number of attributes, possibly greater than the number of data points
▪(Global) distances lose their significance
▪Differences in a few dimensions →high distance despite “similarity”
▪Points are always far apart; there is no “good” value for ε
→Clusters are not found
Solution approaches:
▪Dimension reduction
▪Principal component analysis (PCA)
▪Deep neural networks
▪Subspace clustering Subspace clustering
Search for clusters in subspaces, i.e. in spaces that are 
each spanned by only a subset of the attributes
▪Problem: dAttributes →2dSubspaces
▪Checking all subspaces is often impossible (2100> 1 quadrillion)
▪Necessary: ​​Avoid redundant calculations
▪Known existing approaches: 
CLIQUE, MAFIA, SUBCLU, SUBSCALE SUBSCALE (Kaur & Datta 2015)
(1)Identify clusters in each 1-dimensional subspace
(2)Determine Calle subsets of size k for each cluster (dense units = subclusters of smallest size)
(3)Check whether these also represent dense units in other dimensions →using hash collision: each dense unit receives a unique signature that is hashed
(4)In this way, build successively higher-dimensional dense units until they have maximum dimensionality
(5)Combine overlapping dense units in each subspace to form clusters of maximum size
*uniquely “with very high probability” (Erdos & Lehner, 1941) SUBSCALE (Kaur & Datta 2015)
(1)Identify clusters in each 1-dimensional subspace
(2)Determine Calle subsets of size k for each cluster
(dense units = clusters of smallest size) 
(3)Check whether these are also dense units in other dimensions
(4)In this way, build successively higher-dimensional dense units until they have 
maximum dimensionality
(5)Combine overlapping dense units in each subspace to form clusters 
of maximum size
Pro: Only 1 pass per dimension: O(d) instead of O(2d)!
Con: Passes (steps (2) & (3)) are computationally intensive :
Determine all subsets of size k
There is |㔶| Such subsets per cluster C
→combinatorial explosion
Running time not in O(2d∙n), but in O(d∙ nk)
In addition: High space requirement for hash table Acceleration through parallelization
Possibilities:
▪Parallel processing of dimensions
▪Brings less results than hoped (Datta et al. 2017)
▪Parallel processing of parts of the hash table
▪Scales linearly with #processors, but with higher overall work
(1 data pass per partition per dimension)
→Real time gain <50%
▪Parallel processing of individual dense units
▪Very fine-grained
▪Many (millions) small tasks
→GPU as suitable hardware Acceleration through parallelization
▪GPU: many processors (>5000)
▪Data parallelism: 
GPU threads process identical tasks, but on different 
inputs (“Single Instruction Multiple Thread” – SIMT)
▪Here: Each GPU thread processes 1 dense unit:
Thread i:
(1) Determine the i-th subset (dense unit)
(2) Calculate its unique signature
(3) Enter the subset into the hash table based on the signature “Parallel enumeration” of subsets
▪Subsets (dense units) could be determined in a simple way by
enumerating in (co)lexicographic order.
▪Subset ican be determined in O(k)
▪But: for subset im, subset i-1 must already be known
▪Sequential, not parallelizable
▪Alternative:
Determine each subset independently, only based on its number i
▪Disadvantage: More complex per subset: O(k∙ log n)
▪But: subsets can be calculated in parallel Binomial decomposition of a number i
▪Theorem (cf. Kruskal 1963, Katona 1968)
Every natural number i can be uniquely represented for every k> 0 as the sum of exactly k binomial coefficients (with 0 f n1< n2< … < nk):
㕖=㕛1
1+㕛2
2+ ⋯ +㕛Ā
㕘
We call this the binomial or combinatorial decomposition of i.
•Observation: 
n1,…,nk correspond to the element numbers of the i-th subset of C Direct calculation of the i-th subset
Example: i=7, k= 4 
7=㗎
1+ÿ
2+Ā
3+㗓
4= 0 + 1 + 1 + 5
The set { 0, 2, 3, 5 } is just the 7th subset with 4 elements in 
colexicographic enumeration:
0: {0,1,2,3} 4: {1,2,3,4} 8: {1,2,3,5} 
1: {0,1,2,4} 5: {0,1,2,5} 9: {0,1,4,5} 
2: {0,1,3,4} 6: {0,1,3,5} 10: {0,2,4,5} 
3: {0,2,3,4} 7: {0,2,3,5} 11: {1,2,4,5} Data parallel algorithm
Calculates for a 1-dimensional cluster C= { p1, p2, …, pN} in 
Dimension dalle dense units and enters these into the hash table Hein:
1fori = 0 to㕁
Ā-1 in parallel
2 find n1,…,nksuch that 㕖 =σÿ=1Ā 㕛ÿ
ÿ
3 Di= 㕝㕛1, … , 㕝 㕛Ā
4 calculate signature S(Di)
5 ifHcontains S(Di)
6 append dto dimension list of element S(Di)
7 else
8 insert new element S(Di) withd in dimension list Implementation and tests
▪Master's thesis (Nicolas Kiefer, INFM)
▪Implementation for NVIDIA GPUs with CUDA/C++
▪Efficient GPU hashing method "Stadium Hashing" (Khorasani et al., 2015)
▪Comparative tests with previous methods
▪Results
▪GPU algorithm integrated into SUBSCALE method
▪Acceleration by a factor of about 5 compared to

previous version Signatures and hash table partitions
▪Large amounts of data
→Hash table too large for RAM
▪Simple solution: Partition the hash table (according to hash value = signature)
▪Iterate over all pPartitions
▪Run the algorithm for each partition
▪Only hash the signature if it falls into the current partition
January 27, 2023 16 Signatures and hashtable partitions
▪SUBSCALE divides the range of hash values ​​into equally wide partitions
▪But signatures (= sums of random numbers) are not evenly distributed
▪Most of the partitions are only slightly filled → inefficient
January 27, 2023 17 Further optimization
▪Adaptation of partition sizes
to the distribution function
▪Approx. 50% saving in storage space 
or partitions
▪Approx. 40% time saving for entire
clustering
January 27, 2023 18 Implementation and tests
▪Tests
▪GPU algorithm integrated into SUBSCALE process
▪Acceleration by a factor of about 5 compared to

previous version Performance Tests
▪Overall Runtime
▪SUBSCALE
▪DBSCAN (merge dense units)
▪SUBSCALE
▪Calculate partitions
▪Merge partitions
January 27, 2023 20 Calculating a partition – time shares 
▪Speedup calculation+hashing: ~70x
January 27, 2023 21
Dense unit calculation
and hashing
Write to diskTransform 
hash tableClean up
hash table
Copy dataReset
hash table(s) GPU-accelerated Weighted Aggregation and Disaggregation  
in Multidimensional Databases 
Tobias Lauer  
Hochschule Offenburg Business Intelligence and Corporate Planning Online Analytical Processing (OLAP) 
• Data modeled as multidimensional <cube=  
 
–Dimensions are structured hierarchically: 
•Base elements 
•Consolidated elements 
 
–Data cube operations:  
•Slice, dice, pivot across dimensions 
•Roll-up, drill-down along hierarchies 
 
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget Multidimensional aggregation 
•Pre-computation of aggregates 
–not possible for all aggregates (dimensional 
explosion) 
–not desired 
 
•Online aggregation 
–On demand 
–Useful for planning scenarios (interactive 
write-back) 
–Common when data is stored 
in-memory 
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget In-memory OLAP storage model 
•All data stored in main memory  
  
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 10 11 3 0 14 2 11 0 13 0 0 0 0 37 9 3 4 16 0 0 7 7 5 1 0 6 4 0 0 4 33 10 1 0 11 8 6 3 17 0 2 0 2 0 0 0 0 30 20 9 8 37 19 9 10 38 7 14 0 21 4 0 0 4 100 1 2 0 3 4 0 0 4 0 1 6 7 3 0 1 4 18 0 3 1 4 6 2 0 8 10 0 7 17 0 0 0 0 29 6 0 5 11 0 9 0 9 3 3 2 8 0 0 0 0 28 7 5 6 18 10 11 0 21 13 4 15 32 3 0 1 4 75 27 14 14 55 29 20 10 59 20 18 15 53 7 0 1 8 175 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget In-memory OLAP storage model 
•All data stored in main memory  
•Only store base cells 
Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 11 3 0 2 11 0 0 0 0 9 3 4 0 0 7 5 1 0 4 0 0 10 1 0 8 6 3 0 2 0 0 0 0 1 2 0 4 0 0 0 1 6 3 0 1 0 3 1 6 2 0 10 0 7 0 0 0 6 0 5 0 9 0 3 3 2 0 0 0 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget In-memory OLAP storage model 
•All data stored in main memory  
•Only store base cells 
•Do not store zero-value cells 
 
 Memory saving, data consistency 
 
 
 
 
 1 3 5 2 14 2 8 12 16 8 5 7 9 12 Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 11 3 2 11 9 3 4 7 5 1 4 10 1 8 6 3 2 1 2 4 1 6 3 1 3 1 6 2 10 7 6 5 9 3 3 2 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget 
Represent cells as ( key, value ) pairs, 
e.g. 
( (2, 1, 0) , 4.0 ) 
Note: Values are double precision! In-memory OLAP storage model 
•All data stored in main memory  
•Only store base cells 
•Do not store zero-value cells 
 
 Memory saving, data consistency 
 
•Compute other cells on the fly  
when needed 
 
 Use GPU to accelerate 
 
 1 3 5 2 14 2 8 12 16 8 5 7 9 12 Jan 
Feb 
Mar 
Q1 
Apr 
May 
Jun 
Jul Q2 
Aug Sep 
Q3 
Oct 
Nov Dec 
Q4 
Year 1 5 4 11 3 2 11 9 3 4 7 5 1 4 10 1 8 6 3 2 1 2 4 1 6 3 1 3 1 6 2 10 7 6 5 9 3 3 2 All regions 
Europe 
France 
Italy 
UK 
North America 
USA 
Canada 
Mexico        Deviation 
   Actual 
Budget 27 14 14 55 29 20 10 59 20 18 15 53 7 0 1 8 175 GPU Two algorithmic approaches to GPU aggregation 
•Target-driven 
–For each target  cell 
•find relevant base cells (parent-to-child map) 
•aggregate 
 
•Source-driven 
–Pre-filter base cells relevant for area of all target cells 
–For each base  cell 
•create all relevant target paths (child-to-parent map) 
•look up in hashmap 
•add to aggregate Jan Feb Mar Apr May Jun Jul Aug Sep Dec Nov Oct Q1 Q2 Q3 Q4 Year 
Jan Feb Mar Apr May Jun Jul Aug Sep Dec Nov Oct Q1 Q2 Q3 Q4 Year Target-driven aggregation 
•Fast parallel aggregation step 
 
 
•Utilizes shared, global and constant memory 
•Coalesced memory access 
•Almost no thread divergence  
 
 Multi-GPU solution 
Performance optimized 
bulk aggregations 
2-step prefiltering Target-Driven Aggregation 
(3) Aggregation on GPU 
–Parallel reduction 
–Completely done in  
shared memory of GPU 
 
 
 
Copy result back to CPU 
 
(4) Final summarization on CPU Timing results 
050100150200250300
99%50%34%20%17% 5% 3% 1%
0.60%
0.30%
0.10%
0.04%CPU
GPUSelectivity CPU GPU Speedup  
99% 298ms 7ms 42.6x 
50% 146ms 7ms 20.6x 
34% 106ms 3ms 35.3x 
20% 59ms 4ms 14.8x 
17% 50ms 3ms 16.6x 
5% 20ms 3ms 6.7x 
3% 13ms 3ms 4.3x 
1% 7ms 3ms 2.3x 
0.6% 4ms 2ms 2.0x 
0.3% 2ms 2ms 1.0x 
0.1% 1ms 2ms 0.5x 
0.04% 1ms 2ms 0.5x Test data: 
3M records 
7 dimensions Optimizing groups of queries 
•Fact table prefiltering 
–<Stream compaction=  
 
•Simultaneous calculation  
of multiple queries 
–Fewer kernel launches Sparse  target areas 
•Sparsity: most cell values in an area of computed cells are 0, because 
no underlying cells exist 
 
•With target-driven aggregation, those are still „computed <  
–Lookup of base cells  
–Non-neglibile cost 
 
•Too expensive: 0-value computation dominates processing time in 
sparse areas Handling large sparse areas 
•Requirements  
Performance and memory consumption that correlate 
with number of non-zero  target cells 
•Solution 
Source-driven approach  
 - Serialized aggregation with atomics 
 - Utilize hash tables on GPU            source cells 
target cells + + + + + + Source-driven aggregation 
Jan ... Apr parent 
 map Q1 Q2 Year ...  ... ...  ... ... ...  ... ... ...  ... ... ...  ... ... ... 
... 10 
atomic add h(x) ... ... ... ... ... ... ... ... ... Jan, 2011  
sold units  
Q1, all years  
sold units  source cells 
target cell  
hash table Year 
... 10 target cell area  
+ ... 10 + Year, all years  
sold units  
atomic add Atomics: Contention 
thread serialization Great improvement 
 in Fermi and Kepler 
 over CC 1.X ...  ... ...  ... ... ...  ... ... ...  ... ... ...  ... ... ... ... 
h(x) ... ... ... ... ... ... ... ... ... 
... ... + 
atomic add Reducing contention 
...  ... ...  ... ... ...  ... ... 
h1(x) ... ... ... ... ... ... ... ... ... __ballot : 
merge with 
first? 
+ 
warp 
preaggregation warp 1 
...  ... ...  ... ... ...  ... ... 
+ warp 2 
h2(x) warp-wise 
different 
hash 
functions Speedup: Small target areas (1-11 cells) 
GPU Target Driven GPU Source Driven  Speedup vs. CPU 
Speedup vs. CPU 
Selectivity Selectivity 55,9 
54,9 
27,8 76,7 
26,4 
13,7 
3,5 2,0 0,9 
0,010,020,030,040,050,060,070,080,0
0,00001 0,0001 0,001 0,01 0,1 113,2 
12,9 18,3 
9,5 
3,0 1,6 0,7 0,5 0,1 0,010,020,030,040,050,060,070,080,0
0,00001 0,0001 0,001 0,01 0,1 1
Database:  1B records (filled cube cells) 
GPU:  3x Tesla C2070 (18 GB RAM) Larger areas  
Calculation times Speedup Time in seconds 
Database:  40M records (filled cube cells) 
GPU:  2x Tesla K20 (10 GB RAM) 15,6 35,0 
3,2 3,6 
0510152025303540
1547 target cells 38012 target cellsCPU
GPU source driven
4,9 9,7 
0,0 2,0 4,0 6,0 8,0 10,01547 target cells38012 target cells
Speedup over CPU CPU algorithm  
(multi-core) Comparison: aggregation algorithms 
•CPU algorithm good for 
low aggregation 
 
•Target driven algorithm: 
good for small and/or 
dense target areas 
 
•Source driven algorithm: 
optimized for large and 
sparse areas 
 Target size Selectivity 
GPU 
target 
driven 
GPU source 
driven 
Sparsity Remaining challenges  
•Decision mechanism 
–When GPU, when CPU? 
–Which GPU algorithm in which situation? 
 
•Find suitable thresholds 
 
•What about large and dense  target areas?  
 GPU memory problem Writeback in Top-Down Planning 
• Writeback: =opposite direction= 
of aggregation (disaggregation) 
 
•Value inserted at high level of 
aggregation is broken down to 
lower levels until the base level 
 
•All underlying base cells are 
modified, depending on the type 
of writeback Ranges and areas 
•Base elements in each dimension are 
collected in ranges   
 
D0: { [0,0] , [2,2] }        | D0| = 2 
 
D1: { [0,0] , [2,3] }        | D1| = 3 
 
D2: { [0,2] }         | D2| = 3 
 
 
•The Cartesian product of ranges 
across all dimensions forms an area 
D0× D1× D2 Multiply-base distribution Multiply-base distribution Multiply-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 240 Multiply-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 240 Multiply-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 240 
x2 Multiply-base distribution 
16 3 24 10 21 35 
4 26 58 31 17 32 
67 48 16 13 16 26 
51 10 54 78 73 44 24 92 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 480 Multiply-base distribution 
16 3 24 10 21 35 
4 26 58 31 17 32 
67 48 16 13 16 26 
51 10 54 78 73 44 24 92 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 480 Set-base distribution 
•Every relevant base cell in the area 
is set to the same given value 
 
•Naïve approach:  
search for all relevant paths and 
replace cell values 
–Problem: what about zero-value cells, 
which are not represented? 
 
•Better approach: 
(1) Delete all existing cells in area 
(2) Create all cells in area with new 
value Set-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B 10 Set-base distribution 
16 3 24 10 21 35 
4 10 10 10 31 17 10 10 
67 10 10 10 13 10 10 
10 10 10 10 10 
51 10 10 10 73 44 10 10 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B 10 Set-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 Set-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 Set-base distribution 
16 3 24 10 21 35 
4 31 17 
67 13 
51 73 44 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 
10 10 10 10 10 Set-base distribution 
16 3 24 10 21 35 
4 10 10 10 31 17 10 10 
67 10 10 10 13 10 10 
10 10 10 10 10 
51 10 10 10 73 44 10 10 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 Parallel creation of all cell paths in an area 
• <Parallel enumeration= of the 
area: 
–Each thread computes the path of  
=its= cell from the thread ID  
–Problem:   
•Gaps between ranges of a 
dimension prevent simple iterations  
•Iterating over all ranges and counting 
all visited elements is inefficient 
–Solution: 
•Represent ranges by pre-calculated 
prefix sums  (rather than start and 
end points) Prefix sum representation of ranges 
(1)  Find smallest m such that  r[m] ≥ k 
(2)  i = g[m] + k - 1 
Prefix sums of gap lengths:   g = 
Prefix sums of range lengths:  r = 
Index i of kth relevant element in D: Add-base distribution 
•The same given value v is added  
to the value of each relevant base 
cell  
•Approach: 
–Create all cells of the area and set 
value to v (as before) and store them 
temporarily 
–Find all previously  existing relevant 
cells and add their (old) value to the 
one in the new temporary area 
–Delete old relevant cells and persist  
temporary storage Add-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5 27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B+5 Add-base distribution 
16 3 24 10 21 35 
4 13
+5 29
+5 +5 31 17 16
+5 +5 
67 +5 24
+5 8 
+5 13 8 
+5 13
+5 
+5 +5 +5 +5 +5 
51 5 
+5 27
+5 39
+5 73 44 12
+5 46
+5 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 B+5 Add-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5  27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 5 5 5 5 5 
5 5 5 5 5 
5 5 5 5 5 
5 5 5 5 5 Add-base distribution 
16 3 24 10 21 35 
4 13 29 31 17 16 
67 24 8 13 8 13 
51 5  27 39 73 44 12 46 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 18 34 5 21 5 
5 29 13 13 18 
5 5 5 5 5 
10 32 44 17 51 Add-base distribution 
16 3 24 10 21 35 
4 31 17 
67 13 
51   73 44 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 18 34 5 21 5 
5 29 13 13 18 
5 5 5 5 5 
10 32 44 17 51 Add-base distribution 
16 3 24 10 21 35 
4 18 34 5 31 17 21 5 
67 5 29 13 13 13 18 
5 5 5 5 5 
51 10 32 44 73 44 17 51 
19 3 86 54 6 
49 13 90 28 42 11 
2 
81 54 
33 1 
92 
60 340 Performance tests 
Timings 
(in ms) CPU  
Intel DualCore  2x GPU  
GeForce 260  3x GPU  
Tesla C1060  4x GPU  
Tesla C2050  
Multiply-base 1 3,548  466  (7.6 x)  558  (6.4 x)  435   (8.2 x)  
Refresh 1 1,131  200  (5.7 x)  127  (8.9 x)  74 (15.3 x)  
Sum 4,679  666 ( 7.0 x ) 685 ( 6.8 x ) 509  ( 9.2 x ) 
Multiply-base 2 21,542  513   (42 x)  580   (37 x)  448  (48 x)  
Refresh 2 5,508  961  (5.7 x)  617  (8.9 x)  347  (16 x)  
Sum 27,050  1,474  ( 18 x) 1,197  ( 23 x) 795  ( 34 x) 
Timings 
(in ms) CPU  
Intel DualCore  2x GPU  
GeForce 260  3x GPU  
Tesla C1060  4x GPU  
Tesla C2050  
Set-base 14,979  900   (17 x)  715  (21 x)  572  (26 x)  
Refresh 5,598  962  (5.8 x)  610 (9.2 x)  347  (16 x)  
Sum 20,577  1,862  ( 11 x) 1,325 ( 16 x) 919  ( 22 x) 
Timings 
(in ms) CPU  
Intel DualCore  2x GPU  
GeForce 260  3x GPU  
Tesla C1060  4x GPU  
Tesla C2050  
Add-base 110,387  1,465   (75 x)  899 (123 x)  872 (127 x)  
Refresh 5,621  953  (5.9 x)  608      (9 x)  346   (16 x)  
Sum 116,008  2,418  ( 48 x) 1,507   ( 77 x) 1,218  ( 95 x) Speed-up factors (compared to CPU) 
2x GeForce 2603x Tesla C10604x Tesla C2050020406080100
7x 11x 18x 48x 
7x 16x 23x 77x 
9x 22x 34x 95x 
Add-base 
Multiply-base 2 
Multiply-base 1 Set-base Concluding remarks 
•Top-down planning creates and/or manipulates large numbers of data 
records 
•These updates are systematic  and structured 
–Involved data points can be enumerated 
–Perfectly suited for SIMD-like parallelization on GPUs 
•Increased work compared to CPU algorithm 
–but pays off in speedup! 
•CUDA implementation up to 95x faster compared to sequential CPU 
algorithm 
•Easy scaling to multiple GPUs 
